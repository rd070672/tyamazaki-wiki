# マテリアルズ・インフォマティクス
マテリアルズ・インフォマティクスは、材料・計測・プロセス・物理・化学にまたがるデータを、学習と推論の対象として整形し、材料研究の発見速度と説明力を高める学術領域である。重要なのは、モデルの巧拙だけでなく、データの定義、測定条件の明示、因果と相関の区別、そして不確かさの扱いを研究設計に組み込むことである。

## 第1章 はじめに
### 1.1 マテリアルズ・インフォマティクスの成立背景
材料研究では、組成、結晶構造、欠陥、微細組織、外場（温度・磁場・応力など）、測定法が相互に影響し合い、同じ材料名で呼ばれていても結果が大きく変わることが多い。例えば、同じ合金組成でも熱処理温度や冷却速度が違えば相や粒径が変わり、磁化や損失が変化しうる。このように、材料のふるまいは単一の要因で決まらず、多数の要因が絡み合うため、研究者が頭の中だけで全体像を整理することが難しくなってきたのである。

この状況を数理的に捉えると、材料設計変数をまとめたベクトルを$\mathbf{x}$、観測した物性や応答を$\mathbf{y}$とし、未知の写像$\mathbf{y}=f(\mathbf{x})$を推定する問題とみなせる。従来は、経験と直感に基づき$\mathbf{x}$を選び、実験や計算で$\mathbf{y}$を得て、少しずつ$f$の形を推測してきた。しかし、$\mathbf{x}$の次元が増えると、試行回数が増大し、全探索は現実的でなくなる。そこで、データを体系化し、学習可能な形に整え、$f$を統計的に推定することで、限られた試行回数でも効率よく知識を獲得し、探索と理解を同時に進める要求が高まったのである。

さらに、データの種類自体が増えたことも背景である。組成や物性の数値だけでなく、顕微鏡画像、回折・分光スペクトル、時系列信号、シミュレーションの場の分布など、情報量の大きいデータが日常的に得られるようになった。これらは人間が手作業で特徴を抽出するには負担が大きく、解析の一貫性も保ちにくい。計測と計算のデータを同じ土俵に載せ、再現可能な手順で解析し、得られた規則を次の実験・計算に戻す研究方法として、マテリアルズ・インフォマティクスが定着してきたのである。

### 1.2 5つのインフォマティクスの役割分担
材料・計測・プロセス・物理・化学は、同じデータ科学を使うとしても、入力の意味と出力の意味が異なるため、設計思想が変わる。材料インフォマティクスでは、組成や構造と物性の関係を学習し、候補の予測や順位付けを行うことが中心になる。入力は組成・構造・欠陥などであり、出力は磁化、弾性定数、熱伝導率のような物性である。ここで重要なのは、材料の表現方法であり、同じ材料を別表現で与えると学習結果が変わるため、表現の選択は学習器の選択と同等に重要である。

計測インフォマティクスでは、観測信号が物理だけでなく装置応答と前処理の影響を強く受ける。例えば、分光スペクトルの強度は正規化や背景除去で形が変わり、画像は照明条件やコントラストで特徴が変わる。したがって、計測では、何を信号として扱い、どの処理を施したかを明示しないと、学習器が装置差や処理差を学んでしまう危険がある。ここでは、データの標準化とメタデータの設計が中心課題になる。

プロセスインフォマティクスは、合成や加工条件と結果の関係を扱うが、条件空間が連続と離散の混合で、制約も多い。例えば温度は連続だが装置の段取りは離散であり、雰囲気や材料の入手性も制約になる。さらに、プロセスは同一条件でも結果が揺らぐことがあるため、平均だけでなくばらつきも対象にする必要がある。ここでは、逐次的な意思決定と、実行可能性を保ちながら探索する数理が重要になる。

物理インフォマティクスは、保存則、対称性、支配方程式、次元解析などを学習に組み込み、外挿での破綻を抑えながら説明力を高める。物理は、データが少ない領域ほど強い拘束として働くため、学習が取りうる関数形を適切に制限する役割を担う。化学インフォマティクスは、反応、前駆体、合成経路、局所電子状態など、合成可能性と物質形成を強く意識したデータと推論を扱う。材料の候補が良く見えても合成できなければ研究は前に進まないため、化学側の推論は材料探索を現実へ接続するための重要な要素である。

この5つは分断された分野ではなく、同じ研究を異なる角度から見ていると捉えると理解しやすい。材料で候補を提案し、化学とプロセスで作り方を絞り、計測で状態を同定し、物理で整合性を確認する、という形で相補的に機能する。したがって、本書では、同じ学習器がどの章でも登場しうる一方で、何を入力とし、何を出力とし、何を正しさの基準とするかが章ごとに変わることを丁寧に扱うのである。

### 1.3 タスクの分類と評価の見取り図
予測、同定、設計、最適化は似て見えるが、学習が目指すものが異なるため、評価の考え方も異なる。予測は、既知の入力$\mathbf{x}$から出力$\mathbf{y}$を推定する順問題であり、目的は誤差を小さくすることである。ただし材料研究では、誤差が小さいこと以上に、予測がどの範囲で信頼できるかが重要になるため、不確かさの妥当性も合わせて確認する必要がある。同定は、画像やスペクトルといった観測から相、欠陥、状態を推定する問題であり、正解ラベルの定義と装置差への頑健性が主戦場になる。

設計は、目標とする物性や制約を満たす入力$\mathbf{x}$を探す逆問題であり、単に当てるよりも、目的を満たす候補をどれだけ見つけられるかが重要になる。このとき、候補が物理・化学的に成立するか、合成や測定が可能かといった制約が不可欠である。最適化は、実験や計算の結果を逐次的に取り込みながら、次に試すべき条件を選び、少ない試行で改善する問題である。ここでは、改善量だけでなく、試行のコストや時間、失敗リスクまで含めて評価する必要がある。

表として整理すると、タスクごとの入力・出力・評価は次のようになる。ここで重要なのは、同じデータから始めても、研究の目的が変われば評価関数も変わるという点である。例えば回帰の平均二乗誤差が小さくても、上位候補を外すと探索としては失敗である。同様に、分類精度が高くても、新規材料群で破綻するなら研究としての有用性は低い。

| タスク | 入力例 | 出力例 | 評価の考え方 |
|---|---|---|---|
| 予測 | 組成、構造、条件 | 物性値、スペクトル | 誤差に加え不確かさの妥当性、適用範囲の明示を確認する |
| 同定 | 画像、回折、分光 | 相、欠陥、状態 | 装置差と前処理差に対する頑健性、ラベル定義の一貫性を確認する |
| 設計 | 目標物性、制約 | 組成、構造、条件 | 目的達成率、制約満足度、候補の多様性、検証での再現性を確認する |
| 最適化 | 実験結果の逐次データ | 次の実験条件 | コストと改善量の両立、試行回数に対する到達性能、失敗リスクの管理を確認する |

さらに、材料研究では多面的な評価が不可欠である。例えば、予測では平均誤差が小さくても、特定の領域だけ極端に外れていると設計判断が誤る。同定では、全体精度よりも、重要な相や欠陥を見逃さないことが優先される場合がある。したがって、研究の問いに対して、どの評価が最も重要で、どの評価は補助的かを先に決める態度が必要である。

### 1.4 不確かさと外挿の考え方
材料研究では、学習データが覆う領域が偏ることが多く、外挿の判断が研究の成否を左右する。外挿とは、学習データが十分に存在する領域の外で予測を行うことであり、既知の近傍を補間する場合と比べて失敗しやすい。例えば、ある合金系の組成範囲だけ学習して、別の元素を新たに入れた領域を予測すると、モデルは見たことのない化学環境に直面する。このとき、予測値そのものだけでなく、その予測がどれだけ怪しいかを示す尺度が必要になる。

不確かさは、少なくとも三つに分けて考えると整理しやすい。第一は測定や計算に由来する揺らぎであり、同じ条件でも結果が変動する成分である。第二はモデルが学習しきれていないことに由来する成分であり、データが少ない領域で大きくなりやすい。第三はデータ分布のずれであり、学習に使った条件や材料群と、適用したい条件や材料群が異なる場合に生じる。これらを区別しないまま不確かさを一つの数で示すと、どの対策を取るべきかが不明瞭になる。

数式としては、予測値を点ではなく分布として扱い、$\hat{y}$の代わりに$p(y\mid \mathbf{x},\mathcal{D})$を考える立場がある。ここで$\mathcal{D}$は学習データである。分布の平均が予測値、分散が不確かさの指標になるが、分散が大きい理由が上の三分類のどれに近いかを診断することが重要である。不確かさを単なる誤差棒として扱うのではなく、次に取得すべきデータの候補選定、追加実験の優先順位付け、あるいは安全側の設計判断に結びつける設計が必要である。

外挿に対処する基本方針は二つである。一つは、物理・化学的に不可能な領域を先に除外し、モデルが危険な領域に入らないようにすることである。もう一つは、外挿が疑われるときに、素直に追加データを取り、学習領域そのものを広げることである。どちらを採るかは、取得コストと研究の目的で決まるため、外挿をゼロにするのではなく、外挿と付き合う設計を学ぶ必要がある。

### 1.5 学術としての到達点
本分野の到達点は、良い予測器を作ることだけではなく、なぜその結論が支持されるのかを物理・化学の言葉で説明できることである。例えば、モデルがある元素添加を推奨したとき、その理由が単なる相関に留まるのか、結合状態や対称性の変化といった機構仮説に接続できるのかで、学術的価値が変わる。説明は、モデルの内部を可視化するだけでは十分でなく、材料科学として納得できる因果候補の形に落とす必要がある。

再現可能性は学術の前提である。データを共有しない場合でも、少なくとも第三者が同じ条件で再計算・再測定できるだけの情報を提示する必要がある。具体的には、入力データの定義、前処理手順、分割方法、学習設定、評価方法、そして失敗例を含む検証結果の提示が求められる。これらが欠けると、モデルが当たったのか、偶然当たったのか、評価が甘かったのかが区別できなくなる。

また、研究の主張は適用範囲と不可分である。あるモデルが特定の材料群ではよく当たるとしても、別の材料群では当たらないことは珍しくない。したがって、どの材料群、どの条件範囲で、どの程度の精度と不確かさで使えるのかを文章で明示し、読者が誤用しないようにする姿勢が必要である。マテリアルズ・インフォマティクスは、手法の新規性だけでなく、検証可能な主張として積み上げられて初めて学術として成立するのである。

## 第2章 データ設計と再現性
### 2.1 データの最小単位とメタデータ
材料データは数値だけで完結せず、試料履歴、前処理、測定条件、解析手順が一体になって意味を持つ。例えば、磁化の値が一つあっても、温度、磁場、測定モード（ZFC/FCなど）、試料形状、質量・体積の扱いが違えば、比較の意味が変わる。同様に、分光スペクトルのピーク位置が同じでも、エネルギー較正や正規化手順が違えば、同一の議論はできない。したがって、データの最小単位は、値そのものだけではなく、その値がどう得られたかを含むレコードとして設計すべきである。

ここでいうレコードは、少なくとも次の三要素を含むと考えるとよい。第一に観測値または計算値であり、数値、配列、画像などが該当する。第二に条件であり、試料の組成・構造・履歴、測定条件、計算条件が含まれる。第三に由来であり、データの取得日時、装置、オペレータ、解析ソフトのバージョンなど、追跡可能性に関わる情報である。これらを一まとまりとして扱うことで、同じ値が複数回現れても、どの条件差が原因かを後から検討できるようになる。

メタデータは多すぎると運用が破綻しやすいが、少なすぎると再現性が失われる。したがって、欠落してよい情報と必須情報を最初に決める必要がある。次のような表で、最低限の項目群を先に固定してしまうやり方が理解しやすい。研究室内で最初にこの表を決めておくと、データが増えた後でも統合が容易になる。

| 区分 | 例 | 必要である理由 |
|---|---|---|
| 試料同定 | 試料ID、ロット、作製日 | 同一試料かどうかを後から判定するためである |
| 組成・構造 | 組成比、相、格子定数、欠陥情報 | 物性の主要因であり、比較の前提になるためである |
| 履歴 | 熱処理条件、加工条件、雰囲気 | 同じ組成でも結果が変わる主因になりやすいためである |
| 測定条件 | 温度、磁場、周波数、ジオメトリ | 物性値の定義と直接結びつくためである |
| 解析手順 | 正規化、背景除去、フィット条件 | 処理が結果を変えうるためである |
| 由来 | 装置、ソフト、バージョン、担当者 | 追跡と再現のためである |

このように設計すると、データを追加するたびに同じ形式で記録でき、後から機械学習へ投入するときに必要な整形作業が減る。重要なのは、学習のために後付けでメタデータを集めようとすると、思い出せない、記録がない、条件が混ざるといった問題が生じやすい点である。したがって、データ取得の時点で、研究ノートと同等の情報を機械可読な形で残す設計が必要である。

### 2.2 単位・参照状態・定義ゆれの統制
形成エネルギー、磁化、異方性、抵抗率などは、参照状態や測定法の違いで数値が変わるため、比較したい量の定義を固定する必要がある。例えば磁化は、質量あたり（A m$^2$/kg）と体積あたり（A/m）で表現が異なり、密度の違いを挟むと同じ値でも意味が変わる。抵抗率も、薄膜ではシート抵抗（$\Omega/\square$）を経由して抵抗率へ換算することがあり、厚みの不確かさが結果に効く。したがって、単位だけでなく、どの手順でその単位の値を算出したかが必要になる。

参照状態の例として、形成エネルギーを考えると理解しやすい。ある化学式$\mathrm{A}_{n}\mathrm{B}_{m}$の形成エネルギー$E_\mathrm{form}$は、総エネルギー$E(\mathrm{A}_{n}\mathrm{B}_{m})$と参照化学ポテンシャル$\mu_\mathrm{A},\mu_\mathrm{B}$を用いて
$$
E_\mathrm{form}=E(\mathrm{A}_{n}\mathrm{B}_{m})-n\mu_\mathrm{A}-m\mu_\mathrm{B}
$$
のように定義される。ここで$\mu$を何に取るか（例えば単体固体、分子気体、別相など）で$E_\mathrm{form}$の数値は変わる。したがって、形成エネルギーをデータとして扱うなら、参照状態の定義を必ず記録し、同じ参照を用いたデータ同士を比較する設計が必要である。

定義ゆれは、物性名が同じでも中身が異なる形で現れる。例えば磁気異方性は、エネルギー密度（J/m$^3$）としての$K$で報告されることもあれば、試料体積を明示せずエネルギー差（J）として報告されることもある。さらに、結晶方位の取り方や符号規約が違うと、同じ材料でも正負が逆に見えることがある。これを避けるには、記号の定義、座標系、符号規約を文書化し、データベース上でも項目として固定する必要がある。

初学者向けには、データ登録前に、単位と定義の表を必ず作る習慣を推奨する。例えば次のように、物性名、単位、参照、算出方法を一行で固定すると、後から混乱しにくい。

| 物性 | 推奨単位 | 参照・定義として必須の情報 |
|---|---|---|
| 磁化 $M$ | A/m または A m$^2$/kg | 温度、磁場、試料体積または質量、背景補正の有無 |
| 抵抗率 $\rho$ | $\Omega\cdot$m | 試料形状、厚み、電極配置、温度、測定法 |
| 形成エネルギー $E_\mathrm{form}$ | eV/atom | 参照$\mu$の定義、計算条件、正規化方法 |
| 異方性 $K$ | J/m$^3$ | 座標系、方位、符号規約、体積算出方法 |

この統制ができていないと、学習器は物理法則ではなく、単位変換の違いや定義ゆれを学んでしまう。その結果、見かけの相関が現れても、別データへ移すと崩れる。したがって、単位・参照状態・定義ゆれの統制は、モデル以前にデータ側の必須条件である。

### 2.3 データ分割と漏洩の回避
同系列の組成スイープや同一バッチの試料が訓練と評価にまたがると、汎化ではなく近傍補間の性能を測ってしまう。これは材料研究で非常に起こりやすい。例えば、同じ研究室で同じ装置を使い、少しずつ組成を変えたデータを集めた場合、測定系の癖やバッチ固有の傾向が全データに共通して入り込む。このデータをランダムに分割すると、訓練にも評価にも同じ癖が含まれるため、モデルが癖を覚えて高得点を出してしまうことがある。

分割設計は、研究で示したい主張と整合させるべきである。もし主張が、同一系列内での精密な補間性能であるなら、ランダム分割でも意味がある。しかし多くの場合、示したいのは未知の材料群や未知の条件への適用である。その場合、評価データは、訓練データと十分に異なる材料群や条件群で構成する必要がある。化学空間や構造空間で距離を確保する分割、あるいは系列ごとにまとめて外す分割が必要になる。

初学者が混乱しやすい点は、分割方法が一つではないことである。分割方法は、目的に応じて複数用意し、結果の見え方がどう変わるかを示すと理解が深まる。以下に分割の代表的な設計をまとめる。

| 分割設計 | ねらい | 注意点 |
|---|---|---|
| ランダム分割 | 補間性能の確認 | 系列データでは漏洩が起こりやすい |
| グループ分割（バッチ単位） | バッチ依存の漏洩を避ける | グループの定義が必要である |
| 系列外し（元素や化学式単位） | 未知材料群への汎化確認 | 難易度が上がり誤差が増えやすい |
| 条件外し（温度域や周波数域） | 条件外挿の確認 | 条件の偏りが評価に直結する |
| 時系列分割（古い→新しい） | 運用上の将来予測 | 装置状態の変化が混ざりうる |

漏洩を避けるというのは、単に厳しくすることではなく、何を未知と見なすかを研究の問いとして固定することである。例えば、磁性材料の探索で元素添加の効果を論じたいなら、元素種をまたぐ分割が重要である。熱処理条件の最適化を論じたいなら、条件外し分割が重要である。分割設計を先に宣言し、それに沿って評価を行うことで、学術的主張が明確になる。

### 2.4 欠損・外れ値・検出限界の扱い
欠損は単なる空白ではなく、測定不能、未測定、除外など意味が異なるため、欠損の種類を区別して記録する必要がある。例えば、測定時間が足りなくて未測定である場合と、装置の検出限界以下で測定不能である場合では、統計的な意味が異なる。前者は追加測定で埋まるが、後者は上限値や下限値として扱うべき情報を含む。したがって、欠損を一律にNaNとして扱うだけでは、学習が誤る可能性がある。

検出限界は、材料研究で頻繁に現れる。例えば微量元素の濃度、低レベルの磁気信号、微小な異方性などは、測定装置の分解能やノイズにより、ある範囲以下が区別できない。この場合、値がゼロではなく、ある範囲にあるという情報である。これをデータとして扱うには、上限・下限の形で記録する、あるいは検出限界をメタデータとして併記する設計が必要である。

外れ値も同様に、一括して削除すると重要な発見を捨てる危険がある。外れ値は、装置異常、前処理ミス、入力ミスで生じることもあるが、相変態や欠陥形成など物理的に意味のある非連続変化で生じることもある。したがって、外れ値を見つけたら、まず由来を追跡できるかを確認し、再測定や別手段での確認を設計に含めるべきである。外れ値の扱いを決めるときは、削除、修正、保持のどれを選ぶにせよ、理由を記録し、後から第三者が検証できる形にする必要がある。

初学者向けには、欠損と外れ値を扱うときの最小限の整理として、次の区分を導入すると理解しやすい。値そのものだけでなく、ステータスを付けるという発想である。

| 状態 | 意味 | 推奨記録 |
|---|---|---|
| 未測定 | 取得していない | 欠損＋理由（時間不足など） |
| 測定不能 | 検出限界以下など | 上限・下限＋検出限界 |
| 除外 | 解析方針で除いた | 除外フラグ＋根拠 |
| 疑義 | 外れの可能性あり | 疑義フラグ＋再確認状況 |
| 確認済み外れ | 物理的外れである | 根拠データへのリンク |

このように記録しておくと、学習段階で「欠損を平均で埋める」といった乱暴な処理を避けやすい。さらに、再現性の観点からは、欠損や外れ値の処理規則を文書として固定し、同じ規則を別データにも適用できる形にしておくことが重要である。

### 2.5 記録方式と追跡可能性
データは増えるほど、後からの再解析や統合が難しくなるため、項目名、辞書、バージョン、変更履歴を追跡できる形で保存する必要がある。研究室内の共有段階では、個人の命名規則やフォルダ構成に依存しがちだが、人数が増えるほど統一が崩れる。統一が崩れると、同じ意味の項目が別名で増え、逆に同名でも意味が異なる項目が混在し、学習の前段で混乱が起こる。したがって、データ構造を先に決め、運用で守る必要がある。

追跡可能性は、どのデータがどの手順で得られたかを後から辿れる性質である。これには、少なくとも三つのレベルがある。第一に、生データと処理後データの対応が取れることである。第二に、処理に使ったパラメータやソフトのバージョンが記録されていることである。第三に、解析コードや設定ファイルが参照でき、同じ結果を再生成できることである。学術的主張としては、最低でも第二レベル、可能なら第三レベルを目指すべきである。

記録方式としては、表形式（CSVなど）は扱いやすいが、画像やスペクトルなど大きなデータではメタデータとの紐付けが弱くなりやすい。そこで、データ本体はバイナリ形式（画像、HDF5など）で保存し、メタデータは別ファイルで管理し、共通の試料IDで結ぶ方式がよく用いられる。重要なのは形式そのものではなく、どの形式であっても、項目名の辞書と単位、参照、処理履歴が失われないように設計することである。

運用上は、次のような要点を押さえると、初学者でも破綻しにくい。第一に、試料IDを全てのデータに付与し、IDを起点に辿れるようにする。第二に、項目名を辞書で固定し、略語や表記揺れを禁止する。第三に、データの更新は上書きではなく、バージョンとして残し、何が変わったかを変更履歴に書く。これにより、後から結果が変わったときに原因を追えるようになる。

研究室内の共有段階から外部公開を想定した設計にすると、共同研究や論文付随データの提示が容易になる。外部公開を必須としない場合でも、研究の継続性のために追跡可能性は重要である。卒業や異動で担当者が変わっても、データと手順が残っていれば研究は再開できる。したがって、記録方式と追跡可能性は、モデルの前段で研究の品質を決める要素である。


## 第3章 学習・推論の基礎と評価

本章では、マテリアルズ・インフォマティクスで用いる学習と推論を、材料研究の文脈で破綻しない形に定式化し、評価まで含めて理解するための基礎をまとめる。機械学習をブラックボックスとして使いがちであるが、材料分野では入力の作り方、評価の作り方、不確かさの扱いが結論を左右するため、数式で骨格を押さえた上で、どこで注意すべきかを明示する。



### 3.1 学習問題の定式化

材料研究の多くは、ある条件を与えると物性や応答が決まるという意味で、順問題として
$$
\mathbf{y}=f(\mathbf{x})
$$
と書ける。ここで$\mathbf{x}$は入力であり、組成、構造、欠陥、温度、磁場、プロセス条件などをまとめた変数である。$\mathbf{y}$は出力であり、磁化、硬さ、抵抗率、スペクトル、画像特徴量などの観測量である。$f$は未知の写像であり、実験や計算で得られたデータから推定する対象である。

機械学習の基本的な立場では、データ集合$\mathcal{D}=\{(\mathbf{x}_i,\mathbf{y}_i)\}_{i=1}^{N}$が与えられたとき、$f$を近似するモデル$\hat{f}_\theta$（$\theta$はパラメータ）を学習し、未知の入力$\mathbf{x}$に対して$\hat{\mathbf{y}}=\hat{f}_\theta(\mathbf{x})$を出す。ここまでは一般的であるが、材料分野で重要なのは、$\mathbf{x}$が単なる数値ベクトルではなく、複数の表現を持ちうる点である。

例えば、組成$\mathrm{Fe}_{1-x}\mathrm{Co}_{x}$は、(i)$x$という1変数で表すことも、(ii)元素比のベクトル$(c_\mathrm{Fe},c_\mathrm{Co})$で表すこともできる。結晶構造も、(i)格子定数と原子座標の集合で表すことも、(ii)局所環境の統計量（配位数分布など）で表すことも、(iii)対称性不変な記述子で表すこともできる。表現が異なると、同じ学習器でも学習しやすさと外挿の挙動が変わるため、表現選択はモデル選択と同等に重要である。

この点をもう少し数理的に書くため、表現写像（特徴量化）$\phi$を導入し、
$$
\mathbf{z}=\phi(\mathbf{x}),\qquad \hat{\mathbf{y}}=g_\theta(\mathbf{z})=g_\theta(\phi(\mathbf{x}))
$$
と分解して考える。ここで$\mathbf{z}$は学習器に入力する特徴量である。材料分野では、$\phi$の設計が悪いと、モデルが物理的意味のない規則に収束しやすい。例えば、試料IDや測定日といった本来予測に使うべきでない情報が$\phi$に混入すると、モデルは材料の性質ではなく実験順序の癖を学んでしまう。これは第2章で述べた漏洩の問題とも直結する。

また、材料データはノイズを含むため、順問題は厳密には
$$
\mathbf{y}=f(\mathbf{x})+\boldsymbol{\varepsilon}
$$
と書くべきである。$\boldsymbol{\varepsilon}$は測定誤差や再現性の揺らぎを表す。ここで重要なのは、$\boldsymbol{\varepsilon}$が入力$\mathbf{x}$に依存する場合がある点である。例えば、薄膜の抵抗率は厚み測定の誤差が支配的で、厚みが薄いほど相対誤差が大きくなることがある。このような入力依存ノイズ（異分散）は、後述する不確かさ推定にも関わるため、単に平均二乗誤差だけで学習を進めると不適切になることがある。

さらに、研究目的が順問題だけとは限らない。設計は逆問題として
$$
\mathbf{x}=\arg\max_{\mathbf{x}\in\Omega} \; U(\mathbf{x})
$$
のように書ける。ここで$\Omega$は実行可能な領域（合成可能性や安全性の制約を含む）であり、$U(\mathbf{x})$は目的関数（性能指標）である。機械学習は$U$や$f$の推定器を与えるが、最終的な意思決定問題の一部に過ぎない。したがって、学習の定式化では、研究で最終的に何を決めたいのか（予測か、同定か、設計か、最適化か）を先に固定し、それに合わせて$\phi$、$g_\theta$、評価指標を選ぶ必要がある。



### 3.2 損失関数と制約の入れ方

学習とは、モデル$\hat{f}_\theta$がデータをどれだけ説明できるかを表す尺度を定義し、その尺度が小さくなるように$\theta$を調整することである。この尺度が損失関数$\mathcal{L}$である。回帰の基本は平均二乗誤差であり、スカラー出力$y$の場合は
$$
\mathcal{L}_\mathrm{MSE}(\theta)=\frac{1}{N}\sum_{i=1}^{N}(y_i-\hat{y}_i)^2
$$
である。ベクトル出力なら$\|\mathbf{y}_i-\hat{\mathbf{y}}_i\|^2$に置き換える。平均二乗誤差は扱いやすいが、材料研究ではそれだけでは不十分になる場面が多い。

第一に、重要な領域だけ重み付けしたい場合がある。例えば、磁石材料探索で高温域の性能を重視するなら、低温データと同じ重みで学習すると研究目的とずれる。このとき、重み$w_i$を導入して
$$
\mathcal{L}_\mathrm{wMSE}(\theta)=\frac{1}{N}\sum_{i=1}^{N} w_i\,(y_i-\hat{y}_i)^2
$$
とする。$w_i$は任意に見えるが、研究目的（どの条件を優先するか）を数式化したものなので、論文や報告では必ず根拠とともに記述すべきである。

第二に、ノイズの大きさがデータ点ごとに違う場合がある。観測誤差の分散を$\sigma_i^2$とすると、尤度最大化の観点からは
$$
\mathcal{L}(\theta)=\frac{1}{N}\sum_{i=1}^{N}\frac{(y_i-\hat{y}_i)^2}{\sigma_i^2}
$$
のように、誤差の大きい点を過度に支配させない設計が自然になる。$\sigma_i$が未知なら、再現性試験や測定の繰り返しから推定し、メタデータとして持つのが望ましい。

第三に、物理・化学の制約を損失に組み込む必要がある。材料分野の制約は、単に値域制約（例えば濃度は$0\le x\le 1$）だけではなく、保存則や対称性、支配方程式などの形で現れることがある。一般に残差$\mathcal{R}$を導入して
$$
\mathcal{L}(\theta)=\mathcal{L}_\mathrm{data}(\theta)+\lambda \, \mathcal{L}_\mathrm{phys}(\theta)
$$
とし、
$$
\mathcal{L}_\mathrm{phys}(\theta)=\frac{1}{N}\sum_{i=1}^{N}\|\mathcal{R}(\hat{\mathbf{y}}_i,\mathbf{x}_i)\|^2
$$
のように定義する。ここで$\lambda$はデータ適合と物理制約のバランスを決める係数である。$\lambda$を大きくすると物理に厳密に従う方向へ寄るが、データのばらつきや未知の効果を吸収できなくなることがある。逆に$\lambda$が小さすぎると物理制約が飾りになり、外挿で破綻しやすくなる。

初学者にとって重要なのは、$\lambda$は機械学習の都合で決めるパラメータではなく、研究の主張を形作るパラメータであるという点である。例えば、測定ノイズが大きい場合、物理制約を強めることで過学習を抑える意味がある。一方で、物理モデルが近似的である場合、物理制約を強めすぎると、データが示す新しい現象を捨てる危険がある。したがって、$\lambda$は交差検証だけでなく、物理残差の分布や適用範囲の議論と合わせて決めるべきである。

制約の入れ方は損失項以外にもある。例えば、出力の形そのものを制約する（対称性不変な表現を採用する、エネルギーが単調になるようにパラメータ化する）といった構造的制約がある。材料分野では、損失で縛るか、構造で縛るか、あるいは両方を併用するかを検討し、どの制約が何を保証するのかを文章で明確にする必要がある。



### 3.3 不確かさの推定と検証

材料設計において重要なのは、予測値そのものだけでなく、その予測がどれだけ信用できるかである。これが不確かさであり、少なくとも測定ノイズ、モデル不確かさ、分布ずれの三つに分けて扱うと整理しやすい。

測定ノイズは、同じ条件を繰り返してもっても値が揺らぐ成分である。これはデータ側の性質であり、理想的には繰り返し測定から分散を推定できる。モデル不確かさは、学習データが不足している、あるいはモデルが表現しきれていないことから生じる成分であり、データ密度の低い領域で大きくなる。分布ずれは、訓練に使った材料群や条件群と、適用先が異なることで生じる成分であり、外挿に近い状況で顕在化する。

不確かさの表現として、スカラー出力$y$について予測分布を
$$
p(y\mid \mathbf{x},\mathcal{D})
$$
と書く立場を取ると、予測平均$\mu(\mathbf{x})$と予測分散$\sigma^2(\mathbf{x})$を用いて
$$
y\sim \mathcal{N}(\mu(\mathbf{x}),\sigma^2(\mathbf{x}))
$$
のように近似することが多い。ここで$\sigma(\mathbf{x})$が大きいほど、予測が不確かである。重要なのは、$\sigma$を出しただけで満足せず、その$\sigma$が妥当かどうかを検証することである。

妥当性の検証の基本は被覆率である。例えば、予測平均$\mu$と標準偏差$\sigma$を用いて、区間$[\mu-1.96\sigma,\mu+1.96\sigma]$を95%信頼区間だと主張するなら、実際にテストデータの95%程度がこの区間に入るべきである。入らないなら、モデルは過信しているか、あるいは過小評価している。この検証を校正という。校正が取れていない不確かさは、意思決定に使うと危険である。

材料研究では、外挿の検知として不確かさを使いたくなるが、注意が必要である。不確かさが小さいのに外挿で失敗することもある。これは、モデルが自信過剰になっている場合、あるいは訓練データに似た表面特徴だけを見てしまい、実際には異なる物理を見落としている場合に起こる。したがって、不確かさは万能の警報ではなく、分布ずれ検知、物理残差、入力空間での距離など、複数の指標を併用して判断する設計が望ましい。

また、初学者が見落としやすい点として、予測誤差と不確かさは同じではない。誤差は事後的に真値と比べてわかる量であり、不確かさは事前に予測に付随させる量である。研究の現場では真値がわからない状況で意思決定を行うため、不確かさが正しく校正されていることが重要になる。これが、材料探索において不確かさ推定が中心的役割を持つ理由である。



### 3.4 多目的設計とパレート性

材料設計では、良い材料とは一つの数字で決まらないことが多い。例えば、磁性材料では高磁化だけでなく低損失、加工性、耐熱性、コストなどが同時に重要になる。これを無理に単一指標に潰すと、どの目的をどれだけ重視したかが暗黙化し、議論が不透明になる。したがって、多目的最適化として扱い、目的ベクトルを
$$
\mathbf{p}(\mathbf{x})=(p_1(\mathbf{x}),p_2(\mathbf{x}),\dots,p_K(\mathbf{x}))
$$
のように定義し、複数の目的を同時に改善する枠組みを採用するのが自然である。

多目的では、ある目的を改善すると別の目的が悪化するトレードオフが起こりやすい。そこで、パレート支配という関係を定義する。$\mathbf{x}_a$が$\mathbf{x}_b$を支配するとは、全ての目的で$\mathbf{x}_a$が$\mathbf{x}_b$以上に良く、かつ少なくとも一つの目的で厳密に良いことである。つまり
$$
p_k(\mathbf{x}_a)\ge p_k(\mathbf{x}_b)\quad(\forall k),\qquad \exists k: p_k(\mathbf{x}_a)>p_k(\mathbf{x}_b)
$$
が成り立つとき、$\mathbf{x}_b$は候補として劣るとみなせる。

パレート前線とは、他の候補に支配されない候補の集合である。この集合は、研究者が選ぶべき候補群を示すが、最終的にどれを採用するかは、用途や制約、研究上の選好に依存する。ここで重要なのは、選好を後段に分離できることである。つまり、学習は候補集合を提示し、意思決定は用途側の要求をもとに行う。これにより、どのトレードオフを採用したかが明示でき、議論の透明性が上がる。

初学者が陥りやすい誤りは、多目的を重み付き和
$$
S(\mathbf{x})=\sum_{k=1}^{K}\alpha_k p_k(\mathbf{x})
$$
で一つにしてしまい、$\alpha_k$の意味を説明しないことである。重み付き和は便利だが、$\alpha_k$を変えると結論が変わるため、重みの根拠がないと学術的主張として弱い。したがって、本書では、まずパレート性を基礎として候補を整理し、その上で用途に応じた選択を行う流れを基本として扱う。



### 3.5 説明と解釈の道具立て

機械学習が出した予測は、そのままでは材料科学としての理解につながらないことが多い。学術として重要なのは、なぜその予測が出たのか、どの変数が効いているのか、どの仮説が支持されるのかを説明し、次の実験や計算へ接続することである。このとき、特徴量重要度、反実仮想、感度解析は、因果候補を絞る補助手段として役立つ。

特徴量重要度は、入力のどの成分が予測に寄与しているかを示す指標である。ただし、重要度が高いことは因果を意味しない。例えば、ある元素濃度が重要に見えても、それが実際には熱処理条件と相関しているだけで、濃度が直接効いていない可能性がある。したがって、重要度は仮説生成の手がかりであり、因果の主張に直結させてはならない。

反実仮想は、入力$\mathbf{x}$を少し変えたときに出力がどう変わるかを調べる考え方である。数理的には、ある基準点$\mathbf{x}_0$に対して
$$
\Delta \hat{y}=\hat{f}(\mathbf{x}_0+\Delta \mathbf{x})-\hat{f}(\mathbf{x}_0)
$$
を評価する。これにより、モデルがどの方向の変化に敏感かを調べられる。材料設計では、実際に操作できる変数（合成条件や組成）に沿って$\Delta \mathbf{x}$を選ぶことが重要であり、物理的に不可能な変化を調べても意味がない。

感度解析は、微小変化に対する応答を調べる方法であり、連続変数に対しては勾配
$$
\nabla_{\mathbf{x}} \hat{y}
$$
を用いて局所的な感度を評価できる。これは、どの変数が効きやすいかを定量化する助けになるが、非線形性が強い場合は局所感度だけでは全体の挙動を説明できないことがある。したがって、局所と大域の両面から解釈を補強する必要がある。

説明は万能ではないため、物理・化学の既知知見と矛盾しない範囲で、何が言えて何が言えないかを明示する態度が重要である。例えば、説明結果から相関の強い特徴が見つかったとしても、それが因果かどうかは追加実験や介入設計を通じて確かめる必要がある。逆に、説明が難しいモデルであっても、物理残差や対称性の満足度を示すことで、外挿での信頼性を補強できる場合がある。本章の結論として、予測精度の向上と同じくらい、解釈と検証の設計が材料研究では重要であることを強調しておく。



## 第4章 材料インフォマティクス I（表現と予測）

本章では、材料の組成・構造・欠陥や微細組織を、学習器が扱える入力へ写像する方法と、その入力から物性を予測する枠組みを整理する。材料研究では、表現の選択が結論の方向を決めることが多く、同じアルゴリズムでも表現が不適切なら、物理的意味のない相関に依存した予測器ができてしまう。したがって、表現は単なる前処理ではなく、材料科学の仮説を数学へ落とす作業として理解する必要がある。



### 4.1 組成表現と距離

材料の組成を表す最も基本的な方法は、元素濃度のベクトルである。元素集合を$\{e_1,\dots,e_d\}$とし、組成を
$$
\mathbf{c}=(c_1,c_2,\dots,c_d),\qquad c_k\ge 0,\quad \sum_{k=1}^{d}c_k=1
$$
と表す。ここで$c_k$は元素$e_k$のモル分率（または原子分率）である。この表現は単純で便利であるが、$\sum c_k=1$という制約のため、$\mathbf{c}$は$d$次元空間の全体ではなく、$(d-1)$次元の単体（simplex）上にある点として振る舞う。したがって、単純にユークリッド距離
$$
D_\mathrm{E}(\mathbf{c},\mathbf{c}')=\|\mathbf{c}-\mathbf{c}'\|_2
$$
で近さを測ると、材料科学的に重要な差を過小評価する場合がある。

特に問題になりやすいのは、希薄添加（微量元素）で性質が大きく変わる系である。例えば、$c_\mathrm{X}=0.5\%$の添加で相安定性や磁気異方性が変わるとき、$\mathbf{c}$の差は非常に小さいため、距離に基づく学習（近傍探索、カーネル法、クラスタリング）がその重要性を捉えにくい。これは、距離が「濃度差の大きさ」を重視しすぎるためである。

この問題に対して、対数比に基づく表現が検討対象になる。組成は比で意味を持つことが多いため、例えば基準元素$e_1$に対して
$$
z_k=\log\frac{c_k}{c_1}\quad (k=2,\dots,d)
$$
のように比を取ると、希薄成分の相対変化を強調できる。ただし、$c_k=0$で発散するため、微小な$\epsilon$を足して
$$
z_k=\log\frac{c_k+\epsilon}{c_1+\epsilon}
$$
のように扱うことが多い。$\epsilon$は数値的安定化のためであるが、物理的には「検出限界以下」や「未測定」の扱いと絡むため、第2章の欠損設計と整合する形で選ぶ必要がある。

別の視点として、元素の物性を織り込んだ組成表現も重要である。元素ごとに原子番号$Z$、電気陰性度$\chi$、原子半径$r$などの属性$\mathbf{a}(e_k)$を持たせ、組成平均
$$
\bar{\mathbf{a}}=\sum_{k=1}^{d}c_k\,\mathbf{a}(e_k)
$$
や分散
$$
\mathrm{Var}(\mathbf{a})=\sum_{k=1}^{d}c_k\left(\mathbf{a}(e_k)-\bar{\mathbf{a}}\right)^2
$$
を特徴量として使うと、組成差を化学的意味で測れるようになる。これは「組成そのもの」ではなく「化学性質の混合」を表すため、同じ濃度差でも化学的に大きな差がある場合に距離が増える。初学者には、距離が単なる数学ではなく、何を近いと見なすかという研究上の仮説であることを強調しておくべきである。

距離の選択は、予測モデルの挙動に直接影響する。例えば、カーネル回帰やガウス過程では
$$
k(\mathbf{c},\mathbf{c}')=\exp\left(-\frac{D(\mathbf{c},\mathbf{c}')^2}{2\ell^2}\right)
$$
のように距離$D$が類似度に入るため、距離が変わると「どこまでを近いとみなすか」が変わり、外挿の判定も変わる。したがって、距離は実験設計や探索戦略とも結びつくものであり、単に計算が楽だからという理由で決めてはならない。



### 4.2 結晶構造表現

結晶構造の表現は、材料インフォマティクスの中心課題である。なぜなら、物性は組成だけでなく、原子配置と対称性に強く依存するからである。結晶の入力として最も素直なのは格子ベクトル$\mathbf{L}$と原子座標$\{\mathbf{r}_i\}$であるが、この表現は学習器にとって扱いにくい。理由は、(i)原子の並べ替え（同種原子のラベル交換）、(ii)並進（原点の取り方）、(iii)回転、(iv)結晶の周期境界条件に対して、同一構造が多数の等価表現を持つためである。学習器がこれらの等価性を自動的に学ぶのは難しく、データ効率が悪くなる。

したがって、構造表現では、同一構造の等価性を保ちつつ、局所環境の差を識別できるように設計する必要がある。基本方針は二つである。第一は、対称性不変量に変換することである。第二は、局所環境を記述して集約することである。

局所環境の基本は、ある原子$i$の近傍原子$j$との距離$r_{ij}=|\mathbf{r}_i-\mathbf{r}_j|$である。例えば、距離の分布を滑らかに表すために、ガウス型の基底を用いて
$$
\rho_i(r)=\sum_{j\in\mathcal{N}(i)}\exp\left(-\frac{(r-r_{ij})^2}{2\sigma^2}\right)
$$
のような局所原子密度を定義し、それを基底展開して特徴量化する方法がある。ここで$\mathcal{N}(i)$は近傍集合であり、カットオフ$r_\mathrm{c}$で定義される。$\sigma$は平滑化幅であり、熱揺らぎや欠陥の微小変位に対して頑健にする役割を持つ。初学者には、$r_\mathrm{c}$と$\sigma$は材料の相互作用範囲とノイズレベルの仮説であり、闇雲に増減させるのではなく、対象物性（局所支配か長距離支配か）に合わせて設定する必要があると説明するとよい。

対称性不変量の具体例として、距離だけでなく角度も使う。原子$i$を中心とした三体角$\theta_{jik}$を用いて、
$$
G_i^{(2)}=\sum_{j}\exp(-\eta (r_{ij}-R_s)^2),\qquad
G_i^{(3)}=\sum_{j,k}(1+\cos\theta_{jik})^\zeta \exp(-\eta (r_{ij}^2+r_{ik}^2+r_{jk}^2))
$$
のような記述子を構成する考え方がある。ここで$\eta,R_s,\zeta$はパラメータであり、近接距離と角度の寄与をどの程度見るかを決める。重要なのは、角度項を導入すると配位多面体の違い（例えば八面体と四面体）が区別でき、構造相や局所対称性に敏感な物性を捉えやすくなる点である。

結晶構造表現は、単に入力の問題ではなく、物理量のスケールとも結びつく。例えば、弾性定数やフォノンのように長距離の結合が効く量では、局所表現だけで不十分なことがある。一方、局在した磁気モーメントや局所電子状態が支配的な量では、局所表現が非常に効く。したがって、どの表現を採るかは、対象物性が局所支配か多体・長距離支配かという材料科学の理解と整合している必要がある。



### 4.3 欠陥・局所構造・微細組織

平均構造だけで説明できない物性は、欠陥や微細組織に起因する場合が多い。例えば、磁気損失や保磁力、塑性変形、疲労、腐食などは、転位、粒界、析出物、空孔、偏析、相混在などの影響を強く受ける。同じ組成・同じ結晶相でも、熱処理や加工で微細組織が変われば物性が変わるという事実は、材料研究の基本である。したがって、組成と平均構造だけを入力にすると、「同じ入力なのに出力が違う」データが増え、学習器は矛盾を平均化してしまう。

この問題を表現側で扱う方法は大きく三つある。第一に、欠陥や微細組織を直接特徴量として与える方法である。例えば、平均粒径$d$、粒径分布の分散、体積分率、欠陥密度、析出物サイズなどを数値化して入力に加える。磁性では、粒径や欠陥密度が磁壁運動を支配するため、これらの特徴量は物理的意味を持つ。ただし、これらは計測コストが高い場合もあるため、どこまでを必須入力とするかは研究目的と現実的制約で決める必要がある。

第二に、局所環境の統計量で平均構造の外側を表す方法である。例えば、局所配位数$N_i$の分布や、局所ひずみ（近接距離のばらつき）を特徴量化し、平均だけでなく分散や歪度を入力にする。一般に、局所量$u_i$に対して
$$
\bar{u}=\frac{1}{M}\sum_{i=1}^{M}u_i,\qquad
s^2=\frac{1}{M}\sum_{i=1}^{M}(u_i-\bar{u})^2
$$
のような統計量を入れると、同じ平均でもばらつきが違うことを学習器に伝えられる。これは、アモルファスやナノ結晶、欠陥濃度の異なる系で特に重要である。

第三に、画像や時空間データをそのまま扱う方法である。微細組織像、磁区像、EBSDマップ、X線回折の2Dパターンなどは、数値化しにくい情報を含む。画像を特徴量化するには、(i)形態学的特徴（面積、周長、円形度など）を計算する方法、(ii)周波数領域（フーリエスペクトル）を使う方法、(iii)学習器で表現を獲得する方法がある。本書ではプログラム例は出さないが、重要なのは、画像の前処理（スケール合わせ、コントラスト、ノイズ除去）が物理情報を壊しうるため、計測インフォマティクスと同じく処理手順を固定し、再現可能な形で記録することである。

多尺度表現という考え方も重要である。材料の性質は、原子スケールの結合状態、ナノスケールの欠陥、ミクロスケールの粒界、マクロスケールの形状や応力状態が連鎖して決まる場合がある。したがって、単一スケールの特徴量ではなく、複数スケールの特徴量を併用し、どのスケールが支配的かを学習後に診断する設計が有効である。初学者には、表現を増やすこと自体が目的ではなく、物性の支配因子候補を入力へ反映することが目的である、と繰り返し強調すべきである。



### 4.4 予測モデルの選択

予測モデルは、表現$\phi(\mathbf{x})$で得た特徴量$\mathbf{z}$から出力$\mathbf{y}$を推定する関数$g_\theta$である。モデル選択は、表現の性質、データ量、ノイズ、外挿の必要性、解釈性の要求に応じて行うべきである。初学者は精度だけでモデルを選びがちであるが、材料研究では「どの条件で壊れるか」を把握しないと、探索で誤った結論に到達しうる。

線形モデルは、最も単純で解釈性が高い。スカラー出力$y$に対して
$$
\hat{y}=\mathbf{w}^\top \mathbf{z}+b
$$
と書け、$\mathbf{w}$の符号や大きさから寄与方向を読みやすい。ただし、相互作用が強い系や非線形性が支配的な系では表現不足になりやすい。例えば、元素間相互作用が二次以上で効く場合、線形では
$$
y \approx \sum_k w_k z_k
$$
のように足し合わせしかできず、相互作用項$z_k z_l$の効果を表現できない。これを補うには、二次項を追加するか、非線形モデルへ移る必要がある。

木系モデル（決定木、ランダムフォレスト、勾配ブースティング）は、非線形性と相互作用を自動的に拾いやすく、少量データでも強いことが多い。特徴量のスケーリングに鈍感で、欠損の扱いが比較的容易な場合もある。一方で、分割設計が甘いと系列の癖を覚えて高得点を出しやすく、外挿で破綻しやすい。したがって、第2章で述べた系列外しや化学空間分割と組み合わせて評価する必要がある。

カーネル法やガウス過程は、距離に基づく類似度を使うため、4.1で述べた距離設計がモデル挙動を決める。ガウス過程回帰では、予測は平均と分散を同時に出すことができ、探索や最適化と相性が良い。一般に
$$
\hat{y}(\mathbf{x})=\mu(\mathbf{x}),\qquad \sigma^2(\mathbf{x})=\text{Var}(y\mid \mathbf{x},\mathcal{D})
$$
が得られるため、不確かさ推定が自然に入る。ただし、データ数が増えると計算量が増大しやすく、表現設計が不適切だと不確かさが過信方向へ崩れることがある。

深層学習は表現力が高く、画像やスペクトルのような高次元データをそのまま扱える強みがある。構造をグラフとして扱い、原子をノード、結合や近接関係をエッジとする表現では、対称性に配慮したモデル設計が可能になる。ただし、材料データは一般にデータ数が少なく、ノイズやバイアスも大きいため、深層学習を使う場合は分割設計と正則化、不確かさ推定、物理制約の導入を併用しないと過信につながりやすい。初学者には、表現力が高いモデルほど「学習できてしまうものが多い」ため、評価設計が厳格でなければならないことを理解させる必要がある。



### 4.5 物性予測の検証設計

物性予測で重要なのは、同一系列の中で当たることより、新規領域で破綻しないことである。材料探索は新規領域へ踏み出す行為であり、評価が系列内補間に偏ると、探索での成功確率が見かけより低くなる。したがって、検証設計は研究の主張を定義する作業であり、単なる手続きではない。

検証は、少なくとも三つの観点で設計する必要がある。第一に、化学空間での汎化である。例えば、元素添加の効果を議論するなら、元素種をまたぐ分割（特定元素を含むデータを丸ごとテストへ回すなど）が必要になる。第二に、構造空間での汎化である。例えば、同じ化学式でも相が違う場合に当たるか、欠陥濃度が変わったときに当たるかを評価する。第三に、条件空間での汎化である。温度域、周波数域、応力域など、条件を外した評価を行い、外挿の兆候を確認する。

第2章で述べたように、分割設計の例を再掲すると、目的に応じて次が考えられる。

| 検証設計 | 研究で主張できる範囲 | 注意点 |
|---|---|---|
| ランダム分割 | 近傍補間に近い予測能力 | 系列データで過大評価になりやすい |
| バッチ単位分割 | バッチ差に頑健である | バッチ定義が曖昧だと崩れる |
| 元素・化学式外し | 未知材料群への汎化 | 誤差が増えるのは自然である |
| 相・構造外し | 未知構造への汎化 | 表現が対称性を保つ必要がある |
| 条件外し | 条件外挿への耐性 | 外挿は失敗しやすい前提で議論する |

さらに、評価指標も目的と整合させる必要がある。回帰ならMAEやRMSEがよく使われるが、探索では順位付けが重要なため、上位候補の再現率や順位相関が重要になることがある。不確かさを使うなら、被覆率や校正の指標が必要である。したがって、評価は一つの数値にまとめず、主張に直結する指標を複数提示する姿勢が望ましい。

最後に、検証結果は「どこまでなら信頼できるか」を示すための情報である。誤差が小さいことを誇るだけでは、材料研究としての価値は限定される。どの材料群、どの条件範囲、どの表現とモデルの組み合わせで、どの程度の精度と不確かさで使えるかを文章で明示し、適用範囲を結論の一部として提示する必要がある。



## 第5章 材料インフォマティクス II（探索と設計）

本章では、予測器を用いて材料を探索し、設計問題として解く枠組みを整理する。探索では、候補生成、制約処理、多目的性、意思決定、そして発見を設計指針へ変換する過程が重要になる。予測モデルを作ることがゴールに見えやすいが、材料研究では予測モデルは探索の部品であり、最終的な価値は実験や計算で検証可能な候補と、その背後にある再利用可能な指針である。



### 5.1 逆問題としての材料設計

目標物性$\mathbf{p}^\*$から設計変数$\mathbf{x}$を求める問題は、一般に一意ではない。順問題が$\mathbf{p}=f(\mathbf{x})$であるとき、逆問題は
$$
f(\mathbf{x})\approx \mathbf{p}^\*
$$
を満たす$\mathbf{x}$を探すことである。しかし、$f$が多対一であることは珍しくなく、同じ性能を持つ材料が複数存在しうる。さらに、測定誤差やモデル誤差もあるため、厳密に等しい解を求めるより、許容範囲$\delta$を導入して
$$
\|f(\mathbf{x})-\mathbf{p}^\*\|\le \delta
$$
を満たす解集合を扱う方が実用的である。

逆問題の基本形は、目的関数$J(\mathbf{x})$を定義して最大化（または最小化）することである。例えば単一目的なら
$$
\mathbf{x}^\*=\arg\max_{\mathbf{x}\in\Omega} J(\mathbf{x})
$$
と書ける。$\Omega$は実行可能領域であり、組成範囲、合成可能性、資源制約、安全制約、装置制約などを反映する。ここで重要なのは、$\Omega$を曖昧にしたまま最適化すると、現実には実行できない候補が多数生まれ、探索が空回りする点である。したがって、$\Omega$は探索開始時に可能な限り明示し、後から条件を追加した場合は、その理由と影響を記録する必要がある。

逆問題が難しい理由は、目的が複数であること、制約が多いこと、そして$f$が未知で近似に過ぎないことである。これらを同時に扱うため、設計は「解を一つ出す」よりも「候補集合を出す」問題として捉える方が学術的にも実務的にも自然である。候補集合を出す設計では、性能の良さだけでなく、候補同士の違い（多様性）と、どの制約をどの程度満たしているかを明示することが重要になる。



### 5.2 候補生成と多様性

候補生成では、良さそうな点へ収束するだけでなく、未探索領域を残すことで発見確率が上がる。これは、未知の材料空間では局所最適に陥りやすく、現在の知識に基づく最良が真の最良とは限らないためである。したがって、探索は「活用（exploitation）」と「探索（exploration）」のバランス問題になる。

このバランスを数理的に表す典型的な枠組みとして、獲得関数$\alpha(\mathbf{x})$を最大化して次点を選ぶ方法がある。予測平均$\mu(\mathbf{x})$と不確かさ$\sigma(\mathbf{x})$が得られるとき、例えば
$$
\alpha(\mathbf{x})=\mu(\mathbf{x})+\kappa \sigma(\mathbf{x})
$$
のように設計できる。$\kappa$が大きいほど不確かさの大きい点を優先し、探索側に寄る。$\kappa$が小さいほど平均性能の高い点を優先し、活用側に寄る。初学者にとって重要なのは、$\kappa$は機械学習のパラメータではなく、研究のリスク選好（確実に改善したいのか、新規発見を狙うのか）を反映する量であるという点である。

多様性は単なる散らばりではなく、物理的に成立する範囲での散らばりとして定義する必要がある。例えば、組成空間で遠い点を選ぶだけでは、相分離や脆性で実用にならない領域を大量に含む可能性がある。したがって、多様性は制約を満たす集合$\Omega$の内部で定義し、
$$
\text{候補集合 } \mathcal{X}=\{\mathbf{x}_1,\dots,\mathbf{x}_m\}\subset \Omega
$$
に対して、距離に基づく多様性指標（最小距離の最大化など）を用いることが考えられる。ただし、距離は4.1で述べたように仮説であるため、多様性の定義も同様に仮説である。

候補生成のもう一つの視点は、候補が実験・計算で検証可能な形で出ること、つまり「次の作業へ落ちる」ことにある。材料科学では、候補が抽象的な連続変数で出ても、実際には組成刻み、熱処理温度刻み、設備制約がある。したがって、候補生成では、離散化や実行可能なパラメータ化を組み込み、生成された候補がそのまま実験計画へ移れるようにする必要がある。



### 5.3 制約付き探索

合成可能性、毒性、資源制約、機械特性などの制約は探索に不可欠である。制約を後付けでふるい落とすと、計算資源や実験資源が無駄になりやすい。したがって、探索の最初から制約を数理的に組み込み、無駄な候補生成を減らす設計が必要である。

制約は、等式制約と不等式制約に分けて書ける。一般に
$$
\text{maximize } J(\mathbf{x}) \quad \text{subject to }\quad g_j(\mathbf{x})\le 0,\; h_k(\mathbf{x})=0,\; \mathbf{x}\in\Omega_0
$$
のように定式化する。$\Omega_0$は基本的な境界（例えば濃度範囲）である。$g_j$は不等式制約（毒性が閾値以下、コストが上限以下など）、$h_k$は等式制約（成分和が1、電荷中性など）である。

材料探索で重要なのは、制約の中には「物理法則として絶対」のものと、「研究目的や用途として相対」のものが混在する点である。例えば、成分和$\sum c_k=1$は絶対である。一方、コスト上限や毒性閾値は用途に依存し、研究段階では緩く設定して探索の幅を確保したいこともある。したがって、制約は硬い制約（必ず満たす）と柔らかい制約（ペナルティとして扱う）に分けると設計しやすい。

柔らかい制約は、目的関数にペナルティを加えて
$$
J'(\mathbf{x})=J(\mathbf{x})-\sum_j \beta_j \max(0,g_j(\mathbf{x}))^2
$$
のように扱える。$\beta_j$は制約違反をどれだけ嫌うかを決める係数であり、これも研究上の選好を表す。硬い制約は探索候補集合を$\Omega$として先に絞り、
$$
\Omega=\{\mathbf{x}\in\Omega_0\mid g_j(\mathbf{x})\le 0\;\forall j\}
$$
の中で探索する。初学者には、制約を後段で落とす方式は「見かけの候補数を増やすが有効候補を減らす」ことが多い点を説明し、制約は探索の前提として設計する姿勢を教える必要がある。



### 5.4 多目的探索の意思決定

多目的では、最終的にどの候補を選ぶかが研究の主張になる。第3章で述べたパレート前線は候補集合を与えるが、そこから一点を選ぶのは意思決定であり、材料科学・工学の判断が必要になる。重要なのは、重み付けを固定して機械的に一点を選ぶのではなく、候補集合とトレードオフを提示し、選択理由を説明できる形にすることである。

多目的の形式は、目的ベクトル$\mathbf{p}(\mathbf{x})$を最大化する問題であり、パレート集合$\mathcal{P}$を求めることに対応する。意思決定は、$\mathcal{P}$の上で追加の選好や制約を導入して候補を絞る作業である。例えば、用途上どうしても下限がある目的があれば、不等式制約として
$$
p_k(\mathbf{x})\ge p_k^\mathrm{min}
$$
を課し、満たす候補だけを残す。こうすると、選好が「下限を満たすこと」で明示され、議論が透明になる。

一方、用途が複数ある、あるいは研究段階で用途を決めきれない場合は、複数の意思決定シナリオを提示するのが学術的に有効である。例えば、低損失重視、低コスト重視、高温安定性重視など、異なる要求を置いたときに、パレート集合のどの領域が選ばれるかを示す。これにより、探索結果が用途によってどう変わるかが明確になり、研究の再利用性が高まる。



### 5.5 発見から指針への変換

探索が当たったという事実だけでは、次の研究へ接続しにくい。なぜその領域が良いのかを要因候補に分解し、再利用可能な材料設計指針へ変換する必要がある。指針とは、単なる経験則ではなく、少なくとも「どの変数がどの方向に効くか」「どの範囲で成立するか」「どの例外があるか」を含む形で記述された知識である。

変換の基本は、(i)表現空間での規則性の抽出、(ii)感度解析による寄与方向の推定、(iii)物理・化学の既知知見との整合確認、である。例えば、学習器の予測$\hat{y}$に対して感度$\partial \hat{y}/\partial z_k$を評価し、どの特徴量が支配的かを調べる。局所的には
$$
\Delta \hat{y}\approx \sum_k \frac{\partial \hat{y}}{\partial z_k}\Delta z_k
$$
と近似でき、$\Delta z_k$を物理量（元素特性平均、局所配位数、格子歪みなど）に対応づけることで、設計変数の変更がどの方向に効くかを議論できる。

ただし、ここでも因果と相関の区別が必要である。指針として主張するには、追加の検証が必要である。例えば、指針が「ある元素添加で局所歪み分布が広がることが損失低減に寄与する」という形で得られたなら、別の元素や別のプロセスでも同様の歪み分布制御で同じ傾向が出るかを検証する。複数系で再現できれば指針の一般性が高まる。再現できない場合は、どの条件で成立しないかを同時に記述することで、指針としての信頼性が上がる。

最終的に、探索は材料名の発見だけで終えるのではなく、材料科学としての理解へ接続する必要がある。発見を指針へ変換する作業は、次章以降で扱う計測・プロセス・物理・化学インフォマティクスとも強く結びつく。例えば、計測で得た局所状態の同定が指針の根拠になり、物理制約が指針の適用範囲を定め、化学・プロセスが実行可能性を保証する。したがって、本章で得た候補や規則性は、以後の章で扱う手法と往復しながら、学術的主張として固めていくべきである。


## 第6章 計測インフォマティクス I（信号処理と特徴抽出）

本章では、計測データを材料科学の情報へ変換するための信号処理と特徴抽出を扱う。計測インフォマティクスで最も重要なのは、データが物理現象だけでなく装置と解析手順の影響も強く受けるという事実を、最初から数理的に組み込む姿勢である。フィルタや正規化は単なる処理に見えるが、材料研究では処理が観測結果を変形し、結論の再現性や比較可能性に直結するため、処理の目的と限界を言語化する必要がある。



### 6.1 計測データが持つ二重性

計測信号は、測りたい物理量の反映であると同時に、装置応答と解析手順の反映でもある。これを最も簡潔に書くと、観測信号$y(t)$は真の物理過程$s(t)$が装置の伝達関数$h(t)$で畳み込まれ、さらにノイズ$n(t)$が加わったものとして
$$
y(t)=(h*s)(t)+n(t)=\int h(\tau)s(t-\tau)\,d\tau+n(t)
$$
と表せる。スペクトルや画像でも本質は同じであり、観測は「真の現象」と「観測モデル」の合成である。この式の意味は、たとえ同じ試料を測っても、装置の分解能や検出器の応答が違えば、得られるデータの形が変わるということである。

さらに、解析手順も観測結果に影響する。例えば、バックグラウンド除去、正規化、平滑化、微分などの操作は、データを別の関数へ写像する操作であり、元データの情報を保存する操作ではない場合がある。したがって、計測データは生データ（raw）と処理後データ（processed）を対として保持し、処理条件を追跡可能にしておく必要がある。これは第2章の追跡可能性の議論が、計測分野では特に強く要求されることを意味する。

計測データの二重性を意識すると、研究上の主張の仕方も変わる。例えば、ある処理後スペクトルのピークが大きいと主張する場合、そのピークが物理現象によるのか、装置応答や処理によって強調されたのかを区別する必要がある。区別できないなら、「処理後スペクトルにおいて」という条件を明示し、物理量への直接対応を主張しない態度が必要である。

また、二重性は比較の設計にも直結する。異なる装置や異なる測定日で取得したデータを比較するなら、装置応答の違いを補正するか、あるいは同一装置条件で再測定して比較の前提を揃える必要がある。補正を行う場合でも、補正が「物理差」を消していないかを検証しなければならない。したがって、計測インフォマティクスでは、データ処理は物理理解の一部であり、単なる数値操作ではない。



### 6.2 スペクトルの前処理設計

スペクトル（XRD、XAS、XMCD、ラマン、赤外など）の前処理は、特徴を抽出しやすくするために不可欠である。一方で、前処理は情報の欠落や歪みも引き起こしうるため、目的に従って設計し、処理が物理量の推定に必要なのか、解釈を歪める可能性があるのかを記述する必要がある。

スペクトルを$y(E)$（$E$はエネルギーや角度）とし、背景$b(E)$が加わっていると考えると
$$
y(E)=s(E)+b(E)+n(E)
$$
と書ける。背景除去は$b(E)$を推定して引く操作であるが、$b(E)$をどの関数形で近似するかにより、$s(E)$の形が変わる。例えば、滑らかな多項式で背景を近似する場合、多項式次数を上げすぎると、本来のゆるやかな物理信号まで背景として消してしまう危険がある。逆に次数が低すぎると、背景が残ってピーク強度が過大評価される。したがって、背景除去は「正しい答えが一つ」ではなく、どの成分を物理信号とみなすかの仮説である。

正規化も同様に重要である。例えば、前縁（pre-edge）と後縁（post-edge）の平均値を用いてスケールを揃える正規化は、試料厚みや検出効率の違いを比較可能にする一方で、絶対強度情報を失う。したがって、絶対強度が物理量（例えば吸収係数の絶対値）に対応する場合は、正規化の段階でその情報を捨てていないかを検討する必要がある。逆に、形状比較（ピーク位置や相対強度）が目的なら、正規化は比較を支える基本操作になる。

平滑化（スムージング）はノイズを減らし、ピーク検出や特徴抽出を安定化させる。しかし、平滑化は高周波成分を削る操作であり、分解能限界に近い微細構造を消す可能性がある。周波数領域で考えると、平滑化はフィルタ$H(\omega)$をかける操作であり、
$$
Y_\mathrm{smooth}(\omega)=H(\omega)Y(\omega)
$$
となる。ここで$|H(\omega)|<1$の領域が広いほどノイズは減るが、同時に信号も失う。したがって、平滑化は「ノイズ低減」と「情報保持」のトレードオフであり、目的に応じて窓幅やフィルタ形を決める必要がある。

微分は特徴を強調する一方で、ノイズも増幅する。離散データ$y_i=y(E_i)$に対して一次差分
$$
\frac{dy}{dE}\Big|_{E_i}\approx \frac{y_{i+1}-y_{i-1}}{E_{i+1}-E_{i-1}}
$$
を取ると、ゆるやかな背景は抑えられるが、測定ノイズが大きいと差分が不安定になる。したがって、微分を使う場合は、先に適切な平滑化を行うか、微分と平滑化を同時に行う手法（例えばSavitzky–Golay型の発想）を採る必要がある。微分は特徴抽出の強力な道具であるが、処理の意味を理解せずに使うと、物理の差ではなくノイズの差を学習させる危険がある。

前処理の設計では、処理の目的を明確にし、処理後に何を特徴として使うかまで含めて一貫した設計にする必要がある。例えば、ピーク位置を物理量（格子定数、化学シフト）として解釈したいなら、エネルギー校正や角度校正の不確かさを見積もり、ピーク位置の誤差がどの程度かを示すべきである。逆に、クラスタリングで形状を分類したいなら、絶対位置の校正誤差が分類を支配しないように、位置合わせや規格化を検討する必要がある。



### 6.3 画像特徴と形態学

顕微鏡像やマッピングデータ（光学顕微鏡、SEM、TEM、MFM、MOKE、EBSDなど）では、磁区幅、粒径、欠陥密度、相分布が物性と結びつく場合が多い。画像は情報量が大きいが、そのままでは比較が難しいため、特徴抽出が重要になる。特徴抽出は大きく、セグメンテーションに基づく形態学的特徴と、セグメンテーションを介さない統計的特徴に分けられる。

セグメンテーションとは、画像$I(\mathbf{r})$（$\mathbf{r}$は画素位置）を領域に分割し、例えば相Aと相B、磁区の上向きと下向きなどのラベルを割り当てる操作である。二値化の最も簡単な形は閾値$\tau$を用いて
$$
M(\mathbf{r})=
\begin{cases}
1 & I(\mathbf{r})\ge \tau\\
0 & I(\mathbf{r})<\tau
\end{cases}
$$
とすることである。しかし、閾値$\tau$はコントラストや照明条件に依存しやすく、倍率や撮像条件が変わると同じ物理状態でも$I(\mathbf{r})$の分布が変わる。したがって、閾値を固定する設計は比較可能性を損なう可能性がある。ここで必要なのは、画像取得条件（倍率、露光、ゲイン、照明）を統制し、可能なら校正用の基準試料で条件差を評価することである。

形態学的特徴としては、領域の面積$A$、周長$P$、円形度$4\pi A/P^2$、配向、連結度、最近接距離などが基本になる。例えば粒径評価では、領域の等価円直径$d_\mathrm{eq}=2\sqrt{A/\pi}$を用い、粒径分布として平均と分散を特徴量にすることが多い。磁区幅評価では、自己相関関数
$$
C(\Delta \mathbf{r})=\langle I(\mathbf{r})I(\mathbf{r}+\Delta \mathbf{r})\rangle_{\mathbf{r}}
$$
やフーリエスペクトルのピークから代表長さを推定する方法もある。これらはセグメンテーションに依存しにくく、コントラストの違いに比較的頑健な場合がある。

ただし、画像特徴は物理情報だけでなく、撮像条件の差に強く影響される。倍率が変われば画素当たりの実空間長さが変わり、同じ粒径でも見え方が変わる。コントラスト調整は領域分割を変え、ノイズ除去は微細欠陥を消す可能性がある。したがって、画像特徴を比較する場合は、少なくともスケール校正（画素サイズ）、コントラスト処理、ノイズ処理の条件を統一し、処理手順を記録して追跡可能にする必要がある。

初学者には、画像特徴が「見た目の説明」ではなく、「物性と結びつく量へ落とす作業」であることを教える必要がある。例えば、磁区幅が損失と相関するという主張をするなら、磁区幅をどう定義し、どう測り、どの不確かさがあるかを示す必要がある。定義が曖昧なまま特徴を使うと、学習器は特徴を通じて装置差や処理差を学んでしまい、物理の主張が弱くなる。



### 6.4 時系列データと非定常性

温度掃引、磁場掃引、緩和現象、パルス応答などは非定常であり、固定窓の統計量だけでは機構の差を捉えにくい。非定常とは、平均や分散などの統計量が時間とともに変化することであり、同じデータでも観測区間によって見える性質が変わる。したがって、時系列データは「一つの分布」として扱うのではなく、「状態が遷移する過程」として扱う必要がある。

時系列$y(t)$の最も基本的な分解は、トレンドと変動である。例えば
$$
y(t)=m(t)+\epsilon(t)
$$
と書き、$m(t)$がゆっくり変わる成分、$\epsilon(t)$が揺らぎである。温度掃引中の磁化$M(T)$を時間系列とみなすと、$m$は平衡磁化曲線に相当し、$\epsilon$はノイズや非平衡揺らぎに相当する。固定窓平均だけで特徴を作ると、遷移点や変化点をぼかしてしまうことがあるため、変化点（change point）やセグメント分割を導入して、区間ごとに特徴を抽出する設計が有効になる。

状態空間的な表現も重要である。観測$y(t)$の背後に隠れ状態$x(t)$があり、
$$
x(t+1)=F(x(t),u(t))+\eta(t),\qquad y(t)=G(x(t))+\xi(t)
$$
と書けると考えると、$x(t)$の推定は機構の推定に直結する。ここで$u(t)$は外場や制御入力であり、温度掃引や磁場掃引のプロトコルが含まれる。計測インフォマティクスでは、測定手順の切り替え（例えば掃引速度変更、磁場方向変更、励起波形変更）をメタデータとして与え、モデルが「いつ条件が変わったか」を知った上で推定できるようにすることが望ましい。

周波数解析の観点からも非定常性は重要である。定常信号ならフーリエ変換で周波数成分を議論できるが、非定常では周波数成分が時間とともに変わる。したがって、時間局所的な周波数特徴（時間周波数表現）を用いて、どのタイミングでどの周波数帯が支配的かを特徴量にすることが考えられる。重要なのは、時間窓をどう選ぶかが物理仮説になる点である。窓が長すぎると変化が平均化され、短すぎると推定が不安定になる。ここでもトレードオフを理解し、対象現象の時間スケールを踏まえて設計する必要がある。



### 6.5 特徴抽出の検証

特徴抽出は、予測精度だけでなく、再現性と解釈可能性で評価すべきである。再現性とは、異なる装置や別日に取得したデータでも同等の特徴が得られることであり、解釈可能性とは、その特徴が物理的意味と一貫して結びつくことである。材料研究では、特徴が安定していないと、モデルの結論が「測定条件の差」を反映してしまい、学術的主張として弱くなる。

再現性の確認には、同一試料の反復測定が基本である。反復測定で得た特徴量$z$の分散$s^2$が、材料間差$\Delta z$に比べて十分小さいかを確認する。例えば、特徴量の信号対雑音比を
$$
\mathrm{SNR}=\frac{\mathrm{Var}_\mathrm{between}(z)}{\mathrm{Var}_\mathrm{within}(z)}
$$
のように定義すると、$\mathrm{SNR}$が小さい特徴は材料差を捉える前に測定揺らぎに埋もれている可能性が高い。もちろん実際の定義は状況依存であるが、考え方として「特徴の揺らぎ」と「材料差」の比を意識することが重要である。

解釈可能性の検証では、特徴が既知の物理関係と整合するかを確認する。例えば、XRDピーク位置から格子定数を推定する場合、ブラーグ条件
$$
2d\sin\theta=n\lambda
$$
から$d$を導き、結晶系に応じて格子定数へ変換する。特徴抽出がこの物理関係と矛盾するなら、前処理やピークフィットの設計が不適切である可能性が高い。ここで重要なのは、特徴抽出は学習器の前段であるため、前段の誤りは後段のモデル全体の誤りになる点である。

最後に、特徴抽出の検証は、外部データや異なる手段によるクロスチェックで強くなる。例えば、分光で推定した価数状態が、別の計測（電気抵抗や磁化）と整合するかを確認する。整合すれば特徴の意味が強化され、整合しなければ「何を特徴として抽出しているのか」を見直す必要がある。計測インフォマティクスにおいては、特徴抽出はデータ駆動と物理解釈の接点であり、ここを曖昧にすると研究全体の説得力が落ちる。



## 第7章 計測インフォマティクス II（同定・推定・融合）

本章では、計測データから相や状態を同定し、物理量を推定し、複数の計測モダリティを融合して機構理解へ接続する枠組みを扱う。同定や推定は、単に分類や回帰を行うだけでなく、教師信号の定義、観測モデルの明示、装置差の扱い、不確かさの提示まで含めて設計する必要がある。同定結果の数字が出た時点で結論が得られたように見えるが、材料研究では「その数字が何を意味し、どの条件で成立するか」を同時に示して初めて学術的主張となる。



### 7.1 相同定と状態推定

回折や分光から相や状態を推定する問題は、教師信号（ラベル）の定義が結果を左右する。例えば、XRDパターンから相を同定する場合、相は一つに決まるとは限らず、相混在、配向、歪み、微結晶化によりパターンは連続的に変化する。したがって、ラベルを「相A」「相B」のように離散的に切ると、境界領域のデータが曖昧になり、学習器は境界の扱いで不安定になる。

この問題を整理するため、相や状態を確率として扱う立場がある。例えば、相集合$\{1,\dots,K\}$に対して
$$
\mathbf{q}=(q_1,\dots,q_K),\qquad q_k\ge 0,\quad \sum_k q_k=1
$$
を相分率の推定値として出力し、点ラベルではなく分布として状態を表す。これは相混在を自然に表現でき、境界領域の曖昧さも含めて扱える。一方、教師データとして真の$\mathbf{q}$を用意するのは難しいため、専門家の推定や別手段の定量（リートベルト解析など）と整合させる設計が必要になる。

専門家のラベル付けを用いる場合は、合意形成の手続きを記録し、ラベルの不確かさをモデル側で扱う設計が重要である。例えば、複数専門家の判断が割れるなら、その分散自体をラベル不確かさとして記録し、損失関数に反映することが考えられる。ラベルの不確かさを無視すると、モデルは矛盾した教師信号を無理に合わせようとして過学習し、外部データで破綻しやすくなる。

状態推定でも同様の問題がある。例えば、酸化状態、スピン状態、磁気秩序状態などは連続量や混合状態として現れる場合がある。離散分類に落とすのか、連続推定にするのか、混合比として扱うのかは、物理的定義と測定分解能に依存する。ここで重要なのは、推定結果が「測定が区別できる範囲」の主張であるという点である。区別できないものを区別したように見せる推定は、学術的に不適切である。



### 7.2 物理モデルと学習の併用

計測の解釈には、線形結合モデル、選択則、分解能、検出器応答などの観測モデルが関与する。観測モデルを明示した上で学習を用いると、外挿での破綻が減り、推定結果を物理量として報告しやすくなる。ここでは、物理モデルと学習の併用を「観測モデルを挟んだ推定」として整理する。

最も基本的な形は線形混合である。例えばスペクトル$y(E)$が基底スペクトル$s_k(E)$の混合だとすると
$$
y(E)\approx \sum_{k=1}^{K} a_k s_k(E)+b(E)
$$
と書ける。$a_k$は混合係数であり、相分率や状態比に対応づけられることがある。この形を前提にすると、学習は$s_k(E)$の推定や、$a_k$の推定を補助する役割になる。ここで重要なのは、混合が線形で成り立つかどうかは物理仮説であり、自己吸収や飽和、非線形応答がある場合は破れる可能性がある点である。仮説が破れるなら、その範囲を明示して適用範囲を限定する必要がある。

物理モデルを併用するもう一つの利点は、推定対象を物理量にすることである。例えば、ピーク位置や面積を直接推定するのではなく、それらが従う物理関係（例えばブラーグ条件、選択則、遷移確率）を介して推定する。これにより、推定結果が装置差に左右されにくくなり、報告が物理量として一貫する。

数理的には、真の物理量$\theta$（例：格子定数、価数、磁気モーメント）と観測$y$の関係を
$$
y=g(\theta)+n
$$
と置き、$\theta$を推定する問題として扱う。学習は$g$の近似や逆写像の近似として用いられるが、$g$の物理的制約（単調性、範囲、対称性など）を組み込むことで、外挿での破綻を抑えられる。初学者には、学習を「物理モデルの代替」と捉えるのではなく、「物理モデルの不足部分を補う道具」と捉える方が、安全で学術的に強いことを教えるべきである。



### 7.3 装置差・ロット差の扱い

装置や試料ロットの違いは、系統的ずれとして現れることが多い。例えば、エネルギー軸の微小なずれ、分解能の違い、感度の違い、背景形状の違い、試料厚みや表面状態の違いなどが、同じ物理状態でもデータを変える。これらの差を無視すると、学習器は物理差ではなく装置差を識別してしまい、他装置データで破綻する。

一方で、ずれを消すことだけを目的にすると、物理差まで消す可能性がある。例えば、ピーク位置のずれを一律に補正すると、本来の化学シフトを消してしまう危険がある。したがって、どこまでを同一視するのかを物理的に説明できる範囲で制限する必要がある。具体的には、補正対象を「装置由来であると独立に確認できる量」に限定し、物理由来である可能性がある量は補正せず、モデル側で扱う設計が望ましい。

装置差はメタデータとして明示するのが基本である。装置ID、検出器条件、分解能、測定日、操作者などを記録し、必要ならそれらを入力としてモデルに与える。これにより、モデルは「この装置ではこう見える」という補正を内部で学習できる。ただし、この方法は装置差を学習する方向へ寄りすぎる危険もあるため、評価では装置外し（ある装置のデータを丸ごとテストにする）を行い、物理汎化ができているかを確認すべきである。

ロット差も同様である。特にプロセス起源の微細組織差が強い場合、ロット差は物理差そのものであることがある。したがって、ロット差を単なるノイズとして除去するのではなく、ロット差が何に由来するか（組成微差、熱履歴、応力、欠陥密度）を推定し、材料設計へ接続する視点が重要になる。計測インフォマティクスは、ロット差を「邪魔」として扱うだけでなく、プロセス・材料へ戻す手がかりとして扱うことができる。



### 7.4 マルチモーダル融合

画像、スペクトル、組成、プロセス条件を結合すると、単独データでは識別できない機構差が見える場合がある。例えば、同じ相に見える試料でも、微細組織像と分光を合わせると局所状態が違うことが分かり、損失や耐久性の差を説明できることがある。融合は情報を足すだけでなく、時間同期や空間対応を確立してはじめて意味を持つため、対応付けの設計が重要である。

融合の数理的な考え方として、モダリティ$m=1,\dots,M$の観測$\mathbf{y}^{(m)}$が、共通の潜在状態$\mathbf{z}$を介して生成されると考える立場がある。
$$
\mathbf{y}^{(m)} = g_m(\mathbf{z}) + \boldsymbol{\epsilon}^{(m)}
$$
ここで$\mathbf{z}$は試料の状態（相分率、欠陥状態、局所電子状態など）を表し、$g_m$は各計測の観測モデルである。融合の目的は、複数の$\mathbf{y}^{(m)}$から$\mathbf{z}$をより確からしく推定することである。この立場に立つと、融合は「データを結合する」作業ではなく、「共通状態を同定する」作業になる。

実験現場で重要なのは、空間対応と時間対応である。例えば、同一位置を測ったつもりでも、顕微鏡像の視野と分光のビーム位置がずれていれば、別の領域を比較してしまう。時間的にも、測定順序の違いで酸化や緩和が進むと、同一試料でも状態が変わる。したがって、融合を行うなら、位置合わせ、座標変換、時間ログの記録を含めてデータ設計に組み込む必要がある。対応付けが曖昧な融合は、情報が増えるのではなく、誤差源が増えるだけになりうる。

融合の利点は、推定の不確かさが下がる点にもある。異なる観測が同じ$\mathbf{z}$を支持するなら、推定は強化される。逆に、観測間で矛盾が生じるなら、どこかにモデルの不足や測定の問題がある可能性が示唆される。矛盾は悪いことではなく、機構理解や測定改善の手がかりになる。したがって、融合では「一致させる」こと自体を目的にせず、矛盾を検出し説明する姿勢が重要である。



### 7.5 推定結果の提示方法

推定結果は点推定だけでは不十分であり、不確かさと条件依存性を同時に提示する必要がある。材料研究では、推定値が一つでも、それがどの条件範囲で妥当かが分からなければ、第三者は再利用できない。したがって、推定結果の提示は研究主張の範囲を定義する作業である。

不確かさの提示では、少なくとも「測定ノイズ」「モデル不確かさ」「分布ずれ」の可能性を区別して議論するのが望ましい。例えば、推定値$\hat{\theta}$に対して区間推定を
$$
\theta \in [\hat{\theta}-\Delta,\,\hat{\theta}+\Delta]
$$
と提示する場合、$\Delta$がどの不確かさを含むのかを明記する必要がある。測定の繰り返しから得た分散を含むのか、モデルのばらつきを含むのか、未知領域での外挿リスクを含むのかで、同じ$\Delta$でも意味が違う。

条件依存性の提示では、推定が成立する入力範囲を文章で記述する必要がある。例えば、装置条件（分解能、エネルギー範囲）、試料条件（厚み、表面処理）、環境条件（温度、磁場）を明示し、その範囲外では推定を保証しないことを示す。これは、研究を弱くするのではなく、主張を正確にする行為である。

また、推定結果の提示では、物理量への対応関係を明示することが重要である。例えば、スペクトル特徴量から酸化状態を推定したなら、その推定がどの物理モデル（化学シフト、選択則、基底スペクトル混合）に基づくのかを示し、推定がモデル仮定に依存することを明確にする。仮定に依存することを隠すと、結果が普遍的であるかのように誤解され、後の研究で誤用されやすくなる。

最後に、推定結果は材料インフォマティクスへ接続されるべきである。推定は計測のゴールではなく、材料の表現や設計指針へ戻すための情報である。したがって、推定結果を「数値の一覧」として提示するだけでなく、どの特徴が材料のどの性質と結びつくか、どの設計変数を動かすと推定状態が変わるか、といった因果候補まで含めて示すと、研究全体の再利用性が高まる。


## 第8章 プロセスインフォマティクス I（条件空間と最適化）

本章では、熱処理、成膜、焼結、加工、合成といった材料プロセスを、データと数理の観点から設計し直すための考え方を扱う。プロセスは材料インフォマティクスと異なり、時間順序、工程の分岐、装置状態、外乱などが結果に強く影響し、同じ条件でも出力が揺らぐことが多い。したがって、条件を単なる入力ベクトルとして扱うのではなく、工程の構造を保った表現と、ばらつきを含めた最適化を同時に設計する必要がある。



### 8.1 プロセス条件の表現

熱処理や成膜条件は、連続と離散が混在し、制約も多い。例えば熱処理なら、温度$T$、保持時間$t$、昇温速度$r=\mathrm{d}T/\mathrm{d}t$、雰囲気（Ar、真空、反応性ガスなど）、圧力$P$があり、さらに工程が複数段（予備焼鈍→本焼鈍→急冷など）に分かれる。成膜なら、出力$P_\mathrm{power}$、圧力、ガス流量、基板温度、バイアス、ターゲット組成、膜厚、成膜順序といった要素が入る。これらは、単に数値を並べただけでは工程の意味が失われることがある。

工程の本質は、時間と順序である。熱処理プロファイルは関数$T(t)$として表せるが、実データでは離散点列$\{(t_i,T_i)\}$で与えられることが多い。ここで、特徴量として平均温度や最高温度だけを使うと、昇温・降温の違いや保持時間の違いを落としてしまい、同じ特徴量でも異なる組織が得られる状況を説明できない。したがって、プロファイルを表すために、(i)区分的表現（ステップごとの温度と時間）、(ii)基底展開（例えば多項式やスプライン係数）、(iii)物理量へ要約（拡散距離や反応進行度）といった表現を検討する。

物理量へ要約する例として、拡散が支配的な現象では、拡散長$L$が
$$
L \sim \sqrt{D\,t}
$$
で与えられることを用い、温度依存拡散係数$D(T)=D_0\exp(-Q/(k_\mathrm{B}T))$を仮定すると、非等温プロファイル$T(t)$に対して有効な拡散尺度を
$$
\mathcal{D}=\int_0^{t_\mathrm{end}} D(T(t))\,dt
$$
のように定義できる。$\mathcal{D}$は工程全体の「拡散の進みやすさ」を表す要約量であり、昇温速度や保持時間の影響をまとめて扱える。ただし、これは拡散が支配的であるという仮説に依存するため、対象現象（析出、結晶化、相変態、応力緩和など）がどの機構で支配されるかに合わせて要約量を設計する必要がある。

工程の階層性も重要である。例えば、成膜→アニール→評価というパイプラインでは、成膜条件が膜質を決め、アニール条件が相と欠陥を変え、評価が性能を測る。これを単一ベクトルに潰すと、どの工程が支配的かが分からなくなる。したがって、工程ごとに入力ブロックを分け、工程間の依存（成膜結果がアニール結果に影響する）をモデルに反映できる構造を保つことが望ましい。初学者には、プロセス表現は「条件を並べる」のではなく、「工程の構造を残す」ことが核心であると理解させる必要がある。



### 8.2 ノイズと再現性の設計

プロセスは外乱や装置状態に影響されやすく、同一条件でも結果が揺らぐ。例えば、炉内温度の空間分布、ガス純度、ターゲット表面状態、基板位置、真空到達度、チャンバー汚染、熱接触、計測タイミングなどが、物性のばらつきとして現れる。ここで重要なのは、揺らぎを単なる誤差として捨てるのではなく、再現性の推定対象として扱うことである。研究段階では平均性能だけが注目されやすいが、工学へ接続するにはばらつきが支配的なことが多い。

ばらつきを数理的に表すには、同一条件$\mathbf{x}$に対する出力$y$を確率変数として扱い、
$$
y \mid \mathbf{x} \sim \mathcal{N}(\mu(\mathbf{x}),\,\sigma^2(\mathbf{x}))
$$
のように平均$\mu(\mathbf{x})$と分散$\sigma^2(\mathbf{x})$の両方をモデル化する考え方がある。ここで$\sigma(\mathbf{x})$が大きい条件は再現性が低く、たまたま高性能が出ても安定した製造や検証に結びつきにくい。逆に、$\mu$が少し低くても$\sigma$が小さい条件は、安定に性能を得られる可能性が高い。したがって、平均性能とばらつきを同時に最適化する設計が重要である。

この考え方は目的関数にも反映できる。例えば、性能を最大化しつつばらつきを抑えたい場合、単純には
$$
J(\mathbf{x})=\mu(\mathbf{x})-\beta\,\sigma(\mathbf{x})
$$
のように設計できる。$\beta$は再現性をどれだけ重視するかを表す係数であり、研究段階では小さく、製造段階では大きくする、といった運用が考えられる。ただし、$\sigma(\mathbf{x})$の推定には反復実験が必要であり、反復回数が少ないと不確かさが大きくなる。したがって、どの条件で反復を行うか自体が実験設計の問題になる。

再現性の設計では、プロセス条件だけでなく、装置状態をメタデータとして記録することが重要である。装置状態が隠れ変数として効くなら、$\mathbf{x}$が同じでも実際には別の入力になっている。例えば、チャンバー清掃後か、ターゲット交換後か、真空到達時間が長いか短いか、といった情報が結果に影響するなら、それを記録して初めて、ばらつきの原因を分解できる。初学者には、ばらつきは敵ではなく、支配因子を特定するための情報でもあると教える必要がある。



### 8.3 制約付き最適化

安全、コスト、設備制約、材料の入手性などが探索を制限する。制約を明示して最適化に組み込むことで、実行不可能な提案を減らし、探索の効率と説明力を同時に上げられる。材料探索では、性能が良くても実行できない条件は研究の進展に寄与しないため、制約は後付けではなく設計の一部として扱うべきである。

制約付き最適化は一般に
$$
\text{maximize } J(\mathbf{x}) \quad \text{subject to}\quad g_j(\mathbf{x})\le 0,\; h_k(\mathbf{x})=0,\; \mathbf{x}\in\Omega_0
$$
で表される。プロセスの場合、$\mathbf{x}$には連続変数と離散変数が混在する。例えば、雰囲気ガス種は離散、温度は連続、工程数は可変といった具合である。この混在は、単純な連続最適化手法では扱いにくく、表現と探索戦略を工夫する必要がある。

制約の扱い方は、硬い制約と柔らかい制約に分けると設計しやすい。硬い制約は安全や設備限界のように必ず守るべきものであり、探索領域を
$$
\Omega=\{\mathbf{x}\in\Omega_0\mid g_j(\mathbf{x})\le 0\;\forall j\}
$$
として先に絞る。柔らかい制約はコストや時間のようにトレードオフ可能なものとして、目的関数にペナルティを入れて
$$
J'(\mathbf{x}) = J(\mathbf{x})-\sum_j \beta_j \max(0,g_j(\mathbf{x}))^2
$$
のように扱える。ここで$\beta_j$は重みであり、研究目的に応じて設定する。重要なのは、制約の種類と扱いを明示しないと、探索結果が何に従って選ばれたのか説明できなくなる点である。

プロセスでは「実行可能性」が特に重要である。例えば、温度は設定値が同じでも、炉の立ち上げ時間や温度安定化時間が異なると実行時間が変わる。設備利用時間が限られている場合、実行時間も制約になる。したがって、制約は単に物理的上限下限だけでなく、運用制約（時間、段取り、メンテナンス）も含めてモデル化する必要がある。初学者には、制約は探索を狭めるためのものではなく、探索を現実へ接続するための言語であると理解させる必要がある。



### 8.4 多目的プロセス設計

歩留まりや均一性が重要な場合、最良値だけ追うと製造に結びつかない。例えば、ある条件で最大性能が出ても、膜厚ムラが大きい、欠陥が多い、再現性が悪いなら、工学的価値は低い。研究段階から工学的要請へ接続するには、目的を複数化し、パレート性で候補群を扱う必要がある。

多目的最適化では、目的ベクトル$\mathbf{p}(\mathbf{x})=(p_1(\mathbf{x}),\dots,p_M(\mathbf{x}))$を最大化し、パレート集合を求める。例えば、性能$p_1$、均一性$p_2$、再現性$p_3$、コスト$p_4$を考えると、単一指標に潰すと重みが暗黙化し、議論が不透明になる。パレート前線として候補群を提示すれば、性能を少し下げると均一性や再現性が大きく改善する、といったトレードオフが明示できる。

多目的設計の利点は、意思決定を後段へ分離できる点にもある。研究段階では候補群を提示し、用途や制約が確定した段階で選択を行う方が合理的である。例えば、量産を視野に入れるなら均一性と再現性を強く重視し、探索の早期段階では性能を幅広く見る、といった戦略が取れる。初学者には、最適化は「最良点を一つ出す」ことではなく、条件と目的が変わる現実に対応できる候補集合を用意することだと教える必要がある。



### 8.5 因果推論の導入

条件と結果の相関だけでは、操作変数が本当に効いているかが確定しない。プロセスでは、装置状態や環境が交絡し、見かけの相関が生まれやすい。例えば、成膜圧力を変えたら性能が変わったとしても、実際にはチャンバー汚染状態が同時に変化していたかもしれない。このような交絡があると、相関に基づく最適化は再現できず、条件を変えても同じ改善が得られない。

因果の基本は、介入の効果を定義することである。操作変数$X$を変えたときの出力$Y$の変化を、因果効果として$\Delta Y=\mathbb{E}[Y\mid \mathrm{do}(X=x_1)]-\mathbb{E}[Y\mid \mathrm{do}(X=x_0)]$のように表す立場がある。ここで$\mathrm{do}(\cdot)$は介入を意味し、単なる条件付き期待値$\mathbb{E}[Y\mid X=x]$（相関）とは区別される。初学者には、相関は観測の関係であり、因果は操作の効果であることを強調する必要がある。

因果を議論するためには、(i)ランダム化、(ii)介入設計、(iii)交絡の記述が重要になる。ランダム化とは、交絡因子が平均的に均されるように条件順序や割り当てをランダムにすることである。例えば、温度条件を昇順に実行すると、装置状態の時間ドリフトが温度と相関してしまうため、温度条件の実行順序をランダムにするだけで交絡が減る。これは高度な因果手法以前に、実験設計として非常に基本である。

交絡の記述とは、装置状態や環境変数をメタデータとして記録し、モデルの入力に含めるか、解析時に統計的に調整することである。完全に交絡を取り除くことは難しいが、交絡候補を明示し、どこまで制御できたかを述べるだけでも、探索結果の信頼性は上がる。プロセスインフォマティクスでは、最適化の成功が再現性により評価されるため、因果の考え方を導入して「なぜ効くのか」を説明できる形にすることが重要である。



## 第9章 プロセスインフォマティクス II（自律化と制御）

本章では、プロセスの自律化、すなわち閉ループで条件提案と実行と評価を反復し、学習しながら改善する枠組みを扱う。自律化は単にAIを導入することではなく、装置制御、計測、解析、意思決定のインタフェースを固定し、反復可能な系として設計することが本質である。初学者には、自律化は未来技術ではなく、要素を分解して仕様を揃えることで実現できる工学であると理解させる必要がある。



### 9.1 閉ループ実験の構成要素

自律化では、装置制御、計測、解析、意思決定が反復される。閉ループの最小構成は、(1)条件生成、(2)実行、(3)観測、(4)更新である。これを数式で書けば、時刻（反復回数）$t$で条件$\mathbf{x}_t$を実行し、観測$y_t$を得て、データ集合$\mathcal{D}_t=\{(\mathbf{x}_i,y_i)\}_{i=1}^{t}$を更新し、次の条件$\mathbf{x}_{t+1}$を決める、という反復である。

重要なのは、各要素の入出力仕様を固定し、モジュール交換可能にしておくことである。例えば、解析モジュールが出力する指標$y$の定義（ピーク面積、膜厚均一性、相分率など）が変わると、過去データとの整合が崩れ、閉ループ全体が破綻する。したがって、閉ループ設計では、データ形式、メタデータ、単位、前処理、評価指標を固定し、変更が必要ならバージョンとして管理する必要がある。

また、閉ループでは失敗（実行不能、測定失敗、異常値）が必ず起こる。失敗を「欠損」として扱うのではなく、失敗の種類と原因を記録し、次の条件提案に反映できるようにすることが重要である。例えば、装置が安全停止した条件は探索領域から除外すべき硬い制約の情報になる。測定が飽和した条件は、測定レンジの変更や評価指標の再設計につながる。自律化の価値は、成功だけでなく失敗からも学べる点にある。



### 9.2 逐次意思決定の数理

逐次意思決定では、改善の見込みと情報獲得を同時に扱う必要がある。短期的な改善だけを追うと探索が局所化しやすく、未知領域の発見を逃す。したがって、情報獲得を評価に含めた戦略が必要である。第5章で述べた獲得関数の考え方が、プロセスでも中心になる。

例えば、モデルが予測平均$\mu(\mathbf{x})$と不確かさ$\sigma(\mathbf{x})$を出せるとき、次点を
$$
\mathbf{x}_{t+1}=\arg\max_{\mathbf{x}\in\Omega}\left[\mu(\mathbf{x})+\kappa\sigma(\mathbf{x})\right]
$$
で選ぶ戦略は、改善（$\mu$）と探索（$\sigma$）の両方を扱える。ここで$\kappa$を固定すると、探索の進み方が目的に合わないことがあるため、反復回数に応じて$\kappa$を変える設計も考えられる。例えば初期は探索を重視して$\kappa$を大きくし、後半は改善を重視して小さくする、といった方針である。

逐次意思決定で重要なのは、プロセスでは1回の実験コストが高く、制約も多い点である。したがって、単に候補を1点提案するだけでなく、バッチ提案（同時に複数条件を提案する）や、装置の段取りを考慮した提案（温度を飛び飛びにしない、ガス種変更回数を減らす）など、運用制約を組み込む必要がある。初学者には、逐次意思決定は数式だけで完結せず、実験運用と結びつけて設計されることを教える必要がある。



### 9.3 プロセスのデジタルツイン

デジタルツインは、プロセス状態を数理モデルとデータで同時に追跡し、制御へつなぐ考え方である。ここで重要なのは、モデルの精密さよりも、どの状態量を観測し、どの操作量で制御するかという設計である。材料プロセスでは、すべての内部状態を直接測れないことが多いため、観測可能量から状態を推定し、推定状態に基づいて操作量を決める枠組みになる。

状態$\mathbf{x}(t)$、操作$\mathbf{u}(t)$、観測$\mathbf{y}(t)$を用いて
$$
\mathbf{x}(t+1)=F(\mathbf{x}(t),\mathbf{u}(t))+\boldsymbol{\eta}(t),\qquad
\mathbf{y}(t)=G(\mathbf{x}(t))+\boldsymbol{\xi}(t)
$$
と表すと、デジタルツインは$F$と$G$のモデルと、データからの推定を組み合わせて$\mathbf{x}(t)$を追跡する仕組みである。例えば、成膜中のプラズマ状態、炉内の酸素分圧、析出進行度、結晶化度などを状態量として設定し、観測（発光、温度、圧力、インライン分光）から推定して制御に用いる。

この設計で最も難しいのは、状態量の選び方である。状態量は物理的意味があり、かつ観測可能量から推定可能でなければならない。意味はあるが推定不可能な状態量を選ぶと、ツインは機能しない。逆に推定しやすいが意味の薄い状態量を選ぶと、制御の解釈ができず、再利用性が低い。したがって、デジタルツインは、物理理解と計測設計と制御設計を同時に行う統合課題である。



### 9.4 品質保証のための学習

品質保証では、外れを当てることより外れを出さない設計が重要になる。研究段階では最良性能が注目されるが、運用では安定性が最重要になることが多い。異常検知や予兆検知では、正常データの定義が難しく、運用条件の変化を前提にした更新設計が必要である。

異常検知の基本は、観測$\mathbf{y}$が正常分布から外れたかを判定することである。例えば、正常時の特徴量$\mathbf{z}$が平均$\boldsymbol{\mu}$と共分散$\mathbf{\Sigma}$を持つと仮定すれば、マハラノビス距離
$$
D^2=(\mathbf{z}-\boldsymbol{\mu})^\top \mathbf{\Sigma}^{-1}(\mathbf{z}-\boldsymbol{\mu})
$$
が大きいとき異常とみなす、といった考え方がある。ここで重要なのは、正常分布$\boldsymbol{\mu},\mathbf{\Sigma}$は運用条件や装置劣化で変わるため、固定すると誤検知や見逃しが増える点である。したがって、モデル更新の設計（どの頻度で更新するか、どのデータを正常として採用するか）が品質保証の本質になる。

予兆検知はさらに難しい。異常が起こる前の微小な変化を検出するには、時間依存の特徴や状態推定が必要になる。例えば、温度制御の応答遅れ、圧力の微小変動、プラズマ発光のスペクトル形状の変化などが、異常の前兆として現れる場合がある。これらを検出するには、単発の値ではなく、時間窓内の変化率やスペクトル変化を特徴量にする設計が有効である。初学者には、品質保証は分類精度の問題ではなく、運用の中で変化する正常を追跡する問題であると教える必要がある。



### 9.5 プロセス知識の形式化

研究者や技術者の経験は、暗黙知として残りやすい。例えば、装置の癖、段取り、危険条件の回避、良い膜質が出る手順などは、文章化されないまま個人に蓄積されることが多い。しかし、自律化や組織的な学習を行うには、工程の制約や判断基準を形式化してデータと結びつける必要がある。形式化とは、経験を単なるメモから、学習と推論の対象へ変換する作業である。

形式化の第一歩は、判断基準を変数として書くことである。例えば「膜が荒い」という判断を、表面粗さ$R_\mathrm{a}$、AFM画像の周波数成分、欠陥密度などの定量指標に落とす。次に「この条件は危険」という判断を、温度上限、圧力上限、ガス組成の禁止領域などの制約として書く。これにより、暗黙知が探索空間$\Omega$や制約$g_j(\mathbf{x})$としてモデルに入る。

形式化の第二歩は、知識とデータの接続である。例えば「圧力が高いと欠陥が増える」という経験則があるなら、圧力と欠陥指標の関係をデータで検証し、成立範囲と例外条件を記述する。成立範囲が分かれば、その知識は設計指針として再利用できる。例外が見つかれば、追加の状態量（装置汚染、基板温度、ガス純度など）を導入して知識を精密化できる。

形式化の第三歩は、更新可能にすることである。プロセスは装置更新や材料変更で変わるため、知識も固定ではない。したがって、知識を文書として残すだけでなく、バージョン管理し、どの条件で更新されたかを追跡できる形にする必要がある。自律化やデジタルツインは、この形式化された知識を土台にして初めて安定して動く。初学者には、プロセス知識の形式化はAIのためではなく、研究の再現性と組織的学習のために必要であると理解させるべきである。


## 第10章 物理インフォマティクス I（物理制約学習）

本章では、物理法則や対称性などの知識を、学習モデルの中に明示的に組み込む考え方を扱う。材料研究では、全ての条件を網羅するデータを集めることが難しく、学習はしばしば限られたデータから外挿を迫られる。物理制約学習は、データが少ない状況でも、物理的にあり得ない予測を避け、解釈可能な形で推論を行うための設計思想である。



### 10.1 物理制約を学習へ入れる理由

材料研究では、測定や計算のコストが高く、データ数が少なくなりやすい。例えば、第一原理計算でも計算条件（超胞サイズ、$k$点、汎関数、温度、欠陥配置）により計算コストが増大し、実験でも試料作製から測定までに時間がかかる。その結果、学習器に十分な量の教師データを与えられず、単純に高表現力のモデルを使うと、訓練データには合うが新条件で破綻する状況が起こりやすい。これは統計的には過学習と呼ばれるが、材料では「物理的に不自然な外挿」が研究の失敗として現れる点が深刻である。

物理制約を入れるとは、学習が取りうる関数の形を、物理的に許される範囲へ制限することである。例えば、入力$\mathbf{x}$から出力$y$を予測する回帰を考えると、データだけに頼る学習は
$$
\hat{y}=f_\theta(\mathbf{x})
$$
の形で$f_\theta$を自由に近似する。一方で、物理的な制約が「単調性」や「非負性」などとして存在する場合、$f_\theta$がそれを破ると、データ点の間で意味のない振る舞い（例えば負の拡散係数、エネルギーの発散、保存則の破れ）が出る。制約を入れると、訓練点の近傍だけでなく、未観測領域でも物理的に妥当な候補に探索空間を絞り込めるため、外挿での破綻を抑える方向に働く。

さらに重要なのは、制約を入れることで「少ないデータで学べる」方向に寄与する点である。学習では、未知の関数$f$を推定する自由度が大きいほど、多くのデータが必要になる。物理制約は自由度を減らすため、同じデータ数でも推定精度が上がる場合がある。これは、材料研究における現実的制約（データが増やしにくい）と相性が良い。初学者には、物理制約は精度を上げるための技巧ではなく、データ不足という根本制約に対する設計原理であると理解させる必要がある。



### 10.2 保存則・対称性・次元の扱い

物理法則の中でも、保存則と対称性は学習結果の整合性を支える最も基本的な枠組みである。保存則の例として、質量保存、電荷保存、エネルギー保存がある。例えば連続体モデルでは、密度$\rho$と速度場$\mathbf{v}$に対して連続の式
$$
\frac{\partial \rho}{\partial t} + \nabla\cdot(\rho\mathbf{v})=0
$$
が成立し、これを破る予測は物理的に意味を持たない。学習モデルがこの制約を暗黙に学ぶこともあるが、データが少ない場合は破りやすい。したがって、保存則を満たすようにモデル構造や損失関数を設計することが重要になる。

対称性は、物理量が座標の取り方に依存しないことを表す。例えば、結晶を並進させてもエネルギーは変わらない（並進対称性）、全体を回転しても物性は変わらない（回転対称性）、原子の並べ替え（同種原子の置換）で同じ状態なら同じ出力になる（置換対称性）などである。学習器がこれを満たさないと、同じ構造を座標系だけ変えて与えたときに異なる予測を返し、データ表現の仕方に依存した不安定な結果になる。特に、結晶構造や原子配置を入力とするモデルでは、対称性不変な特徴量（距離、角度、局所環境の多体相関など）を用いる設計が重要になる。

次元解析（単位の整合）も、初学者が見落としやすいが極めて重要である。例えば、応力$\sigma$はPa、ひずみ$\varepsilon$は無次元で、弾性エネルギー密度は$\sigma\varepsilon$の次元を持つ。学習で入力や出力のスケールが混在すると、モデルが単位系の違いを学んでしまい、別単位で与えると破綻する。したがって、物理量は単位を揃えるだけでなく、必要に応じて無次元化する設計が有効である。例えば、温度$T$を基準温度$T_0$で割り$\tilde{T}=T/T_0$とする、エネルギーを$k_\mathrm{B}T$で規格化する、といった手法である。無次元化は、モデルが本質的関係を学びやすくするだけでなく、異なる系への移植性を高める。



### 10.3 方程式と学習の結合

支配方程式を$\mathcal{F}(\mathbf{u},\mathbf{x})=0$とし、学習出力を$\hat{\mathbf{u}}$とすると、残差$\mathcal{R}=\mathcal{F}(\hat{\mathbf{u}},\mathbf{x})$を損失に入れられる。これは、観測データに合うだけでなく、方程式を満たす解を選ぶための設計であり、物理インフォマティクスの中心となる考え方である。

基本形として、観測データ$\{(\mathbf{x}_i,\mathbf{y}_i)\}$に対して、データ誤差の損失$\mathcal{L}_\mathrm{data}$と、方程式残差の損失$\mathcal{L}_\mathrm{phys}$を組み合わせて
$$
\mathcal{L}=\mathcal{L}_\mathrm{data}+\lambda\,\mathcal{L}_\mathrm{phys}
$$
とする。例えば回帰なら
$$
\mathcal{L}_\mathrm{data}=\frac{1}{N}\sum_{i=1}^N \|\mathbf{y}_i-\hat{\mathbf{y}}(\mathbf{x}_i)\|^2,
\qquad
\mathcal{L}_\mathrm{phys}=\frac{1}{M}\sum_{j=1}^M \|\mathcal{F}(\hat{\mathbf{u}}(\mathbf{x}_j),\mathbf{x}_j)\|^2
$$
のように書ける。ここで$\{\mathbf{x}_j\}$は観測点に限らず、領域内の任意点（コロケーション点）を取れるため、観測が少なくても物理を満たす方向へ解を誘導できる。

$\lambda$の役割は重要である。$\lambda$が大きすぎると方程式を満たすことが優先され、観測ノイズや未知の物理（モデル化されていない項）を吸収できず、観測に合わなくなる。逆に$\lambda$が小さすぎると、方程式制約が形だけになり、通常のデータ駆動モデルと変わらなくなる。したがって、$\lambda$は単なるハイパーパラメータではなく、方程式モデルをどれだけ信じるか、観測をどれだけ信じるかの研究上の宣言として扱うべきである。

境界条件や初期条件の扱いも同様である。例えば連続体モデルでは、境界$\partial\Omega$上で$\mathbf{u}=\mathbf{u}_\mathrm{bc}$を満たす必要がある。これを損失として
$$
\mathcal{L}_\mathrm{bc}=\frac{1}{B}\sum_{k=1}^B\|\hat{\mathbf{u}}(\mathbf{x}_k)-\mathbf{u}_\mathrm{bc}(\mathbf{x}_k)\|^2
$$
のように追加し、$\mathcal{L}=\mathcal{L}_\mathrm{data}+\lambda\mathcal{L}_\mathrm{phys}+\mu\mathcal{L}_\mathrm{bc}$とする設計が考えられる。材料問題では境界条件が物理結果を支配する場合が多いため、境界条件を曖昧にした学習は再現性を失いやすい。初学者には、方程式の結合は式の美しさではなく、適用条件（境界、初期、単位）を明示して初めて成立することを理解させる必要がある。



### 10.4 マルチスケールの縮約

第一原理、メゾ、連続体の間には縮約と接続が必要である。縮約は情報を捨てる操作でもあるため、何を捨て、何を保持するかを物理量として宣言し、学習モデルが肩代わりする部分を明確にする必要がある。材料では、電子状態→原子配置→欠陥・組織→連続体応答という階層があり、全てを同じ分解能で扱うことはできない。

縮約の具体例として、第一原理計算で得られるエネルギー$E(\{\mathbf{R}_i\})$から、弾性定数や相安定性を抽出することを考える。原子座標$\{\mathbf{R}_i\}$は高次元であり、そのまま連続体へ渡すことはできない。そこで、ひずみ$\varepsilon$に対するエネルギーの2階微分から弾性定数を得る、といった縮約が行われる。例えば単純な一軸ひずみなら
$$
E(\varepsilon)\approx E_0 + \frac{1}{2}V\,C\,\varepsilon^2
$$
のように近似し、係数$C$を材料定数として抽出する。このとき、原子レベルの複雑な自由度は捨てられ、$C$という少数パラメータに要約される。

しかし、縮約で捨てた情報が重要な現象もある。例えば欠陥や微細組織が支配する強度や損失は、平均的材料定数だけでは説明できない。このギャップを埋める方法の一つが、縮約で残した状態変数（例えば相分率、粒径、欠陥密度、磁区幅など）を設計し、それを連続体モデルの内部変数として扱うことである。学習は、この内部変数の進化則や構成式の未知部分を補う役割を持てる。

重要なのは、学習が肩代わりする対象を明確にすることである。例えば、連続体方程式は正しいが材料定数が不確かである、という状況なら、学習は定数推定（同定）の役割を担う。一方で、方程式そのものが未知（例えば複雑な相変態の速度則）なら、学習は進化則の近似を担う。この区別が曖昧だと、学習モデルが何を学んでいるか説明できず、外挿で破綻しやすい。初学者には、マルチスケール接続は「全部AIで学ぶ」ではなく、「物理で決める部分と学習で補う部分を切り分ける」作業であると教える必要がある。



### 10.5 物理制約の検証

物理制約を入れたと言っても、実際に満たされているかは検証が必要である。残差の分布、境界条件の満足度、単位整合の確認を通じて、制約が形だけになっていないことを示すべきである。検証がないと、制約付き学習が単なる宣言に留まり、第三者が再現・評価できない。

第一に、方程式残差$\mathcal{R}$の大きさと分布を確認する。例えば領域内で$\|\mathcal{R}\|$を評価し、どの場所・どの条件で残差が増えるかを示すと、モデルが苦手とする領域が明確になる。これは、追加データ取得やモデル改良の指針にもなる。第二に、境界条件・初期条件が満たされているかを独立に確認する。境界条件を損失に入れても、重みの設定次第で境界近傍が崩れることがあるため、境界上の誤差を別指標として報告する必要がある。

第三に、単位整合・次元整合の確認である。例えばエネルギー密度の単位がJ/m$^3$になっているか、磁化や応力の単位が一貫しているかをチェックし、無次元化した場合は元の物理量へ戻す変換を明示する。これらは地味だが、材料研究では単位の取り違えが致命的な誤結論につながることがある。初学者には、検証は精度評価より一段深い「物理的妥当性の確認」であり、これが物理インフォマティクスを学術的主張にするための必須要件であると教えるべきである。



## 第11章 物理インフォマティクス II（シミュレーション統合と代理モデル）

本章では、計算科学を材料研究に組み込む際に、計算そのものを置き換えたり補助したりする代理モデル（サロゲート）と、複数の計算階層を統合する設計を扱う。材料研究では、第一原理、分子動力学、相場モデル、フェーズフィールド、有限要素法など、異なる仮定と分解能を持つシミュレーションが併用される。物理インフォマティクスの観点では、これらを単に並列に使うのではなく、目的に応じて誤差と不確かさを管理しながら統合することが重要である。



### 11.1 代理モデルの位置づけ

高価な計算を置き換える代理モデルは、単なる近似器ではなく研究設計の一部である。代理モデルが導入される理由は、計算コストの削減だけではない。計算を高速化することで、探索、感度解析、不確かさ評価、多目的最適化など、反復を要する研究操作が可能になる点に価値がある。

代理モデルを考えるとき、まず「何を近似するか」を明確にする必要がある。例えば、入力$\mathbf{x}$（組成、構造、境界条件など）に対して出力$y$（エネルギー、応力、相分率、磁化など）を返す計算コードをブラックボックスと見なすと、代理モデルは
$$
y \approx \hat{y}(\mathbf{x})
$$
を学習する。しかし、材料研究では出力が最終物性ではなく中間量である場合が多い。例えば、第一原理計算の出力をもとに連続体モデルへ材料定数を渡す場合、代理モデルの誤差が最終物性にどう伝搬するかを考えなければならない。ここで「どの誤差が許容され、どの誤差が結論を変えるか」を先に決めておかないと、代理モデルが高精度でも研究主張に耐えない可能性がある。

誤差設計の一例として、代理モデルの誤差$\delta(\mathbf{x})=y-\hat{y}(\mathbf{x})$が最終評価指標$J$に与える影響を、感度で見積もる考え方がある。最終指標が$J=g(y)$で、$g$が滑らかなら
$$
\Delta J \approx g'(y)\,\delta
$$
となる。$g'(y)$が大きい領域では、同じ$\delta$でも結論が大きく変わるため、そこでは代理モデルの精度を高めるか、代理モデルを使わず元計算へ戻す設計が必要になる。初学者には、代理モデルの性能は単独の誤差指標ではなく、研究目的に対する影響で評価すべきだと教える必要がある。



### 11.2 マルチフィデリティの考え方

低精度・低コスト計算と高精度・高コスト計算を組み合わせると、資源配分が合理化される。材料研究では、簡略モデルは広い探索に向き、高精度モデルは限られた候補の精査に向く。この役割分担を数理的に扱うのがマルチフィデリティの考え方である。重要なのは、低フィデリティの情報を過信しない枠組みを作ることである。

基本的なモデル化として、低フィデリティ出力$y_\mathrm{L}$と高フィデリティ出力$y_\mathrm{H}$の関係を
$$
y_\mathrm{H}(\mathbf{x}) = \rho\,y_\mathrm{L}(\mathbf{x}) + \Delta(\mathbf{x})
$$
と表す方法がある。ここで$\rho$はスケーリング係数、$\Delta(\mathbf{x})$は系統差（バイアス）である。低フィデリティが高フィデリティの傾向をある程度捉えているなら、$\rho$と$\Delta$を学習して補正することで、少数の高フィデリティ点から全体を推定できる。ただし、低フィデリティが誤った傾向を持つ領域では、補正が難しくなるため、どの領域で低フィデリティが信頼できるかを検証する必要がある。

資源配分の観点では、「どの点を高フィデリティで計算するか」が重要になる。低フィデリティで広く探索し、高フィデリティは不確かさが大きい点、目的関数が改善しそうな点、物理的に境界に近い点に割り当てる、という設計が合理的である。これは第9章の逐次意思決定とも接続する。初学者には、マルチフィデリティは計算の節約ではなく、限られた計算資源を研究目的に沿って配分するための数理であると理解させる必要がある。



### 11.3 反応拡散・相変態・磁気などの連続体モデルとの統合

連続体モデルは境界条件と構成式に依存し、材料定数の不確かさが結果へ大きく影響する。反応拡散、相変態、磁気などでは、支配方程式の形自体は比較的確立していても、材料固有のパラメータ（拡散係数、界面エネルギー、移動度、磁気異方性、交換定数、ダンピングなど）が不確かで、そこが予測のボトルネックになることが多い。

例えば反応拡散系の基本形は
$$
\frac{\partial c}{\partial t} = \nabla\cdot\left(D(c,T)\nabla c\right) + R(c,T)
$$
で表される。ここで$D$や$R$が未知・不確かである場合、観測データ（濃度分布の時間発展など）から$D$や$R$を同定したい。しかし、どの観測がどのパラメータを識別できるかという識別可能性の議論が不可欠である。例えば、観測が定常状態だけなら$D$と$R$が相互に補償し、同じ定常分布を異なる組み合わせで再現できることがある。この場合、パラメータ推定は不定になり、学習で数値が出ても物理的意味が確定しない。

磁気の例では、連続体としての磁化$\mathbf{m}(\mathbf{r},t)$の時間発展はLandau–Lifshitz–Gilbert方程式で
$$
\frac{\partial \mathbf{m}}{\partial t}
= -\gamma\,\mathbf{m}\times \mathbf{H}_\mathrm{eff}
+ \alpha\,\mathbf{m}\times \frac{\partial \mathbf{m}}{\partial t}
$$
と書ける。ここで$\mathbf{H}_\mathrm{eff}$には交換、異方性、外場、静磁場、磁気弾性場などが入る。モデル構造は知られていても、$A$（交換）、$K$（異方性）、$\alpha$（ダンピング）などの定数が不確かなら、結果は大きく変わる。学習が支援できるのは、これら定数の同定や、未モデル化項の補正である。ただし、観測（例えばMOKE像やMBN信号）がどのパラメータに感度を持つかを見積もらないと、識別不可能なパラメータを無理に推定することになる。

統合の鍵は、(i)方程式構造は物理で固定し、(ii)不確かな部分を学習で補い、(iii)識別可能性を観測設計とセットで考えることである。初学者には、連続体モデルと学習の統合は、方程式を学習で置き換えるのではなく、方程式を軸にして未知量を同定・補正する設計だと教えるべきである。



### 11.4 外挿検知と適用範囲

代理モデルは学習領域の外で破綻しうるため、適用範囲の明示が必要である。材料研究では、探索が進むほど既存データから離れた点を試すため、外挿は避けられない。したがって、外挿を禁止するのではなく、外挿を検知し、危険な領域では元計算や実験に切り替える設計が重要である。

外挿検知の一つは、入力分布の距離である。学習データの入力集合$\{\mathbf{x}_i\}$に対して、新点$\mathbf{x}$がどれだけ離れているかを距離$d(\mathbf{x},\{\mathbf{x}_i\})$で評価する。ただし、高次元では距離が意味を持ちにくい場合があるため、表現空間（特徴量空間）で距離を定義する設計が必要になる。もう一つは、不確かさの増大である。モデルが$\sigma(\mathbf{x})$を出力できるなら、$\sigma$が大きい領域を危険とみなす。さらに物理インフォマティクスでは、物理残差$\|\mathcal{R}\|$の増大も外挿の兆候になり得る。すなわち、予測は出ているが方程式を満たさないなら、その点は危険である。

実運用では、これらを併用し、段階的に切り替える設計が望ましい。例えば、距離が大きいが不確かさが小さい場合は、モデルが過信している可能性があり、追加検証が必要になる。不確かさが大きいが距離が小さい場合は、ノイズが大きい領域やモデルが苦手な局所構造がある可能性がある。物理残差が大きい場合は、モデル化不足（未知物理）が疑われ、代理モデルに頼らず元計算や追加実験で現象理解を優先すべきである。初学者には、適用範囲の明示は責任回避ではなく、学術的主張の範囲を明確にして再現性を確保するための手続きであると教える必要がある。



### 11.5 計算結果の再利用と共有

計算データは、条件が揃えば再利用価値が高いが、条件差が混入すると整合性が崩れる。材料計算では、入力条件（汎関数、擬ポテンシャル、カットオフ、$k$点、スピン設定、温度、セルサイズ、境界条件）により結果が変わるため、計算条件の完全記録が必須である。再利用可能な計算データとは、数値だけでなく、その数値がどの条件で得られたかが追跡できるデータである。

再利用のためには、入力・出力の対応付けを徹底する必要がある。例えば、あるエネルギー値が「構造最適化後の全エネルギー」なのか「単点計算の全エネルギー」なのか、「スピン軌道込み」なのか「無し」なのかで意味が変わる。これを曖昧にしたまま統合解析すると、学習モデルは物理ではなく条件差を学び、予測が破綻する。したがって、計算結果を共有する際は、計算条件をメタデータとして固定化し、バージョン管理する設計が重要である。

共有の観点では、研究室内での共有と外部公開で要求水準が異なる。研究室内共有でも、将来の自分や共同研究者が理解できる形で整理されていないと、再解析が困難になる。外部公開では、第三者が再現できることが要求されるため、入力ファイル、ログ、収束基準、前処理手順、データ整形手順を含めた形で提示する必要がある。初学者には、計算データの価値は結果の数値ではなく、再現可能性と統合可能性により決まると教えるべきである。

## 第12章 化学インフォマティクス I（無機材料：熱力学・相・反応・組織の表現）

本章で扱う「化学インフォマティクス」は、有機分子の反応設計ではなく、無機材料（特に金属・合金・セラミックス）における「相平衡・熱力学・化学反応・組織形成」を、データとして扱い、推論や最適化に接続するための方法論である。材料インフォマティクスが「組成・構造から物性を予測する」軸を強く持つのに対し、本章の化学インフォマティクスは「その組成・相・組織に到達するまでの化学的拘束（平衡）と化学反応（駆動力）」を定量化して、実行可能な探索へ落とす役割を担う。

無機材料では、同じ平均組成でも、どの相がどの割合で共存し、どの元素がどの相へ分配され、どの欠陥や析出物が形成されるかで、物性が大きく変わる。これらは、(i)熱力学（何が平衡として許されるか）と、(ii)速度論（どれだけの時間・温度で実現するか）、(iii)反応（酸化・還元・窒化・脱炭・脱酸などの化学ポテンシャルの変化）で支配される。したがって、無機材料の化学インフォマティクスでは、分子をグラフで表す代わりに、相と組織を記述するための「状態変数」と、それらを変化させる「条件変数」を定義し、平衡条件・反応条件・不確かさを一貫して扱える形式へ落とし込むことが出発点になる。



### 12.1 材料研究における化学インフォマティクスの位置：無機材料の「実現可能性」を決める層

無機材料の探索では、候補組成が見つかったとしても、それが「その相として存在できるのか」「目的相が単相で得られるのか」「酸化や蒸発で組成がずれないか」「熱処理で析出や相変態が起きるか」を無視すると、提案が現場で成立しない。ここで重要なのは、化学は単に“原料選び”ではなく、材料内部で起こる元素分配・相形成・欠陥形成・界面反応を含む、広義の“材料の化学状態”を決める学問であるという点である。

この層を定量化する中心概念がギブズ自由エネルギーである。温度T、圧力p、組成{n_i}において、平衡はギブズ自由エネルギーGを最小にする状態として与えられる：
$$
G(T,p,\{n_i\}) = \sum_{\alpha} n^{\alpha} \, G^{\alpha}(T,p,\{x_i^{\alpha}\})
\quad \text{を最小化}
$$
ここでαは相（フェライト、オーステナイト、金属間化合物、酸化物相など）を表し、x_i^{\alpha}は相α中のモル分率である。材料探索において「相が出る／出ない」は、最終的にはこの最小化問題の結果であり、データとしては「相の種類」「相分率」「元素の分配（どの相にどれだけ入るか）」として現れる。

さらに無機材料では、周囲雰囲気（酸素分圧pO2、水素分圧pH2、窒素分圧pN2、炭素活量aCなど）が反応を通じて材料の化学ポテンシャルを変える。例えば酸化は、金属中の元素の化学ポテンシャルμと、酸素の化学ポテンシャルμ_Oが与える反応駆動力で進行する。つまり、化学インフォマティクスは「組成・相・雰囲気・温度」を一緒に扱い、実験計画（プロセス条件）と材料状態（相・組織）を橋渡しする役割を持つ。



### 12.2 熱力学計算（相平衡・化学ポテンシャル）をデータとして表現する

無機材料のデータで最も重要なものの一つが「相平衡データ」である。これは単なる相図の画像ではなく、(T,p,組成)を入力に、平衡相集合と各相の組成・相分率を出力する関数として捉えると扱いやすい。すなわち、
$$
(T,p,\mathbf{c}) \;\mapsto\; \{\alpha\},\; \{f_{\alpha}\},\; \{x_i^{\alpha}\}
$$
という写像を、計算（CALPHADなど）または実験（平衡化熱処理＋分析）から得る、という見方である。ここでf_{\alpha}は相αの相分率である。

相平衡の条件は「各成分iの化学ポテンシャルが、共存相間で等しい」ことで表される：
$$
\mu_i^{\alpha}(T,p,\{x\}) = \mu_i^{\beta}(T,p,\{x\}) \quad (\text{共存する相 } \alpha,\beta \text{について})
$$
化学ポテンシャルは、相のギブズ自由エネルギーから
$$
\mu_i^{\alpha} = \left(\frac{\partial G^{\alpha}}{\partial n_i}\right)_{T,p,n_{j\neq i}}
$$
として定義される。初学者は「相平衡＝相図」と捉えがちだが、インフォマティクスに接続する際は、相図を支える数式（G、μ、平衡条件）を意識すると、何を入力にして何を出力として扱うかが明確になる。

CALPHAD（Calculation of Phase Diagrams）は、各相のギブズ自由エネルギーG^{\alpha}を経験的・物理的モデルで表し、データ（熱量測定、相境界、活量、第一原理計算など）でパラメータを同定する枠組みである。典型的には、
$$
G^{\alpha}(T,\{x_i\})
= \sum_i x_i G_i^{0,\alpha}(T)
+ RT \sum_i x_i \ln x_i
+ G_{\mathrm{excess}}^{\alpha}(T,\{x_i\})
$$
のように、参照項＋理想混合項＋過剰項で構成される（Rは気体定数）。過剰項はRedlich–Kister型などで表され、相互作用の強さをパラメータ化する。化学インフォマティクスの観点では、これらのパラメータそのものよりも、結果として得られる「相の安定性」「分配」「駆動力」が、探索・設計に直接効く。

この層をデータとして扱うときは、少なくとも以下を一貫して保持する必要がある。
- 入力：T、p、全組成（モル分率または質量分率）、必要なら雰囲気（pO2など）
- 出力：共存相の集合、相分率f_{\alpha}、各相組成x_i^{\alpha}、化学ポテンシャルμ_i、活量a_i
- 由来：用いたデータベース名、バージョン、計算設定（平衡/準安定、含めた相、拘束条件）

これを怠ると、同じ「相図」でも、含めた相が違うだけで結果が変わり、学習が物理差ではなく計算条件差を拾うことになる。



### 12.3 無機材料の化学反応の表現：酸化・還元・窒化・脱炭・脱酸・スラグ反応

無機材料で頻出する化学反応は、固体内部の相変態だけでなく、雰囲気や他相（酸化物、スラグ、基板、坩堝）との反応である。これらは「反応のギブズ自由エネルギー変化ΔG」を核にして表現すると統一的に扱える。一般に反応
$$
\sum_j \nu_j A_j = 0
$$
に対して、
$$
\Delta G = \sum_j \nu_j \mu_j
$$
であり、ΔG < 0ならその方向に進行しやすい。標準状態を用いると、
$$
\Delta G = \Delta G^\circ + RT \ln Q
$$
となる（Qは反応商）。平衡ではΔG = 0であり、平衡定数Kは
$$
K = \exp\left(-\frac{\Delta G^\circ}{RT}\right)
$$
で与えられる。ここまでを押さえると、酸化・還元が「酸素分圧（酸素ポテンシャル）」で決まることが自然に理解できる。

例として金属Mの酸化
$$
\mathrm{M} + \frac{1}{2}\mathrm{O_2} \to \mathrm{MO}
$$
を考えると、反応の進みやすさはμ_O（≒pO2）に強く依存する。雰囲気を変えることは、材料内部の化学ポテンシャル境界条件を変えることに等しい。窒化（pN2）、脱炭（aC）、脱酸（Al、Si、Tiなどによる酸素固定化）、水素脆化に関係する水素化学ポテンシャルなども、同じ形式で整理できる。

また冶金では、金属-スラグ反応（酸化物相との分配）が重要になる。例えば酸素や硫黄、リンなどの不純物は、金属相と酸化物相の間で分配される。これは「分配係数」や「活量係数」を使って表され、最終的には各相の化学ポテンシャル一致条件に帰着する。インフォマティクスでは、反応式そのものよりも、(T,組成,雰囲気)→(分配、反応駆動力、危険性)という予測・最適化問題へ落とし込むことが狙いになる。

無機材料の反応データは、以下のような形式に構造化すると、プロセス条件提案へ接続しやすい。
- 反応の種類：酸化、還元、窒化、脱炭、蒸発、腐食、界面反応
- 条件：T、p、pO2/pN2/pH2、流量、露点、坩堝材、基板材
- 出力：生成相、反応速度（可能なら）、質量増減、組成変化、反応熱（TG-DTA等）
- 判定：目的相維持/崩壊、危険領域（爆発・有害生成物・装置制約）

この「反応の可否とリスク」をデータ化することが、無機材料の化学インフォマティクスの中心になる。



### 12.4 組織（微細組織）を化学の側から記述する：析出・相分離・粒成長・偏析

無機材料では「組織」は物理の問題に見えるが、実際には熱力学（相の安定性）と速度論（拡散・核生成・成長）で決まるため、化学インフォマティクスの重要対象である。ここでのポイントは、組織を画像の特徴量として扱う前に、化学的な状態変数として整理することである。例えば、析出強化合金なら、析出物の相（種類）、体積分率、粒径分布、数密度、界面組成が支配因子になる。これらは「化学的にどの相がどれだけ出るか」と「どれだけの時間で出るか」の両方が必要になる。

組織形成を状態遷移として書くと、プロセス（熱処理）と統合しやすい。熱履歴を操作量u(t)（温度T(t)、雰囲気など）として、状態s(t)（相分率f_{\alpha}(t)、平均粒径d(t)、析出物半径r(t)など）が変化するとみなす：
$$
\frac{d\mathbf{s}}{dt} = \mathbf{F}(\mathbf{s}, \mathbf{u}, \theta)
$$
ここでθは材料固有パラメータ（拡散係数、界面エネルギー、核生成障壁など）である。実験ではs(t)を完全観測できないため、観測z(t)（XRD、SEM/TEM像、硬さ、磁区像など）を通じて推定する。この構造は、計測インフォマティクスと相性が良い。

初学者が混乱しやすいのは「平衡で相が出る＝すぐ出る」ではない点である。平衡計算は“行き先”を示すが、“到達時間”は拡散などの速度論が決める。したがって、化学インフォマティクスでは、平衡データ（相の安定性）と、速度論データ（拡散係数、核生成・成長）を切り分けて扱い、両者を結合して「実際に出る組織」を推定する必要がある。

組織データを収集・選別できるようになるためには、組織を記述する最小単位を決めることが重要である。例えば次のようなレコード設計が考えられる。
- 入力：合金組成、溶製条件、熱履歴（T(t)）、雰囲気、加工履歴（圧延率など）
- 組織出力：共存相、相分率、析出物の種類とサイズ統計、粒径、テクスチャ、偏析指標
- 由来：観察手法（SEM/EBSD/TEM/APTなど）、解析条件、画像処理条件

こうした構造化ができると、組織のビッグデータから「析出物が出てしまう温度域」「粒成長が進む保持時間」「偏析が残りやすい冷却条件」など、プロセス設計へ直結する知識が抽出可能になる。



### 12.5 データの偏り・不確かさ・データベース依存性：無機材料特有の注意点

無機材料の化学データは、成功条件だけでなく、未反応・望まない相が出た条件・酸化で崩壊した条件など、失敗側の情報が設計に効く。しかし現実には、論文・報告には成功例が残りやすく、失敗例は散逸する。これにより、学習は成功分布を過大評価し、境界条件（どこで壊れるか）を学びにくい。したがって、研究室内のデータ蓄積では、失敗モードをカテゴリとして記録し、同じ形式で扱うことが重要である。

また相平衡・熱力学計算は、用いたデータベースに強く依存する。データベースは「相のモデル」と「パラメータ」を含むため、同じ組成でも、含めた相やパラメータセットが違うと結果が変わる。したがって、計算結果をビッグデータとして扱うなら、データベース名・版・含めた相・拘束条件を必ずメタデータとして保持しなければならない。ここを落とすと、後から再現できないだけでなく、学習が“データベース差”を物理差として拾う危険がある。

不確かさも二種類ある。第一は計測・観測の不確かさ（組成分析誤差、温度誤差、相同定誤差など）。第二はモデルの不確かさ（熱力学モデルのパラメータ、速度論パラメータ、未考慮相の存在など）である。無機材料の化学インフォマティクスでは、予測値だけでなく「その予測がどの仮定の上に立つか」を明示することが、実装可能性を担保する。



## 第13章 化学インフォマティクス II（無機材料：CALPHAD・速度論・反応経路・設計への統合）

本章では、第12章で整備した「相平衡・反応・組織」の表現を、実際の設計・探索に使うための推論枠組みへ進める。無機材料の化学インフォマティクスの強みは、物理・化学の拘束（平衡条件、保存則、反応駆動力）を明示できる点にあり、これを代理モデルや最適化、閉ループ実験（第9章）へ接続すると、提案の実行可能性が大きく上がる。



### 13.1 CALPHADを探索に使う：相の安定性・分配・駆動力を特徴量にする

CALPHADは相図を描くためだけではなく、探索のための“特徴量生成器”として非常に有効である。例えば、ある合金組成に対して、温度Tでの平衡相集合、相分率、各相中の元素分配が得られる。これらは、材料インフォマティクスで用いる説明変数として解釈できる。言い換えると、組成ベクトルだけでは捉えにくい「相としての文脈」を、熱力学計算が与えてくれる。

さらに重要なのが駆動力である。準安定相βが存在しうるとき、母相αからβへの析出や相変態の駆動力は、化学ポテンシャル差や自由エネルギー差として表される。単純化して、母相αの組成で評価した自由エネルギー差を
$$
\Delta G_{\mathrm{drive}}(T) = G^{\beta}(T,\{x\}_{\alpha}) - G^{\alpha}(T,\{x\}_{\alpha})
$$
と書けば、ΔG_drive < 0が析出・変態の熱力学的駆動力を示す。実際は界面エネルギーや弾性ひずみなどが核生成障壁に効くが、「どの温度域で駆動力が大きいか」は、熱処理窓の設計に直結する。

インフォマティクスでは、これら熱力学量をそのまま学習へ入れるより、研究目的に合わせて“設計指標”へ整理すると扱いやすい。例えば次のような指標がある。
- 単相安定域の広さ（温度範囲、組成許容幅）
- 目的相の相分率、競合相の相分率
- 元素の分配係数（相間の濃度比）
- 駆動力や化学ポテンシャル（析出、酸化、窒化の起こりやすさ）

これらを“材料候補のスクリーニング”に使い、残った候補へ詳細計算や実験を配分するのが、ビッグデータ時代の合理的な研究設計になる。



### 13.2 速度論（拡散・析出・酸化）を入れて「起こる／起こらない」を分ける

平衡で可能でも、時間スケールが合わなければ現実には起こらない。無機材料で典型的な速度論は拡散であり、最も基本はFickの法則である。濃度cの拡散は
$$
\mathbf{J} = -D \nabla c
$$
（第一法則）と書け、保存則と合わせると
$$
\frac{\partial c}{\partial t} = \nabla \cdot (D \nabla c)
$$
（第二法則）になる。Dは温度依存し、しばしばArrhenius型
$$
D(T) = D_0 \exp\left(-\frac{Q}{RT}\right)
$$
で表される。これが重要なのは、温度が少し変わるだけで拡散速度が指数関数的に変わり、析出や均質化の“実現時間”が大きく変わるからである。

析出・相変態の速度論は、核生成と成長に分かれる。初学者向けには、相変態の進行率X(t)がJMAK式で
$$
X(t) = 1 - \exp(-k t^n)
$$
と表されることが多い（kは速度定数、nは指数）。厳密には系ごとに意味づけが必要だが、少なくとも「温度と時間の組が組織を決める」こと、そしてkが温度依存して大きく変わることを理解するのに役立つ。

酸化も同様に、反応駆動力（ΔG）だけでなく、拡散律速で成長する場合が多い。代表的には酸化膜厚xが
$$
x^2 = k_p t
$$
（放物線則）に従う状況があり、これは拡散律速の典型である。化学インフォマティクスでは、(T,pO2,材料組成)→(酸化の進行度・膜厚・組成変化)を推定し、「どの条件で材料が化学的に壊れるか」を予測することが、探索の失敗率を下げる。



### 13.3 反応条件・雰囲気・熱処理の提案：拘束付き最適化として書く

無機材料の“化学条件提案”とは、溶媒提案ではなく、温度・雰囲気・保持時間・冷却速度・脱ガス条件・坩堝材・基板材など、材料の化学ポテンシャル境界条件を設計することである。これを最適化問題として書けば、設計の透明性が上がる。

例えば、目的相の相分率f_targetを最大化しつつ、競合相f_compを抑え、酸化や蒸発を避けたいとする。このとき条件変数を
$$
\mathbf{u} = (T, t, p\mathrm{O_2}, p\mathrm{N_2}, \text{冷却速度}, \ldots)
$$
として、目的関数Jを
$$
J(\mathbf{u}) = w_1 f_{\mathrm{target}}(\mathbf{u})
- w_2 f_{\mathrm{comp}}(\mathbf{u})
- w_3 \,\mathrm{Risk}(\mathbf{u})
$$
のように定義できる。さらに安全・設備制約を
$$
g_j(\mathbf{u}) \le 0
$$
として入れると、化学条件提案は制約付き最適化に落ちる。実際にはf_targetやRiskは未知で、計算（CALPHAD＋速度論）や実験で評価する必要があるため、ベイズ最適化や能動学習（第9章）と接続するのが自然である。

このとき重要なのは、化学インフォマティクスが“ブラックボックス最適化”に閉じないことである。CALPHADが与える相平衡や化学ポテンシャルは、提案の妥当性を説明する根拠になる。たとえば「この温度で競合相が熱力学的に安定になる」「このpO2では酸化物相が優先して出る」といった説明が可能になり、提案が単なる当て推量ではなく、再利用可能な設計指針へ変換できる。



### 13.4 材料・プロセス・物理との結合：化学を「境界条件」と「中間変数」にする

無機材料の化学は、材料設計の“入力”というより、形成過程を通じて組織を決める“中間層”として働く。したがって統合の鍵は、化学側の出力（μ、活量、相分率、駆動力、反応リスク）を、プロセス側モデルの境界条件・拘束条件として渡す設計である。

例えば、成膜・焼結・熱処理では、雰囲気が酸素化学ポテンシャルを与え、材料内部の欠陥平衡（空孔、置換欠陥）や相安定性を変える。これを抽象化して「化学境界条件」として表し、プロセスモデルの入力へ加えると、同じ温度履歴でも雰囲気で結果が変わることを統一的に扱える。さらに、組織（粒径、析出物、偏析）を媒介変数として材料物性モデルへ渡せば、化学→プロセス→組織→物性の因果鎖が明示できる。

また、反応拡散や相分離を扱う連続体モデル（フェーズフィールド、反応拡散方程式など）では、自由エネルギー密度f(c,T)や化学ポテンシャルμ = ∂f/∂cが支配的になる。ここにCALPHAD由来の自由エネルギーを組み込むと、化学（熱力学）と組織形成（メゾスケール）が直結する。化学インフォマティクスは、このように「熱力学モデルをデータとして接続する」ことで、単なるデータ駆動ではない、拘束付きの予測を可能にする。



### 13.5 学術的主張としての検証：データベース差・未観測領域・失敗領域を含めて示す

無機材料の化学インフォマティクスは、成功例だけを並べても主張として弱い。なぜなら、相平衡や反応は“境界”が重要であり、境界を外すと一気に破綻するからである。したがって検証では、次の3点を必ず意識する。
1) 分割設計：合金系（元素集合）や相の種類、雰囲気条件を跨いだ評価を行い、どの一般化を主張するかを明示する  
2) 失敗条件の取り込み：未反応・競合相出現・酸化崩壊など、失敗モード別に評価し、どこから危険になるかを示す  
3) 不確かさの提示：計測誤差とモデル誤差（DB依存、未考慮相、速度論パラメータ不確かさ）を区別し、適用範囲を文章で固定する  

特にCALPHADを用いる場合、データベース差が大きい。よって、計算結果を示すときは必ず「どのデータベース・どの仮定・どの相集合」で得た結果かを明記し、可能なら複数条件で感度を確認する。これにより、化学インフォマティクスは単なる計算結果の提示ではなく、「再現可能な研究手続き」として成立する。

最後に、検証を材料設計指針へ変換することが到達点になる。たとえば「この元素は目的相を安定化するが、特定のpO2で酸化物が出やすい」「この温度域は駆動力が大きいが拡散が遅く時間が足りない」など、熱力学・速度論・反応の三者で説明できる形に落とす。これができると、探索は当たり外れではなく、根拠のある条件提案として蓄積され、次の材料系にも移植可能になる。


## 第14章 統合設計

本章では、材料・計測・プロセス・物理・化学の5領域を、研究の時間軸に沿って一つの設計として結び直す。第1〜13章で得た道具立てを、材料開発という一連の営みの中で、どの順序で置き、どこで整合性を確かめ、どこで追加データ取得へ戻るべきかを、手順として明文化する。

材料開発では、入力と出力が直列につながり、途中で定義や単位が少しでも崩れると、最後の結論が不安定になる。さらに、誤差や不確かさが段階をまたいで積み重なり、最終の評価指標に効いてくる。したがって統合設計とは、単にデータを結合することではなく、(i)変数の定義、(ii)不確かさの扱い、(iii)制約の入れ方、(iv)検証の設計、(v)更新と共有の仕方を、同じ論理で貫くことである。



### 14.1 研究目標の定式化と変数の階層化

材料開発で最初に必要なのは、研究目標を文章ではなく、評価指標として数学的に定義することである。例えば磁性材料では、飽和磁化$M_s$を大きくしたい、保磁力$H_c$を小さくしたい、鉄損$P$を小さくしたい、という要求が同時に現れる。さらに工学的には、使用温度範囲、機械加工性、資源制約、コストなどが並び、単一のスカラー目標に潰すと、どの要求を犠牲にしたのかが見えにくくなる。そこで、目的関数をベクトルとして
$$
\mathbf{f}(\mathbf{x})=
\begin{bmatrix}
f_1(\mathbf{x})\\
f_2(\mathbf{x})\\
\vdots\\
f_K(\mathbf{x})
\end{bmatrix}
=
\begin{bmatrix}
- M_s(\mathbf{x})\\
H_c(\mathbf{x})\\
P(\mathbf{x})\\
\vdots
\end{bmatrix}
$$
のように定義し、最小化問題として扱うと整理しやすい。ここで負号を付けるのは、最大化したい量を最小化形式に合わせるためである。制約も同時に明文化し、例えば
$$
g_j(\mathbf{x})\le 0 \quad (j=1,\ldots,m)
$$
として、温度上限、毒性、安全、設備条件、元素使用量などを制約に落とし込む。この段階で重要なのは、後から制約を足すのではなく、研究の最初に「何が許され、何が許されないか」を宣言しておくことである。

次に、設計変数$\mathbf{x}$を一つの箱として扱わず、階層構造として分解する必要がある。材料開発では、化学（前駆体・反応）、プロセス（工程と条件系列）、材料（組成・構造・欠陥・微細組織）、計測（観測と前処理）、物理（方程式・材料定数）が相互に作用する。これを変数として書くなら、
$$
\mathbf{x} = \bigl(\mathbf{x}_{\mathrm{chem}},\ \mathbf{x}_{\mathrm{proc}},\ \mathbf{x}_{\mathrm{mat}},\ \mathbf{x}_{\mathrm{meas}},\ \mathbf{x}_{\mathrmphys}\bigr)
$$
のように分割し、どの成分が操作可能で、どの成分が観測可能かを分けて考える。例えば、研究者が直接操作できるのは、前駆体の選択、雰囲気、温度履歴、時間、成膜レートなどである一方、欠陥密度や粒径分布は結果として現れる内部状態であり、必ずしも直接には操作できない。内部状態は潜在変数$\mathbf{z}$として置き、
$$
\mathbf{z} = h(\mathbf{x}_{\mathrm{chem}},\mathbf{x}_{\mathrm{proc}}) \quad,\quad
\mathbf{y} = f(\mathbf{z},\mathbf{x}_{\mathrm{mat}})
$$
のように、操作→内部状態→物性という因果の向きを意識して記述すると、後で議論が崩れにくい。計測はさらに、物理量$\mathbf{y}$を直接観測するのではなく、観測$\mathbf{s}$として得るため、
$$
\mathbf{s} = \mathcal{M}(\mathbf{y};\ \mathbf{x}_{\mathrm{meas}}) + \boldsymbol{\epsilon}
$$
という観測モデルを必ず挟む。ここで$\mathcal{M}$は装置応答や前処理の影響を含む写像であり、$\boldsymbol{\epsilon}$はノイズである。統合設計では、この観測モデルを暗黙にせず、どの段階でどの変数が「観測値」で、どの変数が「推定した物理量」なのかを言葉と式で区別することが基礎になる。

多目的の扱いとしては、パレート最適を基本として理解するのがよい。すなわち、$\mathbf{x}^\*$がパレート最適であるとは、全ての目的で$\mathbf{x}^\*$以上に良い別解が存在しないことを意味する。現場では重み付け和
$$
J(\mathbf{x})=\sum_{k=1}^{K} w_k f_k(\mathbf{x})
$$
で一つにまとめたくなるが、$w_k$の選び方が研究の選好そのものであり、暗黙のまま進めると議論が不透明になる。したがって、統合設計では、まずパレート集合として候補群を示し、どの候補を採るかの理由を、物理・化学・工学の言葉で説明できる形にすることが重要である。



### 14.2 データの流れと不確かさの伝搬

統合設計の核心は、データが段階をまたいで形式を変えながら流れる点にある。化学では合成経路が工程の系列として記録され、プロセスでは温度・時間・流量・圧力などの条件空間として数値化され、計測ではスペクトル・画像・時系列が観測として得られる。材料では組成・構造・欠陥・微細組織の表現へ整理され、物理では方程式残差や材料定数として再表現される。ここでは「同じ現象を別の座標系で見直している」のであり、各変換は必ず情報損失を伴う。情報損失は、単純な誤差だけでなく、選別バイアス（成功例だけが残る、測定できたものだけが残る）や、未観測領域（データが存在しない化学空間）として現れる。

この連鎖を式で書くと、概念的には
$$
\mathbf{x}_{\mathrm{chem}}
\rightarrow
\mathbf{x}_{\mathrm{proc}}
\rightarrow
\mathbf{z}
\rightarrow
\mathbf{y}
\rightarrow
\mathbf{s}
\rightarrow
\hat{\mathbf{y}}
\rightarrow
\hat{J}
$$
のような流れになる。$\mathbf{z}$は内部状態、$\mathbf{y}$は物理量、$\mathbf{s}$は観測、$\hat{\mathbf{y}}$は推定された物理量、$\hat{J}$は評価指標の推定である。このとき不確かさは、各矢印で増えたり、形を変えたりする。重要なのは、不確かさ$\sigma$を「図に添える誤差棒」ではなく、次のデータ取得を決める量として一貫して扱うことである。

初学者が理解しやすいのは、最終評価$J$が推定量$\hat{\mathbf{y}}$の関数$J(\hat{\mathbf{y}})$であるときの不確かさ伝搬である。小さな変動を仮定すると、一次近似として
$$
\mathrm{Var}(\hat{J}) \approx
\nabla_{\mathbf{y}} J(\hat{\mathbf{y}})^{\mathsf{T}}
\ \Sigma_{\hat{\mathbf{y}}}\
\nabla_{\mathbf{y}} J(\hat{\mathbf{y}})
$$
と書ける。ここで$\Sigma_{\hat{\mathbf{y}}}$は$\hat{\mathbf{y}}$の共分散行列である。例えば$J = a y_1 + b y_2$のような線形結合なら、係数$a,b$が大きい物理量の不確かさが最終に効く。つまり「どの測定精度を上げれば結論が安定するか」は、感覚ではなく、微分と共分散で追跡できる。この考え方は、計測条件の設計や追加実験の優先順位付けに直結する。

不確かさは少なくとも三種類に分けて扱うと混乱が減る。第一は観測ノイズであり、同じ条件で繰り返しても揺らぐ成分である。第二はモデル不確かさであり、学習が十分に確からしい関数を定められていない成分である。第三は分布ずれであり、学習データが覆う領域と新しい候補の領域が異なることで生じる。統合設計では、これらを混同せず、どの段階でどれが支配的かを見極める必要がある。例えば計測インフォマティクスでは観測ノイズと装置差が効きやすく、材料探索では分布ずれが効きやすい。

この不確かさを意思決定へ接続する代表的な枠組みが、逐次選択である。例えば、候補$\mathbf{x}$を次に試すかどうかを決めるために、予測の平均$\mu(\mathbf{x})$と不確かさ$\sigma(\mathbf{x})$を用いて獲得関数$\alpha(\mathbf{x})$を定義し、
$$
\mathbf{x}_{\mathrm{next}}=\arg\max_{\mathbf{x}} \alpha(\mathbf{x})
$$
として選ぶ。ここで$\alpha$は改善を重視する形にも、情報獲得を重視する形にもできる。統合設計では、どの段階で改善を優先し、どの段階で情報獲得を優先すべきかを、研究目標とデータ状況から説明できるようにすることが重要である。データが薄い領域では情報獲得が効きやすく、データが厚い領域では改善が効きやすい、という直観を、数理的に支える役割を不確かさが担う。



### 14.3 統合モデリングの3層

統合モデリングは、表現層・推論層・制約層の3層に分けて整理すると理解しやすい。ここで層とは、同じデータに対して別の役割を与える見方であり、どの領域の手法を使うかよりも「何を担保するためにその層が必要か」を明確にすることが狙いである。

第一の表現層では、観測や記述を、比較可能な形に整える。例えば、組成は単に元素比率を並べるだけではなく、規格化、希薄元素の扱い、参照状態を明示した形で表現する。結晶構造は、座標の取り方や原子番号の並び替えで別物に見えないよう、対称性に関して同一性が保たれる表現へ変換する。計測では、スペクトルなら前処理とそのパラメータを含めて保持し、画像なら倍率・露光・コントラストをメタデータとして残す。化学では、分子や反応の表記揺れを抑え、保存則を破らない記述へ揃える。表現層を数学で書けば、観測$\mathbf{s}$から特徴$\mathbf{x}$への写像
$$
\mathbf{x}=\phi(\mathbf{s})
$$
を定める段階であり、$\phi$の選択が学習結果を大きく左右する。

第二の推論層では、目的に応じて回帰・分類・同定・最適化・生成を組み合わせる。予測なら$\hat{\mathbf{y}}=f_\theta(\mathbf{x})$であり、同定ならラベル$z$に対して$P(z\mid \mathbf{x})$を推定する。設計や探索では、$\mathbf{x}$を固定して$\mathbf{y}$を出す順方向だけでは足りず、目標$\mathbf{y}^\*$に対して候補集合$\{\mathbf{x}\}$を提案する逆方向が必要になる。逆方向は一般に一意ではないため、確率モデルとして
$$
P(\mathbf{x}\mid \mathbf{y})
\propto
P(\mathbf{y}\mid \mathbf{x})P(\mathbf{x})
$$
の形で扱うと、候補の多様性と制約を同時に扱いやすい。ここで$P(\mathbf{x})$は、合成可能性や安全性、資源制約などを反映する事前分布として解釈できる。初学者にとって重要なのは、推論層は万能の黒箱ではなく、何を入力とし、何を出力とし、どの不確かさを出すかを決める設計である、という点である。

第三の制約層では、物理・化学・工学の整合性を明示し、推論層が提案できる範囲を制限する。制約は後付けのふるい落としではなく、モデルの中へ入れるほど効果が大きい。例えば、制約違反量を$\mathcal{R}(\hat{\mathbf{y}},\mathbf{x})$として、損失関数に
$$
\mathcal{L}
=
\mathcal{L}_{\mathrm{data}}(\mathbf{y},\hat{\mathbf{y}})
+\lambda \|\mathcal{R}(\hat{\mathbf{y}},\mathbf{x})\|^2
$$
を加えると、データに合うだけでなく制約を満たす解へ寄りやすくなる。化学では元素収支や電荷保存、危険条件の排除が制約になる。プロセスでは設備上の上限、工程の順序性、制御可能量の範囲が制約になる。物理では保存則、対称性、単位整合、境界条件満足が制約になる。統合とは、3層を段階ごとに整合させ、どこで何を担保しているかを、言葉と数式の両方で示せる状態にすることである。

この3層の理解は、領域横断を助ける。例えば、計測インフォマティクスで得た特徴量を材料設計へ渡すとき、表現層が一致していないと推論層が破綻する。物理制約を入れた代理モデルを探索に使うとき、制約層が曖昧だと提案が危険領域へ入る。化学の合成経路をプロセス最適化へ渡すとき、工程の順序性が表現層で失われると、現実に実行できない提案が生じる。統合設計は、こうした破綻を事前に防ぐための共通言語である。



### 14.4 検証設計

統合した枠組みは、当たった例を並べるだけでは学術的主張にならない。何を未知として扱い、どの条件で妥当で、どこから危険になるかを、検証設計で確定する必要がある。統合モデルは多くの部品から成るため、検証が弱いと、どの部品が結論を支えているのかが不明になり、再現性が担保できない。

第一に、データ分割をランダム分割に頼らないことが重要である。材料探索では、同じ系列の組成スイープや同一バッチが訓練と評価に混入すると、汎化ではなく近傍補間の性能を測ってしまう。したがって、化学空間（元素集合や酸化状態）、構造空間（プロトタイプや局所環境）、工程系列（同じ装置・同じレシピ群）でグループを作り、グループ単位で評価へ回す設計が必要になる。どの分割を選ぶかは、研究主張の形を決める。未知元素を主張するなら元素集合で分割すべきであり、未知構造を主張するならプロトタイプで分割すべきである。

第二に、不確かさの検証を必ず入れる。確率的予測を出すなら、その確率が現実の頻度と合うかを確認する必要がある。例えば、予測区間$I_i$が真値$y_i$を含むかどうかで被覆率を定義し、
$$
\mathrm{Cov}=\frac{1}{N}\sum_{i=1}^{N}\mathbf{1}\{y_i\in I_i\}
$$
が目標値（例えば95%）に近いかを評価する。被覆率が大きくずれる場合、モデルは過信または過小評価をしており、探索や意思決定に使うと危険になる。統合設計では、不確かさが意思決定に直結するため、校正は精度と同じくらい重要である。

第三に、制約満足の検証を独立に行う。物理制約を入れたと言っても、実際に満たされているかは別問題である。方程式残差、境界条件満足度、単位整合、保存則などを、予測精度とは別の指標として報告し、制約が形だけになっていないことを示す必要がある。例えば、残差$\mathcal{R}$の分布を評価し、特定領域で残差が大きいなら、その領域での適用に注意が必要である。

第四に、反例を含めることが重要である。成功例だけでは主張範囲が曖昧になるため、失敗条件や目的相が得られない条件を含めて評価し、どの境界で破綻し始めるかを示す必要がある。特に化学・プロセスでは失敗モードが多様であり、0/1の成否だけでなく、未反応、副生成物、再現性不良などの分類を持つと、どの制約が支配しているかを議論しやすくなる。

最後に、統合モデルは部品が多いため、寄与の切り分けが必要である。表現層の違い、前処理の違い、制約層の有無、分割設計の違いで結論が変わることがある。したがって、研究主張に直結する要素は、比較実験として「どの要素を変えたら何が変わるか」を示し、主張を支える要点を明確にすることが望ましい。



### 14.5 研究運用としての統合

統合の価値は単発の成果より、更新可能な研究基盤になる点にある。計測系の変更、装置更新、試料ロット変化、計算条件変更が起きても破綻しないよう、記録の仕方と更新の仕方を最初から設計する必要がある。統合設計は理論の話に見えるが、実際には「変数定義と追跡可能性」を徹底することで初めて成立する。

第一に、データ辞書を整備する。データ辞書とは、各項目の意味、単位、参照状態、符号規約、欠損の意味、前処理の定義を、文章と例で固定するものである。例えば磁化$M$が体積磁化（A/m）なのか質量磁化（A·m^2/kg）なのか、ヒステリシス測定の掃引速度や最大印加場が何か、XASの正規化がどのエネルギー区間か、といった定義が揺れると、学習は物理ではなく定義の差を拾う。辞書を持つことで、データが研究者間・年度間で混ざっても意味が保たれる。

第二に、バージョン管理を導入する。入力データ、前処理、特徴抽出、学習モデル、評価手順は、少し変えただけで結果が変わり得る。したがって、どの版の前処理$\phi$とどの版のモデル$f_\theta$で得た結果なのかを追跡できるようにし、再計算可能性を担保する必要がある。具体的には、入力・出力の対応付け、条件ファイルの保存、依存関係の記録を徹底し、どこまでが同一条件でどこからが変更かを明確にする。

第三に、共有は数値だけでなく手続きの共有を中心に置く。材料開発では、再現可能性は「同じ数値を出す」ことより「同じ手続きを踏めば同等の結論に至る」ことで担保される。したがって、公開や共同研究では、データの最小単位（値＋条件＋由来）、前処理の仕様、分割の仕様、不確かさ推定の仕様、制約の仕様を一体として提示する必要がある。これにより第三者が追試でき、学術的主張として成立する。

第四に、更新の設計を持つ。統合基盤は固定物ではなく、データが増え、装置が変わり、理解が進むことで更新される。このとき、過去の結果と新しい結果が矛盾した場合に、どちらが間違いかを直ちに断定するのではなく、定義の差、装置差、分割差、制約差を順に点検し、原因を特定できる仕組みを持つことが重要である。統合設計が研究組織の資産になるかどうかは、更新に耐えるかどうかで決まる。



## 第15章 おわりに

本書で扱ったMIは、学習器の選択そのものより、研究の各段階を記述可能な形に整え、制約と検証で支えながら、探索と理解を同時に進める方法論である。材料・計測・プロセス・物理・化学を結び、どの段階で何を定義し、どの段階で何を確かめるかを明文化することで、研究の速度と信頼性を同時に上げられる。

今後は、化学・プロセス・計測の自律化が進み、データ取得の速度が上がる一方で、外挿検知と不確かさの校正、物理制約の検証がより重要になる。探索が高速化するほど、誤った高速化も同じ速度で起こり得るため、主張範囲の確定と再現可能性の担保が研究の中心課題になる。さらに、マルチモーダル（組成・構造・画像・スペクトル・工程系列）を同時に扱う枠組みが一般化するほど、表現層の設計と同一性の定義が難しくなるため、データ辞書と観測モデルの明示が不可欠になる。

学術的には、当たる予測より、なぜ当たるのかを説明でき、どこまで妥当かを宣言できる枠組みが重みを増す。物理制約学習やマルチフィデリティ統合は、データが薄い領域での推論を支えるが、その効果を示すには残差評価や校正評価を含む検証が必要である。化学側では、失敗条件の体系化と合成経路の構造化が進むほど、材料探索が実行可能な提案へ収束しやすくなるため、成功例だけの記録から脱却することが鍵になる。

最終的にMIは、材料科学における新しい研究規範として位置づく。すなわち、データと手続きを一体で提示し、制約と検証で主張範囲を確定し、不確かさを意思決定へ接続するという姿勢である。この規範が広がるほど、研究成果は個別の成功から再利用可能な指針へ変わり、材料開発はより速く、より確かな形で社会実装へ接続していくと考えられる。

