# サポートベクターマシン（SVM）

サポートベクターマシン（SVM）は、マージン最大化に基づく教師あり学習であり、少量データでも高次元特徴量に強い分類・回帰モデルである。材料科学では、組成・構造記述子・プロセス条件・画像特徴などの表形式データを強力に扱える標準手法の一つである。

## 参考ドキュメント
- Cortes, C. and Vapnik, V., Support-Vector Networks (Machine Learning, 1995)
  https://link.springer.com/content/pdf/10.1007/BF00994018.pdf
- Smola, A.J. and Schölkopf, B., A tutorial on support vector regression (Statistics and Computing, 2004)
  https://alex.smola.org/papers/2004/SmoSch04.pdf
- IBM: サポート・ベクトル・マシン（SVM）とは（日本語）
  https://www.ibm.com/jp-ja/think/topics/support-vector-machine


## 1. 位置づけ
SVMが材料データで実用的になりやすい状況は次の通りである。
- データ数が中小規模である（数十〜数万程度、特に数百〜数千で強いことが多い）
- 特徴量が高次元である（記述子が多い、スパース、相互作用が非線形）
- クラス境界が単純ではない（相分類、合成成功/失敗、欠陥の有無など）
- 回帰の非線形近似が必要である（物性予測、プロセス→特性の写像など）

一方、系列・画像・グラフを生のまま学習する用途では、深層学習（CNN/Transformer/GNN）が優先されることも多い。SVMは、特徴抽出（ピーク量、統計量、形態量など）を先に行い、その後段の予測器として配置される場面で強い。

## 2. SVM（分類）の核心：マージン最大化
### 2.1 線形SVM（ハードマージン）
二値分類で、ラベルを $y_i\in\{-1,+1\}$ とし、分離超平面を $f(x)=w^\top x + b$ とする。
ハードマージンSVMは次の最適化で与えられる。

$$
\min_{w,b}\;\frac{1}{2}\|w\|^2
\quad \text{s.t.}\quad
y_i(w^\top x_i+b)\ge 1
$$

$\|w\|$ を小さくすることは、幾何学的マージンを大きくすることに対応する。

### 2.2 ソフトマージン（ノイズ・重なりへの対応）
材料データでは測定誤差・条件差・外れ値があり、完全分離が成立しないことが多い。その場合はスラック変数 $\xi_i$ を導入する。

$$
\min_{w,b,\xi}\;\frac{1}{2}\|w\|^2 + C\sum_i \xi_i
\quad \text{s.t.}\quad
y_i(w^\top x_i+b)\ge 1-\xi_i,\;\xi_i\ge 0
$$

$C$ は誤分類（マージン違反）への罰則であり、一般化と当てはまりのトレードオフを支配する。

### 2.3 双対問題とサポートベクター
最適化を双対化すると、予測関数は一部の点（サポートベクター）のみで表現される。

$$
f(x) = \mathrm{sign}\left(\sum_{i\in SV}\alpha_i y_i K(x_i, x) + b\right)
$$

サポートベクターは、境界の近傍（あるいはマージン違反）に位置し、決定境界を規定する点である。材料データでは、境界近傍の試料（相境界組成、条件の閾値近傍など）を抽出できる場合がある。

## 3. カーネル法（非線形性を入れる標準手段）
SVMの強力さの一つは、特徴写像 $\phi(x)$ を陽に構成せずに内積を計算するカーネルトリックである。

$$
K(x,x') = \phi(x)^\top \phi(x')
$$

代表的カーネルは次の通りである。

| カーネル | 形 | 直観 | 材料データでの典型 |
|---|---|---|---|
| 線形 | $K(x,x')=x^\top x'$ | 線形境界 | 記述子が十分で、説明性も欲しい場合 |
| 多項式 | $K=(\gamma x^\top x' + r)^p$ | 相互作用を増やす | 相互作用次数を限定して試す場合 |
| RBF（ガウス） | $K=\exp(-\gamma\|x-x'\|^2)$ | 局所類似度 | 最初に試す非線形の定番、特徴量スケールに敏感 |

材料科学で重要なのは、類似度の設計である。
- 組成ベース：元素比＋元素物性統計の距離
- 構造ベース：局所環境の類似度（例：局所構造記述子の距離）
- 画像ベース：形態量・テクスチャ量の距離
SVMは、類似度（距離）の良し悪しが性能に直結しやすい。

## 4. SVR（回帰）：ε-不感帯と平滑さ
SVR（Support Vector Regression）は、回帰版SVMであり、誤差が $\varepsilon$ 以内なら罰しない ε-不感損失を用いることが多い。

$$
L_\varepsilon(y,f(x)) = \max(0,\;|y-f(x)|-\varepsilon)
$$

代表的な定式化（概念）：
$$
\min_{w,b,\xi,\xi^\*}\;\frac{1}{2}\|w\|^2 + C\sum_i(\xi_i+\xi_i^\*)
$$
$$
\text{s.t.}\;\;
y_i-(w^\top \phi(x_i)+b)\le \varepsilon+\xi_i,\;\;
(w^\top \phi(x_i)+b)-y_i\le \varepsilon+\xi_i^\*,\;\;
\xi_i,\xi_i^\*\ge 0
$$

$\varepsilon$ はノイズ許容（どこまで誤差を無視するか）であり、計測誤差スケールを反映すると安定しやすい。

## 5. 入力設計（特徴量の作り方）
SVM/SVRは表形式が得意であるため、材料情報を特徴量 $x$ に落とす設計が重要である。

- 組成
  - 元素比（連続値）
  - 元素物性統計（原子半径、電気陰性度、価電子数などの平均との差・分散など）
- 構造
  - 近接距離統計、配位数統計、局所環境記述子の要約
  - 結晶系・空間群などのカテゴリ（扱いは要注意）
- プロセス
  - 温度、時間、雰囲気、冷却速度、圧力など
- スペクトル・回折
  - ピーク位置・強度・幅、ピーク比、領域積分などの要約特徴
- 画像（組織・顕微鏡）
  - 粒径分布統計、形態量、テクスチャ特徴（例：HOG/GLCM系）など

SVMは特徴量スケールに敏感である（特にRBF）。材料データでは単位混在が起きやすく、スケーリング方針を固定してから評価するのが実務的である。

## 6. ハイパーパラメータの要点
- 分類（SVC）
  - C：マージン違反をどれだけ許すか
  - kernel：線形かRBFか等
  - γ（RBF）：局所性の強さ（大きいほど近傍に過剰適合しやすい）
- 回帰（SVR）
  - C：許容違反への罰則
  - ε：ノイズ許容帯（測定誤差の目安に合わせるとよい）
  - γ（RBF）：非線形度合い

材料分野では、ハイパーパラメータ探索より先に、データ分割の設計が重要である（後述）。

## 7. 評価設計
- 近縁材料リークに注意が必要である
  - 同一系列（組成掃引、熱処理掃引）が学習と評価に跨ると過大評価になりやすい
- 外挿評価を明示するのが望ましい
  - 未知組成域、未知元素系、未知プロセス条件で性能が落ちないかを確認する
- 不均衡分類が多い
  - 例：合成成功は少数、欠陥は少数
  - 指標はAccuracyではなく、F1、AUROC、AUPRCなども併用する

## 8. 解釈とXAIの補助線
- サポートベクターを見ると、境界を規定する代表試料が分かる場合がある（相境界・条件閾値の近傍など）
- 線形SVMでは $w$ の符号と大きさが方向性を示すが、相関と因果は別である
- 非線形SVMは直接の係数解釈が難しいため、Permutation importanceやSHAPなどの外部手法を併用するのが現実的である

## まとめ
SVM/SVRは、マージン最大化とカーネル法により、少量でも高次元・非線形な材料データを高精度に扱える教師あり学習である。材料科学での成功要因は、モデル選択そのものよりも、特徴量設計（類似度の設計）、スケーリング、近縁材料リークを避けた分割、外挿評価の設計にある。
