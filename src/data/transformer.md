# Transformer（トランスフォーマー）

Transformerは、注意機構（self-attention）を中核とするニューラルネットワークのアーキテクチャであり、「系列・集合・グラフ・画像パッチ」などの入力から表現（representation）を学習する汎用的な枠組みである。材料科学では、組成・構造記述子・スペクトル・文献テキストなどを統一的に扱える表現学習器として位置づけられる。


## 参考ドキュメント
- Vaswani et al., Attention Is All You Need (NeurIPS 2017)
  https://papers.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf
- Harvard NLP: The Annotated Transformer（概念と実装の解説）
  https://nlp.seas.harvard.edu/annotated-transformer/
- キカガクブログ：Transformerの仕組み解説（日本語）
  https://www.kikagaku.co.jp/kikagaku-blog/deep-learning-transformer/


## 1. 位置づけ
Transformerは、アルゴリズム分類で言うと次の層に属する。

- 学習パラダイム（上位概念）
  - 教師あり学習：分類・回帰
  - 教師なし学習：クラスタリング、密度推定
  - 自己教師あり学習：マスク復元、対比学習など
  - 強化学習：方策学習（ただし材料分野では周辺用途が多い）

- モデル（関数近似器）
  - ニューラルネットワーク（深層学習）

- アーキテクチャ（設計）
  - CNN：局所畳み込みの帰納バイアス
  - RNN/LSTM：逐次状態更新の帰納バイアス
  - GNN：グラフのメッセージパッシング
  - Transformer：注意による「全体参照（global interaction）」を学ぶ

結論として、Transformerは「深層学習における表現学習アーキテクチャ」であり、生成/非生成は学習目的と入出力設計で決まる。


## 2. 中核の数式：Self-Attention（Scaled Dot-Product）
長さ $L$、埋め込み次元 $d$ の入力列（またはトークン集合）を $X\in\mathbb{R}^{L\times d}$ とする。

線形変換で
$$
Q=XW^Q,\quad K=XW^K,\quad V=XW^V
$$

注意は
$$
\mathrm{Attn}(Q,K,V)=\mathrm{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
$$

この計算により、各トークンが他トークン（全体）を参照して特徴を再構成する。材料データでは「元素間」「ピーク間」「原子環境間」などの相互作用表現に相当する。


## 3. 代表的な構成と対応タスク
Transformerは大きく3系統に分かれるが、いずれも分類・回帰・推定に使える。

- エンコーダ型（例：BERT系の考え方）
  - 入力全体から文脈表現を作る
  - 主用途：分類、回帰、抽出、特徴量化（埋め込み）

- エンコーダ・デコーダ型（例：翻訳系の考え方）
  - 入力を条件として出力系列を推定する
  - 主用途：条件付き予測、変換、マルチモーダル対応（テキスト→条件表、スペクトル→相など）

- デコーダ型（自己回帰の話に寄りやすいが、ここでは“系列予測器”）
  - 逐次的な系列推定が得意
  - 主用途：時系列予測、逐次推定、シミュレーション系列のモデル化


## 4. Transformerの強みを「帰納バイアス」で理解する
### 4.1 強み
- 長距離相互作用を直接扱う：$L$ ステップ離れた依存関係も1層で接続できる
- 入力形式の柔軟性：トークン化できればテキスト/スペクトル/組成/画像パッチなどに拡張しやすい
- 表現学習が強い：自己教師あり学習と相性が良く、下流タスク（回帰・分類）へ転用しやすい

### 4.2 弱み
- 計算量：標準注意は $O(L^2)$ になりやすく、長い系列（高分解能スペクトル、長文）で重い
- 物理制約の帰納バイアスが弱い：結晶対称性、PBC、不変性は別途設計が必要である
- データ分割に敏感：近縁系混入で見かけ性能が上がりやすく、材料ドメインでは評価設計が重要である


## 5. 材料科学での「入力設計」：何をトークンにするか
Transformer適用は、材料情報をトークン列（または集合）へ落とす設計で決まる。

- 組成：元素トークン + 含有量（連続値） + 追加特徴（原子半径、電気陰性度など）
- スペクトル（XRD/XAFS/XPSなど）：
  - 区間パッチ（局所窓）をトークン化
  - ピーク列（位置・幅・強度）をトークン化
- 結晶構造：
  - 局所環境（配位ポリヘドラ、近接原子の特徴）をトークン化
  - もしくはグラフ（GNN）× Transformer のハイブリッド
- 画像（組織・顕微鏡）：パッチ列（ViTの考え方）

材料で重要な制約
- 並進・回転・入れ替え不変性
- 周期境界（PBC）
- 化学的妥当性（距離、配位、価数）
これらは表現側（前処理・記述子）か、モデル側（等変性・制約付き設計）で組み込む必要がある。


## 6. 学習の枠組み
Transformerは損失関数を変えるだけで役割が変わる。

- 回帰（物性予測）
$$
\mathcal{L}=\frac{1}{N}\sum_{i=1}^N \left(y_i-\hat{y}_i\right)^2
$$

- 分類（相分類・合成成功判定）
$$
\mathcal{L}=-\frac{1}{N}\sum_{i=1}^N \sum_{c} y_{ic}\log \hat{p}_{ic}
$$

- 自己教師あり（表現学習）
  - マスク復元：入力の一部を隠して復元（テキスト・スペクトルで有効）
  - 対比学習：同一試料の別観測（ノイズ付与、測定条件違い）を近づける

材料分野では「自己教師ありで埋め込みを作り、少数ラベルで微調整」が効きやすい。


## 7. 評価の観点
- 外挿評価：未知元素系、未知組成域、未知プロセス域で性能が落ちないか
- データリーク対策：同一物質の表記揺れ、近縁構造の混入を避けた分割か
- 物理整合性：単位・条件（温度、相、測定法）を揃えた評価か
- 解釈：注意の可視化は参考情報であり、因果の証明ではない


## まとめ
- Transformerは、注意機構により入力全体の相互作用を学ぶ深層学習アーキテクチャであり、機械学習の枠組みでは「表現学習器・予測器」として位置づけられる。材料科学では、組成・スペクトル・文献・画像などをトークン化して統一的に扱える一方、物理制約と評価設計を先に置くことが実用上の鍵である。
- Transformerを材料に適用する際は、モデルよりも「表現（トークン化）」「制約」「データ分割」「評価設計」が支配的である。特に結晶・原子配置は不変性とPBCの扱いが難所であり、必要に応じてGNNや等変性モデルとの併用を前提に設計すべきである。
