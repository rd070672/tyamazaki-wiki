# Transformer（トランスフォーマー）

Transformerは、注意機構（self-attention）を中核として「入力要素どうしの相互参照（global interaction）」を学習するニューラルネットワークのアーキテクチャである。系列（テキスト、時系列）に限らず、集合（組成ベクトル、ピーク集合）、グラフ（原子ネットワーク）、画像パッチ（顕微鏡像、回折像）など、要素をトークン（token）に分解できる入力に対して汎用的に適用できる。

材料科学の観点では、(i) 組成・構造・スペクトル・テキストといった異種データを「トークン列／トークン集合」に統一して扱える点、(ii) 自己教師あり学習（マスク復元、対比学習）によってラベルの少ない領域で表現（embedding）を作れる点、(iii) 得られた表現を回帰・分類・生成へ転用できる点が重要である。一方で、結晶対称性・回転不変性・周期境界（PBC）など物理制約は標準Transformerだけでは自然に入らないため、入力設計と評価設計が性能と信頼性を支配する。

## 参考ドキュメント
- Vaswani et al., Attention Is All You Need (NeurIPS 2017)
  https://papers.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf
- Harvard NLP: The Annotated Transformer（概念と実装の解説）
  https://nlp.seas.harvard.edu/annotated-transformer/
- キカガクブログ：Transformerの仕組み解説（日本語）
  https://www.kikagaku.co.jp/kikagaku-blog/deep-learning-transformer/



## 1. アルゴリズム分類と「何が新しいか」

Transformerは「深層学習モデル」であり、生成モデルかどうかはアーキテクチャそのものではなく、学習目的（損失関数）と入出力設計で決まる。分類・回帰にも、系列生成にも、条件付き予測にも同じ骨格が使える。

- 学習パラダイム
  - 教師あり学習：分類・回帰（物性予測、相分類、合成成功判定など）
  - 教師なし学習：密度推定・クラスタリング（表現空間での群分け）
  - 自己教師あり学習：マスク復元、対比学習（少数ラベルの材料領域で強力）
  - 強化学習：探索・最適化（材料探索は「周辺用途」として使われがち）

- アーキテクチャ
  - CNN：局所畳み込み（近傍のみ参照する先入観）
  - RNN/LSTM：逐次状態更新（時間方向の因果を強く仮定）
  - GNN：グラフ上のメッセージパッシング（近接原子相互作用の先入観）
  - Transformer：注意により「どの要素と相互作用するか」をデータから学ぶ（全結合参照が可能）

Transformerの本質的な差分は、入力要素間の依存関係を固定の近傍規則に縛らず、
「どの要素がどの要素を参照すべきか」を重みとして学習する点にある。
材料では、元素間相互作用、ピーク間対応、局所環境間の関係、文献中の概念依存などを、
同一の計算構造（注意）として記述できる。



## 2. Self-Attention（Scaled Dot-Product Attention）を丁寧に

### 2.1 記号と形（shape）
長さ（トークン数）を $L$、埋め込み次元を $d$ とし、入力を
$$
X \in \mathbb{R}^{L \times d}
$$
とする（行がトークン、列が特徴次元）。

Transformerは各トークンに対して「問い合わせ（Query）」「鍵（Key）」「値（Value）」を作る：
$$
Q = XW^Q,\quad K = XW^K,\quad V = XW^V,
$$
ここで
$$
W^Q, W^K \in \mathbb{R}^{d \times d_k},\quad W^V \in \mathbb{R}^{d \times d_v},
$$
したがって
$$
Q,K \in \mathbb{R}^{L \times d_k},\quad V \in \mathbb{R}^{L \times d_v}.
$$

### 2.2 注意重み
注意の核は、各トークン $i$ が他のトークン $j$ をどれだけ参照するかを
内積（類似度）で測り、それをsoftmaxで確率化して重み付き平均を取る点にある。

スコア行列（相互類似度）：
$$
S = \frac{QK^\top}{\sqrt{d_k}} \in \mathbb{R}^{L \times L}.
$$

注意重み（各行が確率分布）：
$$
A = \mathrm{softmax}(S),\quad
A_{ij} = \frac{\exp(S_{ij})}{\sum_{j'=1}^{L} \exp(S_{ij'})}.
$$

出力（重み付き和）：
$$
\mathrm{Attn}(Q,K,V) = AV \in \mathbb{R}^{L \times d_v}.
$$

この式を「1トークンずつ」書くと直感が出る：
$$
\mathrm{Attn}(Q,K,V)_i = \sum_{j=1}^{L} A_{ij} V_j,
$$
すなわち、トークン $i$ の新しい表現は「他トークンの値 $V_j$ の加重平均」である。

### 2.3 なぜ $\sqrt{d_k}$ で割るのか
内積 $q_i \cdot k_j$ は次元 $d_k$ が大きいほど分散が大きくなり、softmaxが極端（ほぼ0/1）になりやすい。
softmaxの勾配が飽和すると学習が不安定になるため、分散をならす目的で
$$
S_{ij} = \frac{q_i \cdot k_j}{\sqrt{d_k}}
$$
とスケーリングする。経験的にも理論的にも「学習を安定化する定番の正規化」である。

### 2.4 マスク
系列予測（自己回帰）では未来トークンを参照してはいけない。そこでマスク $M$ を導入する：
$$
S' = S + M,
$$
ここで
- 許可する場合：$M_{ij}=0$
- 禁止する場合：$M_{ij}=-\infty$（実装では大きな負値）

として
$$
A = \mathrm{softmax}(S')
$$
とすると、禁止領域は確率が0になる。

典型例：因果マスク（causal mask）
$$
M_{ij} =
\begin{cases}
0 & (j \le i)\\
-\infty & (j>i)
\end{cases}
$$

材料では「未知条件の情報リーク」を防ぐ観点でマスクを使う発想もある（例：測定条件トークンを参照できる範囲を制限する、欠損領域を復元するためのマスクなど）。



## 3. Multi-Head Attention

Self-attentionを1回計算するだけだと、類似度の定義（$Q,K$ の射影）が1種類に固定される。
Multi-head attentionは、異なる射影を $h$ 個用意し、「複数の観点での参照」を並列に計算して結合する。

ヘッド $m=1,\dots,h$ に対して
$$
Q^{(m)} = XW^{Q(m)},\quad
K^{(m)} = XW^{K(m)},\quad
V^{(m)} = XW^{V(m)}.
$$

各ヘッドの注意：
$$
H^{(m)} = \mathrm{softmax}\!\left(\frac{Q^{(m)}(K^{(m)})^\top}{\sqrt{d_k}}\right)V^{(m)}.
$$

結合と出力射影：
$$
\mathrm{MHA}(X)= \mathrm{Concat}\left(H^{(1)},\dots,H^{(h)}\right)W^O.
$$

材料データでは、例えば
- あるヘッドは「元素種の相性（化学的類似）」、
- 別のヘッドは「含有量や電気陰性度差」、
- 別のヘッドは「スペクトル局所形状（ピーク周り）」、
といった異なる関連性を同時に拾う、という解釈がしやすい（ただし注意の可視化は因果証明ではなく、あくまで手がかりである）。



## 4. Transformerブロック（残差・正規化・FFN）を数式で

Transformerの基本ブロックは概ね
1) 注意（Self-Attention / Cross-Attention）
2) 位置ごとのMLP（Feed-Forward Network: FFN）
を、残差接続とLayerNormで安定化しながら積み重ねる。

### 4.1 残差接続とLayerNorm（Pre-Normの形）
実装でよく用いられるPre-Norm形（安定しやすい）：
$$
\tilde{X} = X + \mathrm{Dropout}(\mathrm{MHA}(\mathrm{LN}(X))),
$$
$$
Y = \tilde{X} + \mathrm{Dropout}(\mathrm{FFN}(\mathrm{LN}(\tilde{X}))).
$$

LayerNorm（トークンごとに特徴次元を正規化）：
$$
\mathrm{LN}(x)=\gamma \odot \frac{x-\mu}{\sqrt{\sigma^2+\epsilon}} + \beta,
$$
ここで $\mu,\sigma^2$ は特徴次元に沿った平均・分散、$\gamma,\beta$ は学習可能パラメータ。

残差（$X + \cdot$）は勾配消失を防ぎ、深いネットワークの学習を可能にする。

### 4.2 Position-wise FFN
FFNは各トークンに同一の2層MLP（非線形）を適用する：
$$
\mathrm{FFN}(x)=\phi(xW_1+b_1)W_2+b_2,
$$
典型的に
- 入力：$d$
- 中間：$d_{\mathrm{ff}}$（$d$ の数倍）
- 出力：$d$
で、$\phi$ はReLUやGELUなど。

注意が「トークン間の混合（mixing）」を担い、FFNが「特徴次元方向の非線形変換」を担う、と分けて理解すると分かりやすい。



## 5. 位置情報と「順序の注入」

Self-attentionは本質的に「集合（順序を持たない）」として入力を扱う性質がある。
系列としての順序や、スペクトルのエネルギー軸の並び、画像パッチの空間配置を扱うには、位置情報（Positional Encoding）を埋め込みに足す必要がある。

### 5.1 絶対位置のサイン・コサイン
位置 $p$ と次元 $i$ に対して
$$
\mathrm{PE}(p,2i)=\sin\left(\frac{p}{10000^{2i/d}}\right),\quad
\mathrm{PE}(p,2i+1)=\cos\left(\frac{p}{10000^{2i/d}}\right).
$$
入力に加える：
$$
X^{(0)} = E + \mathrm{PE},
$$
ここで $E$ はトークン埋め込み（元素埋め込み、パッチ埋め込みなど）。

### 5.2 相対位置
「隣接しているか」「どれだけ離れているか」を使いたい場合、相対位置バイアスをスコアに足す：
$$
S_{ij} = \frac{q_i\cdot k_j}{\sqrt{d_k}} + b_{(i-j)}.
$$
スペクトルでは「エネルギー差」、結晶・原子環境では「距離差」や「殻（shell）」に対応する設計がしやすい。

材料系データは、単なる順序よりも「距離・角度・隣接関係」が効くことが多く、位置情報の設計は性能のボトルネックになりやすい。



## 6. エンコーダ／デコーダ／クロス注意

### 6.1 エンコーダ
エンコーダは入力 $X$ を文脈化して表現 $H$ を作る：
$$
H = \mathrm{Encoder}(X).
$$
分類・回帰では、$H$ の集約（平均、[CLS]トークンなど）から予測する：
$$
\hat{y} = g(\mathrm{Pool}(H)).
$$

### 6.2 デコーダと因果マスク
デコーダは過去の出力トークン列 $Y_{<t}$ を因果マスク付き自己注意で処理し、次トークンを予測する：
$$
p(y_t \mid y_{<t}, X) = \mathrm{softmax}(W h_t).
$$

### 6.3 クロス注意
エンコーダ・デコーダ型では、デコーダがエンコーダ出力 $H$ を参照する（cross-attention）。
デコーダ側の状態を $Z$ とすると
$$
\mathrm{CrossAttn}(Z,H)=\mathrm{Attn}(ZW^Q,\ HW^K,\ HW^V).
$$
これにより「入力（組成・スペクトル・条件）を見ながら出力（相、物性、生成列）を作る」条件付き予測が可能になる。

材料で言えば、例として
- スペクトル → 相ラベル列（もしくは相確率）
- 文章（合成手順） → 条件テーブル
- 組成＋プロセス条件 → 物性分布（平均と不確かさ）
のような変換の核になる。



## 7. 計算量と「長い入力が重い理由」

標準の注意では $S = QK^\top$ を計算するため、計算量・メモリ量は概ね
$$
O(L^2 d)
$$
になりやすい。スペクトルを高分解能のまま $L$ を大きくすると急に重くなるのは、この二乗依存が原因である。

対策の方向性（考え方だけ押さえる）：
- スパース注意：参照先を近傍や重要トークンに限定して $L^2$ を減らす
- 低ランク近似：$S$ を低ランクで近似して計算を軽くする
- 線形注意（カーネル化）：softmax注意を特徴写像 $\varphi(\cdot)$ で近似し、
  $$ 
  \mathrm{Attn}(Q,K,V) \approx \left(\varphi(Q)\left(\varphi(K)^\top V\right)\right)
  $$
  のように括り方を変えて $O(L d^2)$ 程度に落とす発想（実装細部は多様）

材料では、(i) スペクトルをパッチ化して $L$ を減らす、(ii) 重要ピークのみトークン化する、(iii) マルチスケール（粗→細）で扱う、などが現実的な落とし所になることが多い。



## 8. 代表的な構成と対応タスク

Transformerは大きく3系統に分かれるが、いずれも分類・回帰・推定に使える（生成専用ではない）。

| 系統 | 代表的な役割 | 材料での例 |
|---|---|---|
| エンコーダ型 | 入力全体から表現 $H$ を作り、下流タスクへ | 物性回帰、相分類、埋め込みでクラスタリング |
| エンコーダ・デコーダ型 | 入力を条件に出力系列を推定 | 条件付き予測、スペクトル→相、テキスト→条件表 |
| デコーダ型 | 因果マスクで系列を逐次生成 | 時系列予測、プロセス系列のモデル化、生成（候補列） |

重要なのは「同じTransformerでも、(i) 入力のトークン化、(ii) 出力の設計（単一値か系列か）、(iii) 損失関数」で性格が変わる点である。



## 9. 材料科学での「入力設計」

Transformer適用は、材料情報をトークン列（または集合）へ落とす設計でほぼ決まる。

### 9.1 組成をトークン化
典型案：
- 元素トークン（元素IDの埋め込み）＋含有量（連続値）＋属性（原子半径、電気陰性度など）

例：元素 $e$ の埋め込み $u_e$ と含有量 $c_e$ を用いて
$$
x_e = u_e + f(c_e),
$$
あるいは連続値をスケーリングして結合：
$$
x_e = [u_e;\ c_e;\ \text{feat}(e)]W.
$$
ここで $[\ ;\ ]$ は連結、$\text{feat}(e)$ は元素属性ベクトル。

注意点：組成（composition）は本質的に「集合」であり、元素順序は意味を持たない。
したがってモデルは「順序不変（permutation invariance）」または「順序等変（equivariance）」を意識すべきである。
素朴に並べた列に位置符号を入れると、不必要な順序依存が混ざることがある。

### 9.2 スペクトル（XRD/XAFS/XPSなど）
スペクトル $s(E)$ をそのまま $L$ 点の系列として入れると $L^2$ が重い。
よくある実用設計：
- パッチ化：窓幅 $w$ の区間を1トークンにする
- ピーク抽出：ピーク位置 $E_p$、幅 $\Gamma_p$、強度 $I_p$ の列をトークン化

ピーク列トークン例：
$$
x_p = [E_p,\ \Gamma_p,\ I_p]W + b
$$
により各ピークを埋め込みに写し、ピーク集合として注意で相互参照させる。
この場合、位置情報は「エネルギー値 $E_p$ 自体」が入るため、単純な位置符号より自然なことが多い。

### 9.3 結晶構造・原子配置
原子 $i$ の近傍環境を特徴化してトークン化する：
$$
x_i = \mathrm{Embed}\left(z_i,\ \{(z_j,\ r_{ij},\ \theta_{ijk},\dots)\}_{j\in \mathcal{N}(i)}\right).
$$
ただし回転不変性・等変性（ベクトルが回転に従って変換する性質）が絡むため、標準Transformerだけで自然に扱うのは難しい。
現実には
- グラフ（GNN）で局所表現を作ってからTransformerでグローバル統合
- 等変性を意識した設計（回転・並進・入れ替えの扱い）
のようなハイブリッド設計が多い。

### 9.4 画像（組織・顕微鏡）とパッチ
画像を $P\times P$ のパッチに分割し、各パッチを線形写像で埋め込む：
$$
x_p = \mathrm{Flatten}(\mathrm{Patch}_p)W + b.
$$
パッチ位置（2D座標）を位置埋め込みとして追加し、注意で遠距離相関（長距離ドメイン相関など）を学ぶ。



## 10. 学習の枠組み（損失関数）

### 10.1 回帰（物性予測）
MSE（平均二乗誤差）：
$$
\mathcal{L}_{\mathrm{MSE}}=\frac{1}{N}\sum_{i=1}^N \left(y_i-\hat{y}_i\right)^2
$$
MAE（平均絶対誤差）：
$$
\mathcal{L}_{\mathrm{MAE}}=\frac{1}{N}\sum_{i=1}^N \left|y_i-\hat{y}_i\right|
$$
材料では外れ値や分布の歪みが効くことがあるため、MAEやHuber損失も検討対象になる。

不確かさ（分散）まで出す設計例（ガウス尤度）：
$$
p(y\mid x)=\mathcal{N}(y;\ \mu(x),\ \sigma^2(x)),
$$
負の対数尤度を最小化：
$$
\mathcal{L} = \frac{1}{N}\sum_{i=1}^N
\left[
\frac{(y_i-\mu_i)^2}{2\sigma_i^2} + \frac{1}{2}\log\sigma_i^2
\right].
$$
測定誤差・条件ばらつきが大きい材料データでは、平均だけでなく不確かさを一緒に出すと運用がしやすい。

### 10.2 分類（相分類・合成成功判定）
クロスエントロピー：
$$
\mathcal{L}_{\mathrm{CE}}=
-\frac{1}{N}\sum_{i=1}^N \sum_{c=1}^{C} y_{ic}\log \hat{p}_{ic}.
$$

### 10.3 自己教師あり（表現学習）
(1) マスク復元（Masked Modeling）
入力トークンの一部を隠して復元する。マスク集合を $\mathcal{M}$ とすると
$$
\mathcal{L}_{\mathrm{mask}} = -\sum_{t\in\mathcal{M}} \log p(x_t\mid x_{\setminus \mathcal{M}}).
$$
スペクトルなら「欠損区間を復元」、画像なら「欠損パッチ復元」、組成なら「元素を隠して復元」などに対応する。

(2) 対比学習（InfoNCEの形）
同一試料の別観測（ノイズ付与、測定条件違い、拡張）を正例（positive）として近づけ、他試料を負例（negative）として離す。
埋め込み $z$ を用いて
$$
\mathcal{L}_{\mathrm{NCE}} = -\log
\frac{\exp(\mathrm{sim}(z,z^+)/\tau)}
{\exp(\mathrm{sim}(z,z^+)/\tau)+\sum_{k}\exp(\mathrm{sim}(z,z_k^-)/\tau)}.
$$
ここで $\tau$ は温度、$\mathrm{sim}$ はコサイン類似度など。
材料では「同一材料の測定条件違い」を正例にできることが多く、ラベル不足を補う戦略として有効になりやすい。



## 11. Transformerの特徴

### 11.1 強み
- 長距離相互作用を直接扱える：任意の $i,j$ が1層で相互参照可能（$A_{ij}$ が直接学習される）
- 入力形式の柔軟性：トークン化できればテキスト／スペクトル／組成／画像パッチへ拡張可能
- 自己教師あり学習と相性が良い：下流タスクに転用しやすい（埋め込みを作って微調整）

### 11.2 弱み
- 計算量：標準注意は $O(L^2 d)$（長いスペクトルや高解像画像で重い）
- 物理制約の帰納バイアスが弱い：回転・並進・PBC・化学的妥当性を別途入れる必要
- 評価が難しい：近縁系混入（データリーク）で見かけ性能が上がりやすい。材料ドメインでは分割設計が性能の解釈を決める



## 12. 評価の観点

### 12.1 外挿評価（distribution shift）
外挿評価（distribution shift）は、未知元素系、未知組成域、未知プロセス条件で性能が落ちないかを見る。
訓練分布 $p_{\mathrm{train}}(x)$ と運用分布 $p_{\mathrm{test}}(x)$ が異なるとき、
単純な平均精度は過大評価になりうる。

### 12.2 データリーク対策（材料で頻出）
- 同一物質の表記揺れ（ICSD/Materials Projectの重複、組成の丸め違い）
- 近縁構造（置換系列や格子定数の微差）が訓練とテストに跨る
- 同一測定ロット・同一論文由来が跨る

これらを避けるには、化学系（family）単位、構造プロトタイプ単位、論文単位などで分割する発想が必要になる。

### 12.3 指標（回帰・分類）
回帰：RMSE, MAE, $R^2$
$$
\mathrm{RMSE}=\sqrt{\frac{1}{N}\sum_{i}(y_i-\hat{y}_i)^2},\quad
R^2 = 1-\frac{\sum_i (y_i-\hat{y}_i)^2}{\sum_i (y_i-\bar{y})^2}.
$$
分類：Accuracy, F1, AUROC など（不均衡ならF1やAUROCが重要）

### 12.4 解釈（注意の可視化）
注意重み $A_{ij}$ を可視化すると「どこを参照しているか」の手がかりになるが、
それは因果（この特徴が物性を決める）を証明するものではない。
解釈を強めるには、アブレーション（トークン削除）、反事実テスト、物理整合性チェックなどと組み合わせる必要がある。



## まとめ
- Transformerは、注意機構により入力全体の相互作用を学ぶ深層学習アーキテクチャであり、生成か非生成かは学習目的と入出力設計で決まる。中核式は
  $$
  \mathrm{Attn}(Q,K,V)=\mathrm{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
  $$
  で表され、各トークンが他トークンを参照して表現を再構成する。
- 材料科学への適用では、モデル選択以上に「トークン化（表現設計）」「物理制約（不変性・PBC・化学妥当性）」「データ分割と外挿評価」が支配的である。特に結晶・原子配置は回転不変性や周期境界が難所であり、必要に応じてグラフ表現や等変性の考え方を併用する設計が現実的である。
- 学習は回帰・分類だけでなく、マスク復元や対比学習など自己教師ありにより表現を作り、少数ラベルで微調整する枠組みが材料データと相性が良い。運用上は「当てる」だけでなく、不確かさ・リーク対策・外挿耐性を含めた評価設計が重要になる。
