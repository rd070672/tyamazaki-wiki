# PHOTON ~Transformerの次へ~

PHOTON（Parallel Hierarchical Operation for TOp-down Networks）は、自己回帰Transformerが持つ長文脈推論のメモリ帯域ボトルネックを、階層的な潜在ストリームを用いて緩和する言語モデルである。トークン列を下位から上位へ圧縮して保持し、生成時には上位の状態更新を主としつつ、下位トークンはチャンク内の局所復元として生成する設計である。[R1]

## 参考ドキュメント
- [R1] PHOTON: Hierarchical Autoregressive Modeling for Lightspeed and Memory-Efficient Language Generation（arXiv:2512.20687v1, 2025-12-22）
- [R2] Attention Is All You Need（arXiv:1706.03762, 2017）
- [RJ1] Transformer推論のKV-cacheに関する日本語解説（NTTPCまたは同等の国内解説記事）

## 1. 背景

自己回帰Transformerは、生成ステップごとに過去の状態へ注意を向けるため、文脈長に比例して注意対象が増大する構造を持つ。[R2] 推論ではKV-cacheを用いて過去のKey/Valueを再計算せずに済ませる一方、文脈が長いほどキャッシュ読み書き量が増え、計算量よりもメモリ転送が律速になりやすいことが指摘されている。[R1][RJ1][RJ2]

とりわけ長いプロンプトを複数回再利用する提供形態では、prefill（プロンプト処理）で大きなキャッシュを構築し、decode（逐次生成）で同じ巨大キャッシュを何度も参照することになる。[R1] この状況では、GPUの演算性能を上げてもメモリ帯域が追いつかず、スループットが伸びない形になりやすい。[RJ1][RJ2]


## 2. PHOTONの狙い

PHOTONは、生成が常にトークン粒度の水平走査である必要はない、という問題設定から出発する。[R1] 言語が階層構造（サブワード・単語・文・談話）を持つという観点を計算資源の観点と結びつけ、粗い状態で文脈を代表させ、必要なときだけ局所的に細部を生成する方針を取る。[R1]

このとき重要なのは、上位状態が推論時にも持続的に保持・更新される点である。[R1] つまり、入力を一度だけ縮約するだけでなく、生成の進行に合わせて上位潜在ストリームも更新され、その上位状態から下位を復元する構造が繰り返される。[R1]

## 3. 記法
語彙を$V$、トークン列を$t_{1:T}\in V^T$とし、埋め込み系列を$X^{(0)}\in\mathbb{R}^{T\times D_0}$とする。[R1] 階層は$L$段で、レベル$l\in\{1,\dots,L\}$にはチャンク長$C_l\in\mathbb{N}$が割り当てられる。[R1]

レベル0のユニット数を$M_0:=T$とし、$M_l:=M_{l-1}/C_l$によって上位ほど系列長が短くなるように定義する。[R1] レベル$l$の$g$番目チャンクに対応するレベル$(l-1)$側インデックス集合を
$$
I_g^{(l)}:=\{(g-1)C_l+i\mid i\in[C_l]\}\subseteq[M_{l-1}]
$$
とし、対応する部分テンソルを$X^{(l-1)}_{I_g^{(l)}}$と書く。[R1]

## 4. モデル構成
PHOTONは大きく、(i) 階層エンコーダ（下位→上位）と、(ii) 階層デコーダ（上位→下位）から構成される。[R1] エンコーダはトークン列を段階的に圧縮して粗い潜在ストリームを作り、デコーダは粗いストリームを条件として下位表現を復元し、最終的に次トークン分布を出力する。[R1]

## 5. 階層エンコーダ
### 5.1 定義
各レベル$l$に対し、エンコーダは
$$
X^{(l)} = E_\theta^{(l)}(X^{(l-1)}) := F_\theta^{(l)}\circ C_\theta^{(l)}(X^{(l-1)})
$$
で与えられる。[R1] ここで$C_\theta^{(l)}$はチャンク化（縮約）を担い、$F_\theta^{(l)}$は縮約後系列に対する因果Transformerである。[R1]

### 5.2 Context Chunker（縮約）
チャンクャは、各チャンク$X^{(l-1)}_{I_g^{(l)}}$を単一ベクトルへ写像して
$$
A^{(l)}:=[A^{(l)}_{1:M_l}]\in\mathbb{R}^{M_l\times D_l},\quad A_g^{(l)}:=C_\theta^{(l)}\!\left(X^{(l-1)}_{I_g^{(l)}}\right)
$$
を得る。[R1] 実装としては、チャンク内の結合（concat）後に線形射影する方法や1D畳み込みが挙げられ、論文では結合＋射影を代表例として採用している。[R1]

### 5.3 Context Encoder（上位の因果依存）
$F_\theta^{(l)}$はチャンク系列$A^{(l)}$に対する自己回帰Transformerであり、
$$
X^{(l)} = F_\theta^{(l)}(A^{(l)})
$$
により文脈化された上位状態を出力する。[R1] これにより、トークンごとに長大な履歴へ注意するのではなく、短いチャンク系列に対してグローバルな因果依存を学習することになる。[R1]

## 6. 階層デコーダ
### 6.1 定義
各レベル$l$で、デコーダは
$$
\hat{X}^{(l-1)} = D_\theta^{(l)}(\hat{X}^{(l)}) := G_\theta^{(l)}\circ U_\theta^{(l)}(\hat{X}^{(l)})
$$
と定義される。[R1] ここで$U_\theta^{(l)}$は上位状態を短い条件系列へ変換するモジュールであり、$G_\theta^{(l)}$は各チャンク内で独立に走る局所自己回帰デコーダである。[R1]

### 6.2 Context Converter（条件系列の生成）
因果性を保つため、レベル$(l-1)$の$g$番目チャンク復元は、レベル$l$の過去チャンク状態$\hat{X}^{(l)}_{1:g-1}$のみに依存するように設計される。[R1] まず
$$
Z_g^{(l-1)}:=U_\theta^{(l)}(\hat{X}^{(l)}_{1:g-1})\in\mathbb{R}^{R_l\times D_{l-1}}
$$
を計算し、これをチャンク内局所生成の「前置条件系列」として用いる。[R1] $R_l$は条件系列長であり、局所デコーダが参照できる上位情報の帯域幅を決めるパラメータである。[R1]

### 6.3 Local Autoregressive Decoder（チャンク内復元）
局所デコーダ$G_\theta^{(l)}$は、チャンク内トークン（あるいは下位ユニット）を自己回帰的に生成するが、注意範囲は「条件系列$Z_g^{(l-1)}$」と「同一チャンク内の過去ユニット」に厳密に制限される。[R1] これにより、decode時に参照すべきKV-cacheがグローバル系列長ではなくチャンク長に比例する形へ変わる。[R1]

## 7. 学習目標
PHOTONは最終的に通常のdecoder-only言語モデルとして振る舞うことを維持しつつ、階層潜在表現が安定して機能するよう補助損失を導入する。[R1] 論文ではテンソル同形の非類似度$D(\cdot,\cdot)$を用いて損失を定義しており、これは実装でMSEやコサイン距離などへ具体化できる形になっている（どれを採用するかは実装選択である）。[R1]

### 7.1 次トークン損失
出力$\hat{X}^{(0)}$から得られるロジットに対して通常の負対数尤度を最小化する：
$$
\mathcal{L}_{token}=-\sum_{i=1}^{T-1}\log p_\theta(t_{i+1}\mid t_{1:i}).
$$
これにより、PHOTONは出力レベルでは標準的な自己回帰LMとして整合する。[R1]

### 7.2 再帰復元損失
中間潜在ストリームが上位から確実に復元できるよう、各レベルでエンコーダ表現とデコーダ復元を整列させる：
$$
\mathcal{L}_{rec}
=\frac{1}{\sum_{l=1}^L M_l C_l D_{l-1}}
\sum_{l,g} D\!\left(\hat{X}^{(l-1)}_{I_g^{(l)}},\, X^{(l-1)}_{I_g^{(l)}}\right).
$$
この正則化は、階層状態が推論時の持続表現として機能するための整合条件を与える。[R1]

### 7.3 次文脈損失
上位表現が「圧縮的でありつつ予測的」になるよう、各チャンク表現が過去チャンクから予測できることを促す：
$$
\mathcal{L}_{context}
=\sum_{l=1}^L\frac{1}{M_l-1}\sum_{g=2}^{M_l}D\!\left(X_g^{(l)},\,A_g^{(l)}\right).
$$
ここで$X_g^{(l)}$は$A_{1:g-1}^{(l)}$のみから因果Transformer$F_\theta^{(l)}$で計算されたものとして扱う。[R1]

### 7.4 重み付け
論文では損失をスカラー重みで制御する前提が述べられており、$\alpha,\beta$で補助損失寄与を調整する設計になっている。[R1] 実験設定としては$\alpha=\beta=0$とし、補助損失を切った条件でも有効性を示している点が特徴である。[R1]

## 8. 推論時の計算負荷とメモリ転送
### 8.1 Transformerにおけるprefillとdecodeの性質
prefillはプロンプト全トークンを並列処理するため、自己注意とFFNが支配的で演算律速になりやすい。[R1] 一方decodeは1トークンずつ進むため、巨大なKV-cacheを反復参照する形になり、文脈長とバッチサイズに比例してメモリ転送量が増え、帯域律速になりやすい。[R1][RJ1][RJ2]

### 8.2 PHOTONにおける計算量スケーリング
レベル$l$のグローバルエンコーダは$M_l\approx T/C_{\le l}$（$C_{\le l}:=\prod_{k=1}^l C_k$）長の系列に対して注意を計算するため、prefill計算量は
$$
\sum_{l=1}^L \mathcal{O}(M_l^2)
=\sum_{l=1}^L \mathcal{O}\!\left(\left(\frac{T}{C_{\le l}}\right)^2\right)
$$
と整理される。[R1] 局所デコーダ側は各チャンク内で長さ$C_l$に注意を制限するため、生成側の支配項は
$$
\sum_{l=1}^L \mathcal{O}(M_l C_l^2)
=\sum_{l=1}^L \mathcal{O}\!\left(\frac{T}{C_{\le l}}\cdot C_l^2\right)
$$
となり、グローバル系列長$T$への二乗依存を局所化で弱める構造である。[R1]

### 8.3 KV-cache容量の見積もり
PHOTONでは、エンコーダ各レベルの状態$X^{(l)}$と、デコーダの条件系列$Z^{(l-1)}$を保持する形になるため、KV-cacheの支配項を
$$
\sum_{l=1}^L M_l + \sum_{l=1}^L M_{l-1}R_l
$$
と記述している。[R1] vanilla Transformerが$T$長のキャッシュを保持するのと比べ、$M_l\ll T$となる上位系列に寄せることで、decode時に反復参照する対象の総量を減らすことが狙いである。[R1]

## 9. Vanilla Transformer・Block Transformerとの比較
PHOTONは、(i) vanilla Transformer、(ii) Block Transformer（グローバルブロック＋ローカルトークン）と比較されている。[R1][R3] Block TransformerはKV-cacheをブロック単位へ縮約して負荷を下げるが、生成を駆動するストリームは基本的に一段であり、推論時に多段の持続状態を更新していく点がPHOTONの差分として説明されている。[R1][R3]

| 観点 | Vanilla Transformer | Block Transformer | PHOTON |
|---|---|---|---|
| グローバル状態の長さ | $T$ | $T/C$程度（ブロック） | $\{M_l\}$（多段で短い） |
| decode時注意範囲 | 全履歴 | ブロック履歴＋局所 | 条件系列＋チャンク局所 |
| 持続状態の階層 | なし | 事実上一段 | 多段（上位も更新） |
| 目的 | 品質最大化中心 | KV削減 | KV削減と多段状態更新 |

この比較は、KV-cache転送が律速になる設定ほど差が出やすい、という解釈と結びつく。[R1][RJ1][RJ2]

## 10. 実験設定
論文では学習コーパスとしてPile-uncopyrightedを用い、コンテキスト長は最大2048トークンに制限して評価している。[R1][R6] 訓練は50kステップで行い、モデル規模はおよそ160Mから1.2Bまでを比較している。[R1]

アーキテクチャとしては2段（$L=2$）を採用し、チャンク長は$C_1=4, C_2=4$として上位へ行くほど系列長を短縮している。[R1] 実験では補助損失の重みを$\alpha=\beta=0$としても効果が確認されており、構造そのものの寄与が示されている。[R1]

## 11. 評価指標と提供形態の切り分け
論文は、(i) prefillが支配的な条件と、(ii) decodeが支配的な条件を分けて議論している。[R1] これは、長プロンプト処理のコストと逐次生成時のメモリ転送コストが、異なる律速になるためである。[R1][RJ1]

また、メモリあたりスループット（throughput per unit memory）を中心に比較し、KV-cache圧縮が多クエリ提供で効くことを強調している。[R1] この観点は、キャッシュ量そのものが同時処理可能なリクエスト数やバッチサイズを制約し、結果としてサービス性能を決める、というシステム的事情と整合する。[R1][RJ2]

## 12. 主結果の読み方
PHOTONは、KV-cache転送を抑えた設計により、メモリあたりスループットで大きな向上を報告している。[R1] 論文では最大で$103\times$の向上が述べられており、長文脈・多クエリでの利点を示す数値として扱われている。[R1]

一方で、評価文脈長が2048までに制限されていること、最大モデルが1.2Bであること、ハイパーパラメータのアブレーションが十分でないことが限界として明示されている。[R1] より長い文脈やより大規模モデルでの検証は、構造的優位がどの程度維持されるかを判断する上で重要になる。[R1]

## 13. 関連研究との位置づけ
長文脈効率化としては、注意計算のカーネル最適化（例：FlashAttention）や推論システム（例：vLLM）などがあり、これらは主に同一構造のまま実装・スケジューリングで効率化を狙う。[R1][R4][R5] PHOTONはこれらを否定するのではなく、構造そのものを「水平走査から階層状態更新へ」変えることで、キャッシュ参照形態を変えようとする立場である。[R1]

階層的モデルとしてはFunnel-TransformerやTokenizer-free系（MEGABYTEなど）があるが、PHOTONはサブワードLMを対象に、推論時にも多段潜在ストリームを持続させる点を差分として述べている。[R1][R7] ここでは入力単位の粒度変更ではなく、推論時の状態表現を多段にすることで、キャッシュ参照の総量と更新頻度を下げることに焦点がある。[R1]

## 14. 実装・再現
既存のdecoder-only Transformer実装へPHOTONを導入する場合、レベルごとに「縮約（chunker）→上位因果Transformer（context encoder）」を積み重ね、逆方向に「条件系列生成（converter）→チャンク内因果デコーダ（local decoder）」を対応づける必要がある。[R1] このとき因果性の扱いが核心であり、チャンク$g$の生成が上位の$\hat{X}^{(l)}_{1:g-1}$のみへ依存するよう、converter入力とマスクを厳格に構成することが必要である。[R1]

推論ではまず階層prefillで全レベルの状態を構築し、その後は上位状態更新とチャンク内局所生成を繰り返す形になる。[R1] 多クエリ提供ではprefill状態を共有できるため、状態の保持形式と参照コストが実効性能へ直結する点に注意が要る。[R1][RJ2]

## 15. 注意点
$C_l$の選び方は、圧縮率と局所復元の負担のバランスを決めるため重要である。[R1] $C_l$を大きくすると上位系列が短くなりキャッシュ負担は減るが、局所デコーダが扱うチャンクが長くなり、チャンク内注意の計算・メモリが増える。[R1]

$R_l$は上位情報を下位へ降ろす帯域であり、小さすぎると局所復元が上位文脈を十分に受け取れず品質へ響き得る一方、大きすぎると$M_{l-1}R_l$項でキャッシュが増える。[R1] このトレードオフは文脈長やモデル規模、提供形態（prefill支配かdecode支配か）によって最適点が動くため、目的に応じた探索が必要である。[R1]

## 16. まとめと展望
PHOTONは、長文脈推論で顕在化するKV-cache転送律速を、階層潜在ストリームとチャンク局所復元により緩和する言語モデルである。[R1] 上位状態を持続的に更新しつつ下位を局所生成する設計により、メモリあたりスループットの大幅な改善が報告されている。[R1]

今後は、2048を超える長文脈、より大規模なモデル、階層段数や$C_l, R_l$の系統的検証によって、どの条件でどの程度の優位が確立するかを詰める必要がある。[R1] さらに、推論システム側（キャッシュ配置、バッチング、量子化など）との同時最適化により、構造的利点が実サービス性能へどこまで転写されるかが重要な論点になると考えられる。[R1][RJ2]

## その他参考文献
- [R3] Block Transformer（階層化によりKV-cache負荷を下げるTransformer系手法）
- [R4] vLLM / PagedAttention（KV-cache管理を主題にした推論システム）
- [R5] FlashAttention（I/O awareな注意計算最適化）
- [R6] The Pile（大規模言語モデル学習用データセット）
- [R7] Funnel-Transformer、Tokenizer-free系階層LM（MEGABYTE等）
- [RJ2] NVIDIAの日本語技術ブログ等におけるKV-cache・推論最適化解説
