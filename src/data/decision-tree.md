# 決定木アンサンブル学習（Decision-tree ensembles）

決定木アンサンブル学習は、複数の決定木を統合して予測精度と頑健性を高める教師あり学習の代表群である。材料科学では、組成・構造・プロセス由来の表形式特徴量に対して強いベースラインとなり、少量データでも実用性能を出しやすい枠組みである。

## 参考ドキュメント
- L. Breiman, Random Forests, Machine Learning (2001)
  https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf
- L. Ward et al., Matminer: An open source toolkit for materials data mining, Computational Materials Science (2018)
  https://www.sciencedirect.com/science/article/am/pii/S0927025618303252
- 中田和秀（東京科学大学）決定木とアンサンブル学習（講義資料, 日本語）
  https://www.nakatalab.iee.e.titech.ac.jp/text/pdf/ML/ML4.pdf


## 1. 位置づけ
材料データは、次の性質を持つことが多い。
- 非線形で、特徴量間の相互作用が強い（例：元素置換の相互作用、組成×熱処理条件）
- データ数が小さい、あるいは粒度が不均一（文献値、異なる測定条件、欠損）
- 物性の分布が歪む（外れ値、長い裾、対数スケールが自然な量）

決定木アンサンブルは、
- スケーリング不要（標準化に鈍感）
- 非線形・相互作用を自動で拾う
- 欠損や外れ値に比較的頑健（実装依存）
という性質から、材料インフォマティクスの表形式特徴量で高い実用性を持つ。

## 2. 決定木（単体）の要点
決定木は特徴空間を分割し、葉（leaf）ごとに一定の予測値を返すモデルである。

回帰木の典型的表現（葉で定数）：
$$
\hat{y}(x)=\sum_{m=1}^{M} c_m \, \mathbf{1}(x\in R_m)
$$
ここで $R_m$ は領域（分割セル）、$c_m$ はその領域の予測値（例：平均）である。

分割の指標（例）：
- 分類：Gini不純度
$$
G = 1-\sum_{k} p_k^2
$$
- 分類：エントロピー
$$
H = -\sum_k p_k \log p_k
$$
- 回帰：二乗誤差（分散）最小化
$$
\min_{\text{split}} \sum_{x_i\in R_L}(y_i-\bar{y}_L)^2+\sum_{x_i\in R_R}(y_i-\bar{y}_R)^2
$$

単体の決定木は分割を進めると過学習（高分散）しやすい。これを統合で抑えるのがアンサンブルである。

## 3. アンサンブルの基本：平均・投票による分散低減
B本の学習器（木）を $\hat{f}^{(b)}(x)$ とすると、統合予測は

回帰（平均）：
$$
\hat{f}_{ens}(x)=\frac{1}{B}\sum_{b=1}^{B}\hat{f}^{(b)}(x)
$$

分類（多数決または確率平均）：
$$
\hat{p}(y=k|x)=\frac{1}{B}\sum_{b=1}^{B}\hat{p}^{(b)}(y=k|x)
$$

重要なのは、木同士の誤差が「相関しすぎない」ことである。相関が低いほど平均化で誤差が減りやすい。

## 4. 決定木アンサンブルの主要ファミリ
### 4.1 バギング（Bagging）系
考え方：
- データをブートストラップ（復元抽出）で複数セット作る
- 各セットで木を独立に学習
- 平均・多数決で統合する

代表：ランダムフォレスト（Random Forest）
- バギングに加え、各分割で使用する特徴量候補をランダムに間引く（特徴サブサンプリング）
- 木同士の相関を下げ、分散低減を効かせる

代表：Extra Trees（Extremely Randomized Trees）
- 分割の特徴量だけでなく、しきい値（cut point）も強くランダム化し、多様性をさらに上げる発想である
- ノイズが多い・特徴量が多い場合に効くことがある（データ依存）

### 4.2 ブースティング（Boosting）系
考え方：
- 弱学習器（浅い木など）を逐次追加し、前段の誤差を次段で補正する
- バギングが「並列の平均化」であるのに対し、ブースティングは「逐次の誤差修正」である

加法モデルの典型形：
$$
F_M(x)=\sum_{m=1}^{M} \nu \, h_m(x)
$$
ここで $h_m$ は弱学習器（木）、$\nu$ は学習率（shrinkage）である。

勾配ブースティング（GBDT）は、損失関数 $L(y,F(x))$ の負勾配（擬似残差）を木で近似していく、関数空間での勾配降下として理解できる。

## 5. ユースケース
### 5.1 物性予測（回帰）
- 形成エネルギー、バンドギャップ、弾性定数、熱伝導率、誘電率、磁気特性指標など
- 特徴量：組成統計（Magpie系）、構造記述子（配位・距離統計）、プロセス条件（温度・時間・雰囲気）など

TCNやTransformerが波形や系列で強いのに対し、決定木アンサンブルは「表形式の説明変数」で強い、という住み分けになりやすい。

### 5.2 相・状態の分類（分類）
- 相安定／不安定、合成成功／失敗、結晶系、二次相混入の有無
- XRDピーク特徴量（ピーク位置・比・幅）や、条件（熱処理・冷却速度）を入力として分類する設計が典型である

### 5.3 プロセス最適化（教師あり＋探索）
- まずアンサンブルで「プロセス条件→性能」の順モデルを作る
- その後、ベイズ最適化や探索（別項）で逆設計に接続する
材料ではこの接続が実務上の価値になりやすい。

## 6. 注意点
### 6.1 データリーク（近縁物質の混入）
材料データは「同系統が多い」ため、ランダム分割は過大評価になりやすい。
- 同一組成の多形（polymorph）が学習と評価に跨る
- 同一研究グループ条件の系列が評価に混入する
- 組成が微差の系列が両側に入る

対策の例：
- グループ分割（組成式、プロトタイプ、研究IDなど）でGroupKFold相当を作る
- 外挿評価（未知元素系、未知組成域、未知プロセス域）を明示する

### 6.2 外挿が苦手になりやすい
決定木は領域分割であり、学習データの範囲外に対して物理的に妥当な外挿を保証しない。
- 設計意図が外挿なら、物理モデルや制約、あるいは表現（特徴量）に物理を入れる必要がある

### 6.3 特徴量重要度の誤読
決定木系の重要度（split gainや不純度減少）は、カテゴリ数の多い特徴や分散の大きい特徴に偏りうる。
- 置換の利く方法として、Permutation importanceやSHAP等を併用する設計が実務的である
- ただし重要度は因果の証明ではなく、データとモデルの都合を反映する指標である

## 7. 手法の使い分け
| 手法群 | 何に強いか | 何に弱いか | 材料での典型用途 |
|---|---|---|---|
| Random Forest | 頑健、チューニング難度が低い、少量でも戦える | 最高精度はGBDTに譲る場合が多い、外挿は苦手 | まずのベースライン、特徴量探索 |
| Extra Trees | 高い多様性、ノイズに強い場合 | ランダム化が強すぎると性能低下 | 多特徴・ノイズ条件の検討 |
| GBDT系（総称） | 表形式で高精度になりやすい | 過学習とチューニングの管理が要る | 最終モデル候補、性能勝負 |
| AdaBoost系 | 分類で効く場合がある | ノイズ・外れ値に弱い場合 | 追加比較・教育用途 |

注：XGBoost / LightGBM / CatBoostなどの実装差・カテゴリ処理・欠損処理は別項で整理するのがよい。

## 8. 注意点
- 目的変数の定義は測定条件を揃えているか（温度、相、測定法）
- 訓練・評価分割は近縁物質リークを避けているか（組成・プロトタイプ・系列分割）
- 特徴量は物理的に意味のあるスケールか（対数、規格化、単位）
- ベースラインとしてRF/ExtraTrees/GBDTを同条件で比較しているか
- 重要度・寄与は複数指標（Permutation、SHAP等）で整合を確認したか

## まとめ
決定木アンサンブル学習は、複数の決定木の統合により分散を下げ、表形式データで強い予測性能を示す教師あり学習の中核である。材料科学では、組成・構造記述子・プロセス条件のような非線形かつ少量になりやすいデータに対して有効である一方、近縁材料の混入によるデータリークと外挿性能の限界が主要リスクであり、分割設計と評価設計が成否を支配する。
